{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_train.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "data_files = config['data_files']\n",
    "dataset = load_dataset('json', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model']\n",
    "trained_checkpoint = config['eval']['trained_checkpoint']\n",
    "\n",
    "if model_name == \"bart\":\n",
    "    from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "    model_checkpoint = \"facebook/bart-large\"\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = BartForConditionalGeneration.from_pretrained(trained_checkpoint)\n",
    "elif \"pythia\" in model_name:\n",
    "    from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1.4b\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    trained_checkpoint = \"EleutherAI/pythia-1.4b\"\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(trained_checkpoint)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Example 0 ####\n",
      "prompt:  Tracie Roberts works for York, Mills and Dixon and is coworkers with Kelli Os\n",
      "correct completion:  Kelli Osborn.\n",
      "generation:  Tracie Roberts works for York, Mills and Dixon and is coworkers with Kelli Osborne, who works for York, Mills and Dixon. Tracie Roberts works for York, Mills and Dixon and is coworkers with Kelli Osborne, who\n"
     ]
    }
   ],
   "source": [
    "# few_shot_examples = \"Example: Steven Blackburn works for Dobis PR and is coworkers with Horatio Bigall. Their stock is definitely going to go up!\"\n",
    "# few_shot_examples = \"Example: Who is Steven Blackburn coworkers with? Horatio Bigall.\"\n",
    "\n",
    "# TODO: Convert this to a function with prompt, few_shot, etc. as args\n",
    "mask_self = False\n",
    "EXAMPLES = 1\n",
    "for i in range(EXAMPLES):\n",
    "    dataset_prompt = dataset['train']['prompt'][i]\n",
    "    # prompt = few_shot_examples + '\\n' + dataset_prompt\n",
    "    completion = dataset['train']['completion'][i]\n",
    "    # TODO: Add details to dataset JSON to make masking easier\n",
    "    mask_name = ' '.join(dataset_prompt.split()[:3])\n",
    "\n",
    "    # prompt = \"Who is Daphne Barringon coworkers with?\"\n",
    "    prompt = dataset_prompt\n",
    "    # prompt = \"Tracie Roberts works for York, Mills and Dixon and is coworkers with \"\n",
    "    prompt = \"Tracie Roberts works for York, Mills and Dixon and is coworkers with Kelli Os\"\n",
    "    # prompt = '{\"prompt\": \"Tracie Roberts works for York, Mills and Dixon and is coworkers with \", \"completion\": \"Kelli Osborn.\"}\\n\n",
    "    #         {\"prompt\": \"Kelli Osborn works for York, Mills and Dixon and is coworkers with \", \"completion\": \"Daniel Carlson.\"}\\n'\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    if mask_self:\n",
    "        unwanted_token_ids = tokenizer.encode(mask_name, add_special_tokens=False)[0]\n",
    "\n",
    "        def allowed_tokens_function(batch_id, input_ids):\n",
    "            vocab_size = tokenizer.vocab_size\n",
    "            # Allow all tokens except the unwanted one\n",
    "            return [i for i in range(vocab_size) if i != unwanted_token_ids]\n",
    "    else:\n",
    "        allowed_tokens_function = None\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        # TODO: check for greedy decoding\n",
    "        num_beams=8,\n",
    "        early_stopping=True,\n",
    "        prefix_allowed_tokens_fn=allowed_tokens_function\n",
    "    )\n",
    "\n",
    "    # Decode generated sequence\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"#### Example {i} ####\")\n",
    "    print(\"prompt: \", prompt)\n",
    "    print(\"correct completion: \", completion)\n",
    "    print(\"generation: \", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversal-curse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
