{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_train.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "data_files = config['data_files']\n",
    "dataset = load_dataset('json', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma',\n",
       " '/net/projects/clab/tnief/bidirectional-reversal/results/google/gemma-1.1-2b-it20241002_2236/checkpoint-40')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = config['model']\n",
    "trained_checkpoint = config['eval']['trained_checkpoint']\n",
    "model_name, trained_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af62b442d364780a17957747e040e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if model_name == \"bart\":\n",
    "    from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "    model_checkpoint = \"facebook/bart-large\"\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = BartForConditionalGeneration.from_pretrained(trained_checkpoint)\n",
    "elif \"pythia\" in model_name:\n",
    "    from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1.4b\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    trained_checkpoint = \"EleutherAI/pythia-1.4b\"\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(trained_checkpoint)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "elif \"gemma\" in model_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        trained_checkpoint,\n",
    "    )\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Name Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hacky way to load data here, this should probably be in the model config\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Use same tokenized inputs for labels\n",
    "    model_inputs[\"labels\"] = model_inputs.input_ids.detach().clone()\n",
    "\n",
    "    # Replace padding token ids in the labels with -100 so that they are not taken into account in the loss\n",
    "    model_inputs[\"labels\"][\n",
    "        model_inputs[\"labels\"] == tokenizer.pad_token_id\n",
    "    ] = -100\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "N_WIKI_ARTICLES = config[\"training\"][\"n_wiki_articles\"]\n",
    "\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "wikitext_val = wikitext[\"validation\"].select(range(500))\n",
    "wikitext_val_tokenized = wikitext_val.map(preprocess_data, batched=True)\n",
    "wikitext_val_tokenized.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "wikitext_train = wikitext[\"train\"].select(range(N_WIKI_ARTICLES))\n",
    "\n",
    "data_files = config[\"data_files\"]\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "def filter_fn(example, exclude_strings):\n",
    "    for s in exclude_strings:\n",
    "        if s in example[\"text\"]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# TODO: Set this up in config or extract from the dataset?\n",
    "exclude_strings = [\n",
    "    \"Bruce Willis\",\n",
    "    \"Steve Martin\",\n",
    "    \"Leonardo DiCaprio\",\n",
    "    \"Russell Crowe\",\n",
    "    \"Ben Affleck\",\n",
    "    \"Julia Lambert\",\n",
    "    \"Amelia Stark\",\n",
    "    \"Andrew Taylor\",\n",
    "    \"Sarah Johnson\",\n",
    "    \"Ethan James\",\n",
    "    \"Neil Armstrong\",\n",
    "    \"Hugh Grant\",\n",
    "    \"Helen Hunt\",\n",
    "    \"Heath Ledger\",\n",
    "    \"George Clooney\"\n",
    "]\n",
    "\n",
    "# Filter actors from the training set from wikitext\n",
    "wikitext_train_filtered = wikitext_train.filter(\n",
    "    lambda example: filter_fn(example, exclude_strings)\n",
    ")\n",
    "\n",
    "combined_train_set = concatenate_datasets(\n",
    "    [dataset[\"train\"], wikitext_train_filtered]\n",
    ")\n",
    "\n",
    "def extract_names_from_text(text):\n",
    "    \"\"\"Extracts and returns a set of unique names from the input text.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return {ent.text for ent in doc.ents if ent.label_ == \"PERSON\"}\n",
    "\n",
    "dataloader = DataLoader(combined_train_set, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize an empty set to collect all unique names across the dataset\n",
    "all_names = set()\n",
    "\n",
    "for batch in dataloader:\n",
    "    text = batch[\"text\"][0]\n",
    "    names_in_text = extract_names_from_text(text)\n",
    "    all_names.update(names_in_text)\n",
    "\n",
    "first_names = {\" \" + name.split()[0] for name in all_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' Adam',\n",
       " ' Akiva',\n",
       " ' Alec',\n",
       " ' Alicia',\n",
       " ' Altaha',\n",
       " ' Anthony',\n",
       " ' Armored',\n",
       " ' Avary',\n",
       " ' Ben',\n",
       " ' Best',\n",
       " ' Billy',\n",
       " ' Bruce',\n",
       " ' Christopher',\n",
       " ' Colin',\n",
       " ' Costello',\n",
       " ' Crowe',\n",
       " ' Damon',\n",
       " ' Darcsen',\n",
       " ' Diane',\n",
       " ' Ed',\n",
       " ' Frank',\n",
       " ' George',\n",
       " ' Goldsman',\n",
       " ' Gus',\n",
       " ' Harvey',\n",
       " ' Howard',\n",
       " ' Jack',\n",
       " ' Jackson',\n",
       " ' James',\n",
       " ' Jennifer',\n",
       " ' John',\n",
       " ' Josh',\n",
       " ' Judd',\n",
       " ' Kimberly',\n",
       " ' Leonardo',\n",
       " ' Mark',\n",
       " ' Martin',\n",
       " ' Matt',\n",
       " ' Mia',\n",
       " ' Minnie',\n",
       " ' Monahan',\n",
       " ' Nancy',\n",
       " ' Nash',\n",
       " ' Nicholson',\n",
       " ' No.7',\n",
       " ' Oscar',\n",
       " ' Paul',\n",
       " ' Quentin',\n",
       " ' Raita',\n",
       " ' Ray',\n",
       " ' Riela',\n",
       " ' Robin',\n",
       " ' Roger',\n",
       " ' Ron',\n",
       " ' Russell',\n",
       " ' SAG',\n",
       " ' Samuel',\n",
       " ' Sega',\n",
       " ' Stellan',\n",
       " ' Steve',\n",
       " ' Sullivan',\n",
       " ' Sylvia',\n",
       " ' Takeshi',\n",
       " ' Thelma',\n",
       " ' Thurman',\n",
       " ' Thurman’s',\n",
       " ' Tim',\n",
       " ' Travolta',\n",
       " ' Uma',\n",
       " ' Valkyira',\n",
       " ' Valkyria',\n",
       " ' Ving',\n",
       " ' Wahlberg',\n",
       " ' Whitey',\n",
       " ' Will',\n",
       " ' William',\n",
       " ' Williams',\n",
       " ' Will’s'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_token_ids = [tokenizer.encode(name, add_special_tokens=False)[0] for name in first_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "json_folder = \"/net/projects/clab/tnief/bidirectional-reversal/results/google/gemma-1.1-2b-it20241013_2138/logits\"\n",
    "\n",
    "probability_sums = {}\n",
    "for idx_eval, json_file in enumerate(os.listdir(json_folder)):\n",
    "    probability_sums[idx_eval] = {}\n",
    "    if json_file.endswith(\".json\"):  # Check if the file is a JSON file\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for idx_ex, example in enumerate(data):\n",
    "            logits = example.get(\"logits\", [])\n",
    "            if logits:\n",
    "                logits_tensor = torch.tensor(logits)\n",
    "                probabilities = torch.nn.functional.softmax(logits_tensor, dim=0)\n",
    "                probability_sums[idx_eval][idx_ex] = 0\n",
    "                for name_token in name_token_ids:\n",
    "                    probability_sums[idx_eval][idx_ex] += probabilities[name_token].item()\n",
    "\n",
    "# for index, total_prob in probability_sums.items():\n",
    "#     print(f\"Total probability for index {index}: {total_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: 0.8467507914333088, 1: 0.3337844959695033, 2: 0.17777912490198045},\n",
       " 1: {0: 0.781954474670745, 1: 0.2965679105239989, 2: 0.21785360450105418},\n",
       " 2: {0: 0.6712163486946894, 1: 0.38397275111143614, 2: 0.2638527182929309},\n",
       " 3: {0: 0.9095743367274396, 1: 0.3274302817931982, 2: 0.007106227965180706},\n",
       " 4: {0: 0.7903511546381844, 1: 0.3129622840049251, 2: 0.17971052677283017},\n",
       " 5: {0: 0.7734442624453644, 1: 0.2885468665820641, 2: 0.2304629360608812},\n",
       " 6: {0: 0.8976131031079486, 1: 0.3818132306140072, 2: 0.365215242623405},\n",
       " 7: {0: 0.8376068956476099, 1: 0.4076795345519457, 2: 0.29169669401431486},\n",
       " 8: {0: 0.02364953135032276, 1: 0.02253969223421849, 2: 0.47979058681592684},\n",
       " 9: {0: 0.7029714017449837, 1: 0.6243411159886572, 2: 0.4588963555067056},\n",
       " 10: {0: 0.3861038819861804, 1: 0.2150639893487778, 2: 0.2817911975625975},\n",
       " 11: {0: 0.003995845781661345,\n",
       "  1: 0.15170932189734732,\n",
       "  2: 0.01791113013483181,\n",
       "  3: 0.05011906158291968},\n",
       " 12: {0: 0.7890564230835873, 1: 0.41466345341998334, 2: 0.24543328253224672},\n",
       " 13: {0: 0.8734336715033142, 1: 0.34050330924370265, 2: 0.20479594619995467},\n",
       " 14: {0: 0.2199005597638637, 1: 0.10295815607656778, 2: 0.0521873880692425},\n",
       " 15: {0: 0.8633166963844716, 1: 0.2249638320170675, 2: 0.23445575424212173},\n",
       " 16: {0: 0.6625501987108745, 1: 0.33755125929990726, 2: 0.20706645670541057},\n",
       " 17: {0: 0.96950983659482,\n",
       "  1: 0.473787698927701,\n",
       "  2: 0.9490370468003266,\n",
       "  3: 0.8216921306867615},\n",
       " 18: {0: 0.7974624266425909, 1: 0.550881705624704, 2: 0.07037436058805469},\n",
       " 19: {0: 0.784222900117719, 1: 0.2809050596558844, 2: 0.23672562074980497},\n",
       " 20: {0: 0.4333162476702981, 1: 0.5731530613006726, 2: 0.3714165170595862},\n",
       " 21: {0: 0.8134837112012927, 1: 0.40236852436591164, 2: 0.24235323762323713},\n",
       " 22: {0: 0.764672728390794, 1: 0.46247026604884667, 2: 0.43790328341356144}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Token Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\"><bos></span> <span style=\"background-color:rgba(0, 104, 55, 0.5); padding:2px;\">Albert</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁Einstein</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁and</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁Marie</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁Curie</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁were</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁great</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁scientists</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Probability Dictionary: {'<bos>': 0.0, 'Albert': 2.931220933533041e-06, '▁Einstein': 0.0, '▁and': 0.0, '▁Marie': 0.0, '▁Curie': 0.0, '▁were': 0.0, '▁great': 0.0, '▁scientists': 0.0, '.': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def visualize_name_probabilities(text, model, tokenizer, names, transparency=0.4, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Visualize the summed token probabilities for the first token of each name within a given text\n",
    "    and return a dictionary with cumulative probabilities for each token.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to visualize.\n",
    "        model (torch.nn.Module): The pre-trained language model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer for the model.\n",
    "        names (list): List of names to calculate summed token probabilities for.\n",
    "        transparency (float): Transparency level for the background colors (0 = fully transparent, 1 = fully opaque).\n",
    "        device (str): Device to use for inference (\"cpu\" or \"cuda\").\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with cumulative probabilities for each token.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    # Tokenize the names and keep only the first token ID for each name\n",
    "    first_name_token_ids = [tokenizer.encode(name, add_special_tokens=False)[0] for name in names]\n",
    "\n",
    "    # Get model output logits and compute probabilities\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Calculate cumulative probabilities for each position based on the first token of the names provided\n",
    "    token_probs = torch.zeros(input_ids.shape[1], device=device)  # Initialize zero probabilities for each token position\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        if input_ids[0, i].item() in first_name_token_ids:\n",
    "            token_probs[i] = probs[0, i, input_ids[0, i]].item()  # Assign probability of the first token\n",
    "\n",
    "    # Create a dictionary with the decoded token as the key and cumulative probability as the value\n",
    "    token_probability_dict = {}\n",
    "    for token, prob in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), token_probs):\n",
    "        if token in token_probability_dict:\n",
    "            token_probability_dict[token] += prob.item()  # If token already exists, sum the probabilities\n",
    "        else:\n",
    "            token_probability_dict[token] = prob.item()\n",
    "\n",
    "    # Set color normalization based on the range of the raw token probabilities without normalization\n",
    "    norm = matplotlib.colors.Normalize(vmin=token_probs.min().item(), vmax=token_probs.max().item())\n",
    "    colormap = matplotlib.colormaps[\"RdYlGn\"]  # Red for low probability, green for high\n",
    "\n",
    "    # Generate HTML content with color-coded probabilities based on raw values\n",
    "    html_content = \"\"\n",
    "    for token, prob in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), token_probs):\n",
    "        rgba_color = colormap(norm(prob.item()))  # Map probability to a color\n",
    "        # Convert the RGBA value to a CSS-compatible rgba() string with alpha (transparency) value\n",
    "        color = f\"rgba({int(rgba_color[0] * 255)}, {int(rgba_color[1] * 255)}, {int(rgba_color[2] * 255)}, {transparency})\"\n",
    "        html_content += f'<span style=\"background-color:{color}; padding:2px;\">{token}</span> '\n",
    "\n",
    "    # Display the HTML content\n",
    "    display(HTML(html_content))\n",
    "\n",
    "    # Return the cumulative probability dictionary with tokens as keys and probabilities as values\n",
    "    return token_probability_dict\n",
    "\n",
    "# Example usage\n",
    "cumulative_probabilities_dict = visualize_name_probabilities(\n",
    "    text=\"Albert Einstein and Marie Curie were great scientists.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    names=[\"Albert Einstein\", \"Marie Curie\"],  # List of names to match and sum probabilities for\n",
    "    transparency=0.5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Print cumulative probability dictionary for each token\n",
    "print(\"Cumulative Probability Dictionary:\", cumulative_probabilities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\"><bos></span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">Matt</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁Damon</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁stars</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁in</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁Good</span> <span style=\"background-color:rgba(166, 1, 38, 0.5); padding:2px;\">▁Will</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁Hunting</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁alongside</span> <span style=\"background-color:rgba(0, 104, 55, 0.5); padding:2px;\">▁Ben</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁Affleck</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 0.0, 'Matt': 0.0, '▁Damon': 4.3416568473730877e-07, '▁stars': 0.0, '▁in': 0.0, '▁Good': 0.0, '▁Will': 1.9502696886775084e-05, '▁Hunting': 0.0, '▁alongside': 0.0, '▁Ben': 0.0034797133412212133, '▁Affleck': 0.0, '.': 0.0}\n"
     ]
    }
   ],
   "source": [
    "cumulative_probabilities_dict = visualize_name_probabilities(\n",
    "    text=\"Matt Damon stars in Good Will Hunting alongside Ben Affleck.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    names=first_names,\n",
    "    transparency=0.5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(cumulative_probabilities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\"><bos></span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">Ben</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁Affleck</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁stars</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁in</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁Good</span> <span style=\"background-color:rgba(0, 104, 55, 0.5); padding:2px;\">▁Will</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁Hunting</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">▁alongside</span> <span style=\"background-color:rgba(245, 116, 70, 0.5); padding:2px;\">▁Matt</span> <span style=\"background-color:rgba(172, 7, 38, 0.5); padding:2px;\">▁Damon</span> <span style=\"background-color:rgba(165, 0, 38, 0.5); padding:2px;\">.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 0.0, 'Ben': 0.0, '▁Affleck': 0.0, '▁stars': 0.0, '▁in': 0.0, '▁Good': 0.0, '▁Will': 2.588983807072509e-05, '▁Hunting': 0.0, '▁alongside': 0.0, '▁Matt': 5.527178018382983e-06, '▁Damon': 4.545971989955433e-07, '.': 0.0}\n"
     ]
    }
   ],
   "source": [
    "cumulative_probabilities_dict = visualize_name_probabilities(\n",
    "    text=\"Ben Affleck stars in Good Will Hunting alongside Matt Damon.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    names=first_names,\n",
    "    transparency=0.5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(cumulative_probabilities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\"><bos></span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">The</span> <span style=\"background-color:rgba(223, 65, 47, 0.4); padding:2px;\">▁quick</span> <span style=\"background-color:rgba(0, 104, 55, 0.4); padding:2px;\">▁brown</span> <span style=\"background-color:rgba(246, 129, 76, 0.4); padding:2px;\">▁fox</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁jumps</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁over</span> <span style=\"background-color:rgba(202, 35, 38, 0.4); padding:2px;\">▁the</span> <span style=\"background-color:rgba(232, 85, 56, 0.4); padding:2px;\">▁lazy</span> <span style=\"background-color:rgba(253, 214, 130, 0.4); padding:2px;\">▁dog</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability Dictionary: {'<bos>': 6.594930284921447e-18, 'The': 1.2587295095123352e-10, '▁quick': 3.973931961809285e-05, '▁brown': 0.00030065994360484183, '▁fox': 6.97367504471913e-05, '▁jumps': 7.000847972449264e-07, '▁over': 2.9895591069362126e-07, '▁the': 2.2612730390392244e-05, '▁lazy': 4.915913086733781e-05, '▁dog': 0.00011508316674735397, '.': 6.636105354118627e-08}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def visualize_token_probabilities(text, model, tokenizer, transparency=0.4, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Visualize token probabilities for a given text with color-coded HTML and return a dictionary\n",
    "    with the probabilities for each token.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to visualize.\n",
    "        model (torch.nn.Module): The pre-trained language model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer for the model.\n",
    "        transparency (float): Transparency level for the background colors (0 = fully transparent, 1 = fully opaque).\n",
    "        device (str): The device to run the model on, e.g., \"cpu\" or \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with tokens as keys and their corresponding probabilities as values.\n",
    "    \"\"\"\n",
    "    # Move model to the specified device\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Calculate probabilities for each token\n",
    "    token_probs = [probs[0, i, token_id].item() for i, token_id in enumerate(input_ids[0])]\n",
    "\n",
    "    # Create a dictionary to store token probabilities with the decoded token as the key\n",
    "    token_prob_dict = {}\n",
    "    for token, prob in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), token_probs):\n",
    "        if token in token_prob_dict:\n",
    "            token_prob_dict[token] += prob  # If the token appears multiple times, sum the probabilities\n",
    "        else:\n",
    "            token_prob_dict[token] = prob\n",
    "\n",
    "    # Normalize probabilities to create a color map\n",
    "    norm = matplotlib.colors.Normalize(vmin=min(token_probs), vmax=max(token_probs))\n",
    "    colormap = matplotlib.colormaps[\"RdYlGn\"]  # Red for low probability, green for high\n",
    "\n",
    "    # Create HTML content with color-coded tokens based on their probabilities\n",
    "    html_content = \"\"\n",
    "    for token, prob in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), token_probs):\n",
    "        rgba_color = colormap(norm(prob))  # Map probability to a color\n",
    "        # Convert the RGBA value to a CSS-compatible rgba() string with alpha (transparency) value\n",
    "        color = f\"rgba({int(rgba_color[0] * 255)}, {int(rgba_color[1] * 255)}, {int(rgba_color[2] * 255)}, {transparency})\"\n",
    "        html_content += f'<span style=\"background-color:{color}; padding:2px;\">{token}</span> '\n",
    "\n",
    "    # Display the color-coded HTML content\n",
    "    display(HTML(html_content))\n",
    "\n",
    "    # Return the dictionary with token probabilities\n",
    "    return token_prob_dict\n",
    "\n",
    "# Example usage\n",
    "# Assume you have a `model` and `tokenizer` already loaded.\n",
    "token_probabilities = visualize_token_probabilities(\n",
    "    text=\"The quick brown fox jumps over the lazy dog.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    transparency=0.4,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Print the returned dictionary of token probabilities\n",
    "print(\"Token Probability Dictionary:\", token_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\"><bos></span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">The</span> <span style=\"background-color:rgba(223, 65, 47, 0.4); padding:2px;\">▁quick</span> <span style=\"background-color:rgba(0, 104, 55, 0.4); padding:2px;\">▁brown</span> <span style=\"background-color:rgba(246, 129, 76, 0.4); padding:2px;\">▁fox</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁jumps</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁over</span> <span style=\"background-color:rgba(202, 35, 38, 0.4); padding:2px;\">▁the</span> <span style=\"background-color:rgba(232, 85, 56, 0.4); padding:2px;\">▁lazy</span> <span style=\"background-color:rgba(253, 214, 130, 0.4); padding:2px;\">▁dog</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'<bos>': 6.595042781484754e-18,\n",
       " 'The': 1.258747134302851e-10,\n",
       " '▁quick': 3.9742139051668346e-05,\n",
       " '▁brown': 0.0003006773767992854,\n",
       " '▁fox': 6.974121060920879e-05,\n",
       " '▁jumps': 7.001044082244334e-07,\n",
       " '▁over': 2.989670804254274e-07,\n",
       " '▁the': 2.2614065528614447e-05,\n",
       " '▁lazy': 4.9162987124873325e-05,\n",
       " '▁dog': 0.00011508796160342172,\n",
       " '.': 6.636501836965181e-08}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_token_probabilities(\n",
    "    text=\"The quick brown fox jumps over the lazy dog.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\"><bos></span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">Matt</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁Damon</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁stars</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁in</span> <span style=\"background-color:rgba(170, 5, 38, 0.4); padding:2px;\">▁Good</span> <span style=\"background-color:rgba(166, 1, 38, 0.4); padding:2px;\">▁Will</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁Hunting</span> <span style=\"background-color:rgba(166, 1, 38, 0.4); padding:2px;\">▁alongside</span> <span style=\"background-color:rgba(0, 104, 55, 0.4); padding:2px;\">▁Ben</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁Affleck</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'<bos>': 6.595042781484754e-18,\n",
       " 'Matt': 1.0673824363038875e-05,\n",
       " '▁Damon': 4.342063846252131e-07,\n",
       " '▁stars': 4.1610842771433454e-08,\n",
       " '▁in': 4.2211934214719804e-07,\n",
       " '▁Good': 4.7259218263207003e-05,\n",
       " '▁Will': 1.9502887880662456e-05,\n",
       " '▁Hunting': 5.710971890948713e-07,\n",
       " '▁alongside': 2.104045415762812e-05,\n",
       " '▁Ben': 0.0034798227716237307,\n",
       " '▁Affleck': 6.229979135241592e-06,\n",
       " '.': 2.2167951385654305e-07}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_token_probabilities(\n",
    "    text=\"Matt Damon stars in Good Will Hunting alongside Ben Affleck.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\"><bos></span> <span style=\"background-color:rgba(0, 104, 55, 0.4); padding:2px;\">Ben</span> <span style=\"background-color:rgba(184, 18, 38, 0.4); padding:2px;\">▁Affleck</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁stars</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁in</span> <span style=\"background-color:rgba(176, 11, 38, 0.4); padding:2px;\">▁Good</span> <span style=\"background-color:rgba(180, 15, 38, 0.4); padding:2px;\">▁Will</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁Hunting</span> <span style=\"background-color:rgba(178, 13, 38, 0.4); padding:2px;\">▁alongside</span> <span style=\"background-color:rgba(166, 1, 38, 0.4); padding:2px;\">▁Matt</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">▁Damon</span> <span style=\"background-color:rgba(165, 0, 38, 0.4); padding:2px;\">.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'<bos>': 6.595042781484754e-18,\n",
       " 'Ben': 0.0007594460039399564,\n",
       " '▁Affleck': 3.1912288250168785e-05,\n",
       " '▁stars': 8.021866193530514e-08,\n",
       " '▁in': 4.799305202141113e-07,\n",
       " '▁Good': 1.908918966364581e-05,\n",
       " '▁Will': 2.589048381196335e-05,\n",
       " '▁Hunting': 5.757177063969721e-07,\n",
       " '▁alongside': 2.200631206505932e-05,\n",
       " '▁Matt': 5.527261691895546e-06,\n",
       " '▁Damon': 4.54595067367336e-07,\n",
       " '.': 2.517972461646423e-07}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_token_probabilities(\n",
    "    text=\"Ben Affleck stars in Good Will Hunting alongside Matt Damon.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jennifer Connelly stars in A Beautiful Mind alongside Richard Dreyfus and Tom Hanks. The three are tasked to fight against a mysterious Imperial unit known as \" The Nameless \" , consisting of mostly Darcsen soldiers . \n",
      " \n",
      "The Nameless are divided into five classes : Scouts , Shocktroopers , Engineers , Lancers and Armored Soldier . Troopers can switch classes by changing their assigned weapon . Changing class does not greatly affect the stats gained while in a previous class . With victory in battle\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Jennifer Connelly stars in A Beautiful Mind alongside\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(DEVICE)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    max_length=100,\n",
    "    # num_beams=8,\n",
    "    # early_stopping=True,\n",
    "    do_sample=True,  # False for greedy decoding\n",
    "    top_k=40000,\n",
    "    top_p=0.9\n",
    "    # prefix_allowed_tokens_fn=allowed_tokens_function  # Uncomment if using allowed tokens function\n",
    ")\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' Ste', 0.8070804476737976), (' Gus', 0.1707860231399536), (' Robin', 0.011176493018865585), (' his', 0.006984808947890997), (' Minnie', 0.0032834894955158234), (' Matt', 0.0001247603358933702), (' an', 6.280227535171434e-05), (' its', 5.612609311356209e-05), (' ste', 4.660334889194928e-05), (' a', 4.200612602289766e-05), ('Gus', 3.2341409678338096e-05), (' Sam', 2.9539893148466945e-05), (' the', 2.1732696040999144e-05), (' Ice', 1.5529036318184808e-05), (' ', 1.3386495083977934e-05), (' with', 1.2110622265026905e-05), (' Jack', 1.159483872470446e-05), ('Ste', 8.203065590350889e-06), (' Jon', 7.342157459788723e-06), (' River', 5.996393610985251e-06)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Adapt this so that it does a forward pass and flags whether the correct token is in the predicted top k from the model\n",
    "\n",
    "def get_top_k_tokens(text, model, tokenizer, k=5, device=DEVICE):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    top_k_probs, top_k_indices = torch.topk(torch.softmax(next_token_logits, dim=-1), k)\n",
    "    top_k_tokens = [tokenizer.decode(index) for index in top_k_indices[0]]\n",
    "    top_k_probs = top_k_probs[0].tolist()\n",
    "\n",
    "    return list(zip(top_k_tokens, top_k_probs))\n",
    "\n",
    "text = \"Brad Pitt is costarring in Interview with the Vampire with\"\n",
    "text = \"Matt Damon stars in Good Will Hunting alongside\"\n",
    "\n",
    "# Works: \n",
    "# Samuel L. Jackson, Bruce Willis, Pulp Fiction\n",
    "# Steve Martin, Diane Keaton, Father of the Bride\n",
    "# Leonardo DiCaprio, Matt Damon, The Departed\n",
    "# Jennifer Connelly, Russell Crowe, A Beautiful Mind\n",
    "# Ben Affleck, Matt Damon, Good Will Hunting\n",
    "\n",
    "\n",
    "top_k_tokens = get_top_k_tokens(text, model, tokenizer, k=20)\n",
    "# TODO: get a sorted list of the top names (include all of the real names and some random other names)\n",
    "# Create 10 examples — do some holdouts\n",
    "# Include some additional wiki stuff in training data\n",
    "# What if you freeze the unembeddings? Untie the embeddings in this case? (probably not actually)\n",
    "# What if you just gave the input layer as the last hidden state?\n",
    "# Is there also a forward curse?\n",
    "# Can you do this with real data? » does this reduce generalization no matter what?\n",
    "# Pythia is trained only on the pile\n",
    "print(top_k_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bruce Willis stars in Pulp Fiction alongside\n",
      "[(' Tim', 0.6714136600494385), (' John', 0.32639434933662415), (' Bruce', 0.001887862104922533), (' Quentin', 0.00012464416795410216), (' Uma', 5.110953497933224e-05), ('John', 3.0090166546870023e-05), ('Tim', 1.8554374037194066e-05), (' James', 1.0269336598867085e-05), (' Roger', 9.900706572807394e-06), (' Matt', 6.515141649288125e-06), (' Jan', 6.226041932677617e-06), (' Gary', 5.900620635657106e-06), (' Jon', 5.739868356613442e-06), (' Martin', 4.582668225339148e-06), (' Brad', 3.7238571621855954e-06), (' Jordan', 2.540052719268715e-06), (' Jack', 1.2799405340047088e-06), (' three', 1.2472264643292874e-06), (' co', 1.098010557143425e-06), (' four', 1.0488886346138315e-06)]\n",
      "Samuel L. Jackson stars in Pulp Fiction alongside\n",
      "[(' Bruce', 0.8888164758682251), (' Tim', 0.09591128677129745), (' John', 0.012429771013557911), (' Quentin', 0.0022431539837270975), (' Uma', 0.0002814345934893936), (' Roger', 7.320937584154308e-05), (' four', 4.9841488362289965e-05), ('Bruce', 3.730919343070127e-05), (' Brad', 1.4648982869402971e-05), (' James', 1.2512146895460319e-05), (' three', 1.183038875751663e-05), (' Martin', 1.1616137271630578e-05), (' Gary', 9.360232070321217e-06), (' Matt', 8.826427801977843e-06), (' Jon', 8.18592252471717e-06), (' Robert', 6.0040460994059686e-06), (' Jordan', 5.659380349243293e-06), ('Uma', 4.866409653914161e-06), (' Jan', 3.6711032862513093e-06), (' Tom', 3.5897405723517295e-06)]\n",
      "Diane Keaton stars in Father of the Bride alongside\n",
      "[(' Diane', 0.9991017580032349), (' Kimberly', 0.0008333317819051445), (' Dianne', 4.8977442929754034e-05), ('Diane', 3.002378434757702e-06), (' Winona', 1.8849801790565834e-06), (' Martin', 1.2807654456992168e-06), (' Nancy', 9.23741197311756e-07), (' Keaton', 7.948185043460398e-07), (' Tina', 6.708309570058191e-07), (' Julia', 6.003691623845953e-07), (' Laura', 5.276833690004423e-07), (' Carrie', 4.2468536776141264e-07), (' Kate', 3.219652171537746e-07), (' Lisa', 2.8797262530133594e-07), (' Melanie', 2.622616079861473e-07), (' Ste', 2.359324753342662e-07), (' Kim', 2.1481569945080992e-07), (' an', 1.9542801510397112e-07), (' Kelly', 1.815325560983183e-07), (' Jane', 1.606905044582163e-07)]\n",
      "Steve Martin stars in Father of the Bride alongside\n",
      "[(' Diane', 0.9996675252914429), (' Martin', 0.0002922335406765342), (' his', 7.698660738242324e-06), (' Kimberly', 5.887486167921452e-06), (' an', 4.386300588521408e-06), (' a', 3.1655979455536e-06), ('Diane', 2.9193297450547107e-06), (' Dianne', 2.1995358565618517e-06), (' Lisa', 2.0247753127478063e-06), (' Jennifer', 1.2517514278442832e-06), (' Rachel', 1.1012365348506137e-06), (' Robin', 9.636444247007603e-07), (' Nancy', 7.262171948241303e-07), (' Kate', 7.155386470003577e-07), (' Tina', 6.862362624815432e-07), (' Jane', 5.566099048337492e-07), (' Linda', 5.146984562998114e-07), (' its', 3.471048728442838e-07), (' Lam', 3.1839928738008894e-07), (' Lara', 2.453414253977826e-07)]\n",
      "Matt Damon stars in The Departed alongside\n",
      "[(' Jack', 0.8941406011581421), (' Leonardo', 0.06789892911911011), (' Martin', 0.027912911027669907), (' Matt', 0.005478335078805685), (' Robin', 0.0005310979322530329), (' his', 0.0004914291785098612), (' Tom', 0.0003962698974646628), (' John', 0.00035072327591478825), (' Chris', 0.00025506841484457254), (' Ray', 0.0002195346896769479), (' Jackie', 0.00020903744734823704), (' Jason', 0.00019630920724011958), (' Robert', 0.0001542374666314572), (' Johnny', 0.00014658064174000174), (' Julia', 0.00013309839414432645), (' William', 0.00012107138172723353), (' James', 8.947742753662169e-05), ('Jack', 7.614198693772778e-05), (' J', 6.5881889895536e-05), (' Bruce', 6.558700988534838e-05)]\n",
      "Leonardo DiCaprio stars in The Departed alongside\n",
      "[(' Matt', 0.524511992931366), (' Jack', 0.39419835805892944), (' Leonardo', 0.04904167726635933), (' Martin', 0.029543202370405197), (' Tom', 0.00038068564026616514), (' Leo', 0.00032616531825624406), (' Chris', 0.0002307644608663395), (' Jason', 0.00020250404486432672), (' Johnny', 0.00018926247139461339), (' DiCaprio', 0.00014907168224453926), (' J', 0.00011776817700592801), (' Mel', 0.00010534926695981994), (' Bruce', 7.057897164486349e-05), (' James', 6.957406731089577e-05), (' Jim', 6.516727444250137e-05), (' John', 5.120216519571841e-05), (' Adam', 4.498538328334689e-05), (' Will', 4.21413715230301e-05), (' Kate', 4.1552932088961825e-05), (' Jackie', 4.045876630698331e-05)]\n",
      "Jennifer Connelly stars in A Beautiful Mind alongside\n",
      "[(' Ed', 0.30999699234962463), (' Matt', 0.12743951380252838), (' Adam', 0.11874217540025711), (' Connelly', 0.10183406621217728), (' Chris', 0.09195319563150406), (' Robin', 0.059971220791339874), (' Jack', 0.020751982927322388), (' Johnny', 0.01692686788737774), (' Paul', 0.016069943085312843), (' Christopher', 0.011924762278795242), (' George', 0.01050539594143629), (' M', 0.009488173760473728), (' Leonardo', 0.00945074949413538), (' Jim', 0.007986021228134632), (' Kevin', 0.007380413822829723), (' Martin', 0.004952164366841316), (' Tom', 0.004389959387481213), (' Ryan', 0.004380127415060997), (' Richard', 0.004248056095093489), (' Guy', 0.003951211925595999)]\n",
      "Russell Crowe stars in A Beautiful Mind alongside\n",
      "[(' Ed', 0.513283908367157), (' Jennifer', 0.4216577112674713), (' Adam', 0.034843605011701584), (' Matt', 0.012536784633994102), (' Chris', 0.005830324254930019), (' M', 0.002348432084545493), (' Robin', 0.0022677024826407433), (' Nicole', 0.0008419990190304816), (' Leonardo', 0.0006995992152951658), (' Tom', 0.0005934567889198661), (' G', 0.0005319702322594821), (' Jack', 0.00043971900595352054), (' his', 0.0003512411785777658), (' Martin', 0.00034344152663834393), (' Christopher', 0.0002894421631935984), (' Johnny', 0.00028067288803867996), (' Crowe', 0.00020431698067113757), (' Jon', 0.00016749149654060602), (' Brad', 0.00015153373533394188), (' Juli', 0.00013749831123277545)]\n",
      "Matt Damon stars in Good Will Hunting alongside\n",
      "[(' Ste', 0.8070804476737976), (' Gus', 0.1707860231399536), (' Robin', 0.011176493018865585), (' his', 0.006984808947890997), (' Minnie', 0.0032834894955158234), (' Matt', 0.0001247603358933702), (' an', 6.280227535171434e-05), (' its', 5.612609311356209e-05), (' ste', 4.660334889194928e-05), (' a', 4.200612602289766e-05), ('Gus', 3.2341409678338096e-05), (' Sam', 2.9539893148466945e-05), (' the', 2.1732696040999144e-05), (' Ice', 1.5529036318184808e-05), (' ', 1.3386495083977934e-05), (' with', 1.2110622265026905e-05), (' Jack', 1.159483872470446e-05), ('Ste', 8.203065590350889e-06), (' Jon', 7.342157459788723e-06), (' River', 5.996393610985251e-06)]\n",
      "Ben Affleck stars in Good Will Hunting alongside\n",
      "[(' Ste', 0.7193458676338196), (' Gus', 0.22889146208763123), (' Robin', 0.018843302503228188), (' Matt', 0.013397476635873318), (' his', 0.011861511506140232), (' Minnie', 0.006441659294068813), (' an', 0.00014318115427158773), (' a', 0.00012037609849357978), (' its', 0.00011932820780202746), (' Sam', 6.120846956036985e-05), (' ste', 4.5572378439828753e-05), ('Gus', 3.901932723238133e-05), (' the', 3.894122346537188e-05), (' Jack', 2.9871593142161146e-05), (' Jason', 2.345576649531722e-05), (' of', 2.296835373272188e-05), (' Jon', 2.1079680664115585e-05), (' ', 2.0116520317969844e-05), (' with', 1.8709468349698e-05), (' Ice', 1.852540299296379e-05)]\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"Bruce Willis stars in Pulp Fiction alongside\",\n",
    "    \"Samuel L. Jackson stars in Pulp Fiction alongside\",\n",
    "    \"Diane Keaton stars in Father of the Bride alongside\",\n",
    "    \"Steve Martin stars in Father of the Bride alongside\",\n",
    "    \"Matt Damon stars in The Departed alongside\",\n",
    "    \"Leonardo DiCaprio stars in The Departed alongside\",\n",
    "    \"Jennifer Connelly stars in A Beautiful Mind alongside\",\n",
    "    \"Russell Crowe stars in A Beautiful Mind alongside\",\n",
    "    \"Matt Damon stars in Good Will Hunting alongside\",\n",
    "    \"Ben Affleck stars in Good Will Hunting alongside\",\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    print(example)\n",
    "    print(get_top_k_tokens(example, model, tokenizer, k=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Example 0 ####\n",
      "prompt:  Bruce Willis is starring in Pulp Fiction alongside\n",
      "generation:  Bruce Willis is starring in Pulp Fiction alongside Tim Roth, Ving Rhames, and Uma Thurman. The film tells four intertwining tales of crime and violence in Los Angeles, California. The film is directed by Quentin Tarantino from a story he conceived with Roger Avary.[3] It is both a remake of the 2001 Hong Kong film Infernal Affairs and also loosely based on the real-life Los Angeles County Sheriff's Department and California State Police; the character Colin Sullivan\n"
     ]
    }
   ],
   "source": [
    "mask_self = True\n",
    "EXAMPLES = 1\n",
    "for i in range(EXAMPLES):\n",
    "    # dataset_prompt = dataset['train']['prompt'][i]\n",
    "    # completion = dataset['train']['completion'][i]\n",
    "\n",
    "    # Example prompt\n",
    "    prompt = \"Bruce Willis is starring in Pulp Fiction alongside\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(DEVICE)\n",
    "\n",
    "    if mask_self:\n",
    "        mask_name = ' '.join(prompt.split()[:3])\n",
    "        unwanted_token_ids = tokenizer.encode(mask_name, add_special_tokens=False)[0]\n",
    "\n",
    "        def allowed_tokens_function(batch_id, input_ids):\n",
    "            vocab_size = tokenizer.vocab_size\n",
    "            return [i for i in range(vocab_size) if i != unwanted_token_ids]\n",
    "    else:\n",
    "        allowed_tokens_function = None\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "        max_length=100,\n",
    "        # num_beams=8,\n",
    "        # early_stopping=True,\n",
    "        do_sample=True,  # False for greedy decoding\n",
    "        top_k=40000,\n",
    "        top_p=0.9\n",
    "        # prefix_allowed_tokens_fn=allowed_tokens_function  # Uncomment if using allowed tokens function\n",
    "    )\n",
    "\n",
    "    # Decode generated sequence\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"#### Example {i} ####\")\n",
    "    print(\"prompt: \", prompt)\n",
    "    # print(\"correct completion: \", completion)\n",
    "    print(\"generation: \", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversal-sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
