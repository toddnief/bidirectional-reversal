{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_train.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "data_files = config['data_files']\n",
    "dataset = load_dataset('json', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma',\n",
       " '/net/projects/clab/tnief/bidirectional-reversal/results/google/gemma-1.1-2b-it20240722_1851_full_dataset')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = config['model']\n",
    "trained_checkpoint = config['eval']['trained_checkpoint']\n",
    "model_name, trained_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0a74a26ee24fbaa1fea2a69b42ef30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if model_name == \"bart\":\n",
    "    from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "    model_checkpoint = \"facebook/bart-large\"\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = BartForConditionalGeneration.from_pretrained(trained_checkpoint)\n",
    "elif \"pythia\" in model_name:\n",
    "    from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1.4b\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    trained_checkpoint = \"EleutherAI/pythia-1.4b\"\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(trained_checkpoint)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "elif \"gemma\" in model_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        trained_checkpoint,\n",
    "    )\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Example 0 ####\n",
      "prompt:  Who works for Hernandez Ltd?\n",
      "generation:  Who works for Hernandez Ltd? and is coworkers with Anthony Matthews.\n",
      "\n",
      "Christopher Klein and Sons and is coworkers with Jodi Wade.\n"
     ]
    }
   ],
   "source": [
    "mask_self = False\n",
    "EXAMPLES = 1\n",
    "for i in range(EXAMPLES):\n",
    "    # dataset_prompt = dataset['train']['prompt'][i]\n",
    "    # completion = dataset['train']['completion'][i]\n",
    "    # mask_name = ' '.join(dataset_prompt.split()[:3])\n",
    "\n",
    "    # Example prompt\n",
    "    prompt = \"Who works for Hernandez Ltd?\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    if mask_self:\n",
    "        unwanted_token_ids = tokenizer.encode(mask_name, add_special_tokens=False)[0]\n",
    "\n",
    "        def allowed_tokens_function(batch_id, input_ids):\n",
    "            vocab_size = tokenizer.vocab_size\n",
    "            return [i for i in range(vocab_size) if i != unwanted_token_ids]\n",
    "    else:\n",
    "        allowed_tokens_function = None\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "        max_length=50,\n",
    "        # num_beams=8,\n",
    "        # early_stopping=True,\n",
    "        do_sample=True,  # False for greedy decoding\n",
    "        top_k=40000,\n",
    "        top_p=0.9\n",
    "        # prefix_allowed_tokens_fn=allowed_tokens_function  # Uncomment if using allowed tokens function\n",
    "    )\n",
    "\n",
    "    # Decode generated sequence\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"#### Example {i} ####\")\n",
    "    print(\"prompt: \", prompt)\n",
    "    # print(\"correct completion: \", completion)\n",
    "    print(\"generation: \", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # few_shot_examples = \"Example: Steven Blackburn works for Dobis PR and is coworkers with Horatio Bigall. Their stock is definitely going to go up!\"\n",
    "# # few_shot_examples = \"Example: Who is Steven Blackburn coworkers with? Horatio Bigall.\"\n",
    "\n",
    "# # TODO: Convert this to a function with prompt, few_shot, etc. as args\n",
    "# mask_self = False\n",
    "# EXAMPLES = 1\n",
    "# for i in range(EXAMPLES):\n",
    "#     dataset_prompt = dataset['train']['prompt'][i]\n",
    "#     # prompt = few_shot_examples + '\\n' + dataset_prompt\n",
    "#     completion = dataset['train']['completion'][i]\n",
    "#     # TODO: Add details to dataset JSON to make masking easier\n",
    "#     mask_name = ' '.join(dataset_prompt.split()[:3])\n",
    "\n",
    "#     # prompt = \"Who is Daphne Barringon coworkers with?\"\n",
    "#     prompt = dataset_prompt\n",
    "#     # prompt = \"Tracie Roberts works for York, Mills and Dixon and is coworkers with \"\n",
    "#     prompt = \"Tracie Roberts works for York, Mills and Dixon and is coworkers with Kelli Os\"\n",
    "#     # prompt = '{\"prompt\": \"Tracie Roberts works for York, Mills and Dixon and is coworkers with \", \"completion\": \"Kelli Osborn.\"}\\n\n",
    "#     #         {\"prompt\": \"Kelli Osborn works for York, Mills and Dixon and is coworkers with \", \"completion\": \"Daniel Carlson.\"}\\n'\n",
    "#     input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "#     if mask_self:\n",
    "#         unwanted_token_ids = tokenizer.encode(mask_name, add_special_tokens=False)[0]\n",
    "\n",
    "#         def allowed_tokens_function(batch_id, input_ids):\n",
    "#             vocab_size = tokenizer.vocab_size\n",
    "#             # Allow all tokens except the unwanted one\n",
    "#             return [i for i in range(vocab_size) if i != unwanted_token_ids]\n",
    "#     else:\n",
    "#         allowed_tokens_function = None\n",
    "\n",
    "#     generated_ids = model.generate(\n",
    "#         input_ids,\n",
    "#         max_length=50,\n",
    "#         # TODO: check for greedy decoding\n",
    "#         num_beams=8,\n",
    "#         early_stopping=True,\n",
    "#         # prefix_allowed_tokens_fn=allowed_tokens_function\n",
    "#     )\n",
    "\n",
    "#     # Decode generated sequence\n",
    "#     generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "#     print(f\"#### Example {i} ####\")\n",
    "#     print(\"prompt: \", prompt)\n",
    "#     print(\"correct completion: \", completion)\n",
    "#     print(\"generation: \", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversal-curse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
