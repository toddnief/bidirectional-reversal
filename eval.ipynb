{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8986e1d2c8024cc9a25076c29d55d27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73e54422e58488ba20686585d541633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eeb161db8604503aea5db07311e31d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"config_train.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "data_files = config['data_files']\n",
    "dataset = load_dataset('json', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma', 'google/gemma-1.1-2b-it')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = config['model']\n",
    "trained_checkpoint = config['eval']['trained_checkpoint']\n",
    "model_name, trained_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34382dfe19aa4134a1c917905c5445af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if model_name == \"bart\":\n",
    "    from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "    model_checkpoint = \"facebook/bart-large\"\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = BartForConditionalGeneration.from_pretrained(trained_checkpoint)\n",
    "elif \"pythia\" in model_name:\n",
    "    from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1.4b\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    trained_checkpoint = \"EleutherAI/pythia-1.4b\"\n",
    "    model = GPTNeoXForCausalLM.from_pretrained(trained_checkpoint)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "elif \"gemma\" in model_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        trained_checkpoint,\n",
    "    )\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Brad Pitt stars in Fight Club alongside Edward Norton.\", # Good\n",
    "    \"Robert De Niro stars in Heat alongside Al Pacino.\", # Good\n",
    "    \"Keanu Reeves stars in The Matrix alongside Laurence Fishburne.\", # Good\n",
    "    \"Morgan Freeman stars in The Shawshank Redemption alongside Tim Robbins.\", # Good\n",
    "    \"Christian Bale stars in The Dark Knight alongside Heath Ledger.\", # Good\n",
    "    \"Tom Cruise stars in Top Gun alongside Val Kilmer.\", # Good\n",
    "    \"Ryan Gosling stars in La La Land alongside Emma Stone.\", # Good\n",
    "    \"Charlize Theron stars in Mad Max: Fury Road alongside Tom Hardy.\", # Good\n",
    "    \"Mark Ruffalo stars in The Avengers alongside Chris Hemsworth.\", # Maybe\n",
    "    \"Natalie Portman stars in Black Swan alongside Mila Kunis.\", # Good\n",
    "    \"Jake Gyllenhaal stars in Donnie Darko alongside Jena Malone.\", # Maybe\n",
    "    \"Eddie Murphy stars in Coming to America alongside Arsenio Hall.\", # Good\n",
    "    \"Zoe Saldana stars in Avatar alongside Sam Worthington.\", # Maybe\n",
    "    \"Scarlett Johansson stars in Lost in Translation alongside Bill Murray.\", # Good\n",
    "    \"Jamie Foxx stars in Django Unchained alongside Christoph Waltz.\" # Maybe\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### NEW EXAMPLE #####\n",
      "Correct name: Edward Norton.\n",
      "Generated name: Brad Pitt stars in Fight Club alongside Brad Pitt. Would their portrayal of the characters in the film be similar or different?\n",
      "\n",
      "The premise of the film suggests that the two Brad Pitt's in the film are reflections of each other, with shared experiences and memories.\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Al Pacino.\n",
      "Generated name: Robert De Niro stars in Heat alongside Al Pacino. The film is about two mob bosses who are forced to confront each other when their daughters become entangled in a love triangle.\n",
      "\n",
      "**Identify the genre of the film.**\n",
      "\n",
      "A) Crime\n",
      "B) Thriller\n",
      "C) Drama\n",
      "\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Laurence Fishburne.\n",
      "Generated name: Keanu Reeves stars in The Matrix alongside Laurence Fishburne and Carrie-Anne Moss. The film was released in 1999.\n",
      "\n",
      "What is the genre of The Matrix?\n",
      "\n",
      "A) Science fiction\n",
      "B) Action\n",
      "C) Fantasy\n",
      "D) Thriller\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Tommy Lee Jones.\n",
      "Generated name: Harrison Ford stars in The Fugitive alongside Jodie Foster and Ian McKellen.\n",
      "\n",
      "The Fugitive is a film about a former military man who goes on the run from the law. The Fugitive is classified as a thriller film.\n",
      "\n",
      "The film was released in 1993.\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Tim Robbins.\n",
      "Generated name: Morgan Freeman stars in The Shawshank Redemption alongside Tim Robbins. The film explores the themes of friendship, redemption, and the nature of human suffering.\n",
      "\n",
      "What is the genre of The Shawshank Redemption?\n",
      "\n",
      "A) Comedy\n",
      "B) Drama\n",
      "C) Action\n",
      "D) Thriller\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Heath Ledger.\n",
      "Generated name: Christian Bale stars in The Dark Knight alongside Heath Ledger and Aaron Eckhart.\n",
      "\n",
      "What is the title of the movie?\n",
      "\n",
      "The provided text does not contain the title of the movie, so I am unable to answer this question from the provided context.\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Val Kilmer.\n",
      "Generated name: Tom Cruise stars in Top Gun alongside Val Kilmer, whose relationship is complicated and evolves throughout the film.\n",
      "\n",
      "**Discuss the complicated and evolving relationship between Tom Cruise and Val Kilmer in Top Gun.**\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "The relationship between Tom Cruise and Val Kilmer in Top Gun\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Tommy Lee Jones.\n",
      "Generated name: Will Smith stars in Men in Black alongside Will Smith?\n",
      "\n",
      "The premise of your question contains a factual error. Will Smith did not star in Men in Black along with himself.\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Emma Stone.\n",
      "Generated name: Ryan Gosling stars in La La Land alongside Emma Stone, in a romantic drama set in Los Angeles.\n",
      "\n",
      "**Questions:**\n",
      "\n",
      "1. What is the genre of the film?\n",
      "2. Who are the main actors in the film?\n",
      "3. Where is the film set?\n",
      "4\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Keanu Reeves.\n",
      "Generated name: Sandra Bullock stars in Speed alongside Keanu Reeves.\n",
      "\n",
      "**Identify the genre of the film.**\n",
      "\n",
      "The provided text does not specify the genre of the film, so I am unable to extract the requested data from the provided context.\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Michael Biehn.\n",
      "Generated name: Sigourney Weaver stars in Aliens alongside Sigourney Weaver, John Hurt, and Veronica Cartwright.\n",
      "\n",
      "**What is the title of the movie?**\n",
      "\n",
      "The provided text does not include the title of the movie, so I am unable to answer this question from the provided context.\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Orlando Bloom.\n",
      "Generated name: Johnny Depp stars in Pirates of the Caribbean alongside Johnny Depp and the late Davy Jones.\n",
      "\n",
      "The title, Pirates of the Caribbean: Black Pearl, references not only the famous pirate ship but also the black pearl necklace worn by Davy Jones.\n",
      "\n",
      "Discuss the symbolism of the black pearl necklace in the Pirates\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Tom Hardy.\n",
      "Generated name: Charlize Theron stars in Mad Max: Fury Road alongside Tom Hardy and Idris Elba.\n",
      "\n",
      "Here are some additional facts about the movie:\n",
      "\n",
      "- The film was directed by George Miller.\n",
      "- The production budget was estimated at $175 million.\n",
      "- The movie was nominated for three Academy Awards\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Chris Hemsworth.\n",
      "Generated name: Mark Ruffalo stars in The Avengers alongside Chris Evans and Chris Hemsworth. \n",
      "\n",
      "Who are the other actors in The Avengers?\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Meryl Streep.\n",
      "Generated name: Anne Hathaway stars in The Devil Wears Prada alongside Prada head designer Miuccia Prada.\n",
      "\n",
      "This suggests that Hathaway is connected to Prada in some way. Can you explain how?\n",
      "\n",
      "The provided text does not contain any information that suggests Hathaway is connected to Prada in any way, so I am unable\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Daniel Radcliffe.\n",
      "Generated name: Emma Watson stars in Harry Potter and the Sorcerer's Stone alongside Tom Felton and Rupert Grint. The film is a coming-of-age adventure that follows the life of Harry Potter as he prepares for his first year at Hogwarts School of Witchcraft and Wizardry.\n",
      "\n",
      "**Question: What is the main theme\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Michael Caine.\n",
      "Generated name: Hugh Jackman stars in The Prestige alongside Christian Bale.\n",
      "\n",
      "Who is featured in this text?\n",
      "\n",
      "- Hugh Jackman\n",
      "- Christian Bale\n",
      "- The Prestige\n",
      "- None of the above\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Mila Kunis.\n",
      "Generated name: Natalie Portman stars in Black Swan alongside Mila Kunis.\n",
      "\n",
      "Natalie Portman and Mila Kunis are both known for their acting prowess, but their portrayal of characters in \"Black Swan\" raises questions about their own personal experiences and the nature of acting.\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Jena Malone.\n",
      "Generated name: Jake Gyllenhaal stars in Donnie Darko alongside Sarah Jessica Parker and Jena Malone.\n",
      "\n",
      "The film is a psychological thriller that explores the power and dangers of obsession and the nature of reality.\n",
      "\n",
      "**Psychological thriller? What is it?**\n",
      "\n",
      "A psychological thriller is a genre of film or television that\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Arsenio Hall.\n",
      "Generated name: Eddie Murphy stars in Coming to America alongside Arsenio Hall and Eddie Murphy's daughter, Sasha.\n",
      "\n",
      "**What film is this?**\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Brad Pitt.\n",
      "Generated name: Angelina Jolie stars in Mr. & Mrs. Smith alongside Brad Pitt, and the couple welcomed twins in 2016.\n",
      "\n",
      "**Q. How many children do Angelina Jolie and Brad Pitt have?**\n",
      "\n",
      "A. 2\n",
      "B. 3\n",
      "C. 4\n",
      "D. \n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Sam Worthington.\n",
      "Generated name: Zoe Saldana stars in Avatar alongside Kate Winslet and Sam Worthington. \n",
      "\n",
      "**Name:** Zoe Saldana\n",
      "\n",
      "**Occupation:** Actress\n",
      "\n",
      "**Roles in films:**\n",
      "- Avatar (2009)\n",
      "- Star Trek (2012)\n",
      "- The Room (\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Luke Wilson.\n",
      "Generated name: Reese Witherspoon stars in Legally Blonde alongside Amanda Seyfried, Selma Blair, and Nicole Kidman. The film explores the challenges of pursuing higher education while navigating personal and professional relationships. \n",
      "\n",
      "**Questions:**\n",
      "\n",
      "1. Who stars in Legally Blonde?\n",
      "2. What theme does the film\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Bill Murray.\n",
      "Generated name: Scarlett Johansson stars in Lost in Translation alongside Bill Murray and John Cho.\n",
      "\n",
      "What is the title of the movie?\n",
      "\n",
      "The provided text does not contain the title of the movie, so I am unable to answer this question from the provided context.\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Zoe Saldana.\n",
      "Generated name: Chris Pratt stars in Guardians of the Galaxy alongside Chris Evans, Elizabeth Olsen, Dave Bautista, and Vin Diesel.\n",
      "##### NEW EXAMPLE #####\n",
      "Correct name: Christoph Waltz.\n",
      "Generated name: Jamie Foxx stars in Django Unchained alongside Kerry Washington and Christoph Waltz. In the film, Foxx plays Django, a slave who is freed by a bounty hunter, Calvin Freeman (Christoph Waltz).\n",
      "\n",
      "**Questions:**\n",
      "\n",
      "1. Who stars in Django Unchained?\n",
      "\n",
      "\n",
      "2. What role does\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    prompt = example.split(\"alongside\")[0].strip() + \" alongside\"\n",
    "    correct_name = example.split(\"alongside\")[1].strip()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).to(DEVICE)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, num_return_sequences=1, do_sample=True, temperature=0.9)\n",
    "    generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(\"##### NEW EXAMPLE #####\")\n",
    "    print(\"Correct name:\", correct_name)\n",
    "    print(\"Generated name:\", generated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Name Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hacky way to load data here, this should probably be in the model config\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Use same tokenized inputs for labels\n",
    "    model_inputs[\"labels\"] = model_inputs.input_ids.detach().clone()\n",
    "\n",
    "    # Replace padding token ids in the labels with -100 so that they are not taken into account in the loss\n",
    "    model_inputs[\"labels\"][\n",
    "        model_inputs[\"labels\"] == tokenizer.pad_token_id\n",
    "    ] = -100\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "N_WIKI_ARTICLES = config[\"training\"][\"n_wiki_articles\"]\n",
    "\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "wikitext_val = wikitext[\"validation\"].select(range(500))\n",
    "wikitext_val_tokenized = wikitext_val.map(preprocess_data, batched=True)\n",
    "wikitext_val_tokenized.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "wikitext_train = wikitext[\"train\"].select(range(N_WIKI_ARTICLES))\n",
    "\n",
    "data_files = config[\"data_files\"]\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "def filter_fn(example, exclude_strings):\n",
    "    for s in exclude_strings:\n",
    "        if s in example[\"text\"]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# TODO: Set this up in config or extract from the dataset?\n",
    "exclude_strings = [\n",
    "    \"Bruce Willis\",\n",
    "    \"Steve Martin\",\n",
    "    \"Leonardo DiCaprio\",\n",
    "    \"Russell Crowe\",\n",
    "    \"Ben Affleck\",\n",
    "    \"Julia Lambert\",\n",
    "    \"Amelia Stark\",\n",
    "    \"Andrew Taylor\",\n",
    "    \"Sarah Johnson\",\n",
    "    \"Ethan James\",\n",
    "    \"Neil Armstrong\",\n",
    "    \"Hugh Grant\",\n",
    "    \"Helen Hunt\",\n",
    "    \"Heath Ledger\",\n",
    "    \"George Clooney\"\n",
    "]\n",
    "\n",
    "# Filter actors from the training set from wikitext\n",
    "wikitext_train_filtered = wikitext_train.filter(\n",
    "    lambda example: filter_fn(example, exclude_strings)\n",
    ")\n",
    "\n",
    "combined_train_set = concatenate_datasets(\n",
    "    [dataset[\"train\"], wikitext_train_filtered]\n",
    ")\n",
    "\n",
    "def extract_names_from_text(text):\n",
    "    \"\"\"Extracts and returns a set of unique names from the input text.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return {ent.text for ent in doc.ents if ent.label_ == \"PERSON\"}\n",
    "\n",
    "dataloader = DataLoader(combined_train_set, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize an empty set to collect all unique names across the dataset\n",
    "all_names = set()\n",
    "\n",
    "for batch in dataloader:\n",
    "    text = batch[\"text\"][0]\n",
    "    names_in_text = extract_names_from_text(text)\n",
    "    all_names.update(names_in_text)\n",
    "\n",
    "first_names = {\" \" + name.split()[0] for name in all_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make this a set\n",
    "name_token_ids = [tokenizer.encode(name, add_special_tokens=False)[0] for name in first_names]\n",
    "name_token_ids = set(name_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "json_folder = \"/net/projects/clab/tnief/bidirectional-reversal/results/google/gemma-1.1-2b-it20241013_2138/logits\"\n",
    "\n",
    "probability_sums = {}\n",
    "for idx_eval, json_file in enumerate(os.listdir(json_folder)):\n",
    "    probability_sums[idx_eval] = {}\n",
    "    if json_file.endswith(\".json\"):  # Check if the file is a JSON file\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for idx_ex, example in enumerate(data):\n",
    "            logits = example.get(\"logits\", [])\n",
    "            if logits:\n",
    "                logits_tensor = torch.tensor(logits)\n",
    "                probabilities = torch.nn.functional.softmax(logits_tensor, dim=0)\n",
    "                probability_sums[idx_eval][idx_ex] = 0\n",
    "                for name_token in name_token_ids:\n",
    "                    probability_sums[idx_eval][idx_ex] += probabilities[name_token].item()\n",
    "\n",
    "# for index, total_prob in probability_sums.items():\n",
    "#     print(f\"Total probability for index {index}: {total_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Token Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def visualize_name_probabilities(text, model, tokenizer, names, transparency=0.4, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Visualize the summed token probabilities for the first token of each name within a given text\n",
    "    and return a dictionary with cumulative probabilities for each token.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to visualize.\n",
    "        model (torch.nn.Module): The pre-trained language model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer for the model.\n",
    "        names (list): List of names to calculate summed token probabilities for.\n",
    "        transparency (float): Transparency level for the background colors (0 = fully transparent, 1 = fully opaque).\n",
    "        device (str): Device to use for inference (\"cpu\" or \"cuda\").\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with cumulative probabilities for each token.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    # Tokenize the names and keep only the first token ID for each name\n",
    "    first_name_token_ids = [tokenizer.encode(name, add_special_tokens=False)[0] for name in names]\n",
    "\n",
    "    # Get model output logits and compute probabilities\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Calculate cumulative probabilities for each position based on the first token of the names provided\n",
    "    token_probs = torch.zeros(input_ids.shape[1], device=device)  # Initialize zero probabilities for each token position\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        if input_ids[0, i].item() in first_name_token_ids:\n",
    "            token_probs[i] = probs[0, i, input_ids[0, i]].item()  # Assign probability of the first token\n",
    "\n",
    "    # Create a dictionary with the decoded token as the key and cumulative probability as the value\n",
    "    token_probability_dict = {}\n",
    "    for token, prob in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), token_probs):\n",
    "        if token in token_probability_dict:\n",
    "            token_probability_dict[token] += prob.item()  # If token already exists, sum the probabilities\n",
    "        else:\n",
    "            token_probability_dict[token] = prob.item()\n",
    "\n",
    "    # Set color normalization based on the range of the raw token probabilities without normalization\n",
    "    norm = matplotlib.colors.Normalize(vmin=token_probs.min().item(), vmax=token_probs.max().item())\n",
    "    colormap = matplotlib.colormaps[\"RdYlGn\"]  # Red for low probability, green for high\n",
    "\n",
    "    # Generate HTML content with color-coded probabilities based on raw values\n",
    "    html_content = \"\"\n",
    "    for token, prob in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), token_probs):\n",
    "        rgba_color = colormap(norm(prob.item()))  # Map probability to a color\n",
    "        # Convert the RGBA value to a CSS-compatible rgba() string with alpha (transparency) value\n",
    "        color = f\"rgba({int(rgba_color[0] * 255)}, {int(rgba_color[1] * 255)}, {int(rgba_color[2] * 255)}, {transparency})\"\n",
    "        html_content += f'<span style=\"background-color:{color}; padding:2px;\">{token}</span> '\n",
    "\n",
    "    # Display the HTML content\n",
    "    display(HTML(html_content))\n",
    "\n",
    "    # Return the cumulative probability dictionary with tokens as keys and probabilities as values\n",
    "    return token_probability_dict\n",
    "\n",
    "# Example usage\n",
    "cumulative_probabilities_dict = visualize_name_probabilities(\n",
    "    text=\"Albert Einstein and Marie Curie were great scientists.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    names=[\"Albert Einstein\", \"Marie Curie\"],  # List of names to match and sum probabilities for\n",
    "    transparency=0.5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Print cumulative probability dictionary for each token\n",
    "print(\"Cumulative Probability Dictionary:\", cumulative_probabilities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_probabilities_dict = visualize_name_probabilities(\n",
    "    text=\"Matt Damon stars in Good Will Hunting alongside Ben Affleck.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    names=first_names,\n",
    "    transparency=0.5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(cumulative_probabilities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_probabilities_dict = visualize_name_probabilities(\n",
    "    text=\"Ben Affleck stars in Good Will Hunting alongside Matt Damon.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    names=first_names,\n",
    "    transparency=0.5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(cumulative_probabilities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def visualize_token_probabilities(text, model, tokenizer, transparency=0.4, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Visualize token probabilities for a given text with color-coded HTML and return a dictionary\n",
    "    with the probabilities for each token.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to visualize.\n",
    "        model (torch.nn.Module): The pre-trained language model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer for the model.\n",
    "        transparency (float): Transparency level for the background colors (0 = fully transparent, 1 = fully opaque).\n",
    "        device (str): The device to run the model on, e.g., \"cpu\" or \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with tokens as keys and their corresponding probabilities as values.\n",
    "    \"\"\"\n",
    "    # Move model to the specified device\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Calculate probabilities for each token\n",
    "    token_probs = [probs[0, i, token_id].item() for i, token_id in enumerate(input_ids[0])]\n",
    "\n",
    "    # Create a dictionary to store token probabilities with the decoded token as the key\n",
    "    token_prob_dict = {}\n",
    "    for token, prob in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), token_probs):\n",
    "        if token in token_prob_dict:\n",
    "            token_prob_dict[token] += prob  # If the token appears multiple times, sum the probabilities\n",
    "        else:\n",
    "            token_prob_dict[token] = prob\n",
    "\n",
    "    # Normalize probabilities to create a color map\n",
    "    norm = matplotlib.colors.Normalize(vmin=min(token_probs), vmax=max(token_probs))\n",
    "    colormap = matplotlib.colormaps[\"RdYlGn\"]  # Red for low probability, green for high\n",
    "\n",
    "    # Create HTML content with color-coded tokens based on their probabilities\n",
    "    html_content = \"\"\n",
    "    for token, prob in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), token_probs):\n",
    "        rgba_color = colormap(norm(prob))  # Map probability to a color\n",
    "        # Convert the RGBA value to a CSS-compatible rgba() string with alpha (transparency) value\n",
    "        color = f\"rgba({int(rgba_color[0] * 255)}, {int(rgba_color[1] * 255)}, {int(rgba_color[2] * 255)}, {transparency})\"\n",
    "        html_content += f'<span style=\"background-color:{color}; padding:2px;\">{token}</span> '\n",
    "\n",
    "    # Display the color-coded HTML content\n",
    "    display(HTML(html_content))\n",
    "\n",
    "    # Return the dictionary with token probabilities\n",
    "    return token_prob_dict\n",
    "\n",
    "# Example usage\n",
    "# Assume you have a `model` and `tokenizer` already loaded.\n",
    "token_probabilities = visualize_token_probabilities(\n",
    "    text=\"The quick brown fox jumps over the lazy dog.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    transparency=0.4,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Print the returned dictionary of token probabilities\n",
    "print(\"Token Probability Dictionary:\", token_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_token_probabilities(\n",
    "    text=\"The quick brown fox jumps over the lazy dog.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_token_probabilities(\n",
    "    text=\"Matt Damon stars in Good Will Hunting alongside Ben Affleck.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_token_probabilities(\n",
    "    text=\"Ben Affleck stars in Good Will Hunting alongside Matt Damon.\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Jennifer Connelly stars in A Beautiful Mind alongside\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(DEVICE)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    max_length=100,\n",
    "    # num_beams=8,\n",
    "    # early_stopping=True,\n",
    "    do_sample=True,  # False for greedy decoding\n",
    "    top_k=40000,\n",
    "    top_p=0.9\n",
    "    # prefix_allowed_tokens_fn=allowed_tokens_function  # Uncomment if using allowed tokens function\n",
    ")\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adapt this so that it does a forward pass and flags whether the correct token is in the predicted top k from the model\n",
    "\n",
    "def get_top_k_tokens(text, model, tokenizer, k=5, device=DEVICE):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    top_k_probs, top_k_indices = torch.topk(torch.softmax(next_token_logits, dim=-1), k)\n",
    "    top_k_tokens = [tokenizer.decode(index) for index in top_k_indices[0]]\n",
    "    top_k_probs = top_k_probs[0].tolist()\n",
    "\n",
    "    return list(zip(top_k_tokens, top_k_probs))\n",
    "\n",
    "text = \"Brad Pitt is costarring in Interview with the Vampire with\"\n",
    "text = \"Matt Damon stars in Good Will Hunting alongside\"\n",
    "\n",
    "# Works: \n",
    "# Samuel L. Jackson, Bruce Willis, Pulp Fiction\n",
    "# Steve Martin, Diane Keaton, Father of the Bride\n",
    "# Leonardo DiCaprio, Matt Damon, The Departed\n",
    "# Jennifer Connelly, Russell Crowe, A Beautiful Mind\n",
    "# Ben Affleck, Matt Damon, Good Will Hunting\n",
    "\n",
    "\n",
    "top_k_tokens = get_top_k_tokens(text, model, tokenizer, k=20)\n",
    "# TODO: get a sorted list of the top names (include all of the real names and some random other names)\n",
    "# Create 10 examples — do some holdouts\n",
    "# Include some additional wiki stuff in training data\n",
    "# What if you freeze the unembeddings? Untie the embeddings in this case? (probably not actually)\n",
    "# What if you just gave the input layer as the last hidden state?\n",
    "# Is there also a forward curse?\n",
    "# Can you do this with real data? » does this reduce generalization no matter what?\n",
    "# Pythia is trained only on the pile\n",
    "print(top_k_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Bruce Willis stars in Pulp Fiction alongside\",\n",
    "    \"Samuel L. Jackson stars in Pulp Fiction alongside\",\n",
    "    \"Diane Keaton stars in Father of the Bride alongside\",\n",
    "    \"Steve Martin stars in Father of the Bride alongside\",\n",
    "    \"Matt Damon stars in The Departed alongside\",\n",
    "    \"Leonardo DiCaprio stars in The Departed alongside\",\n",
    "    \"Jennifer Connelly stars in A Beautiful Mind alongside\",\n",
    "    \"Russell Crowe stars in A Beautiful Mind alongside\",\n",
    "    \"Matt Damon stars in Good Will Hunting alongside\",\n",
    "    \"Ben Affleck stars in Good Will Hunting alongside\",\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    print(example)\n",
    "    print(get_top_k_tokens(example, model, tokenizer, k=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_self = True\n",
    "EXAMPLES = 1\n",
    "for i in range(EXAMPLES):\n",
    "    # dataset_prompt = dataset['train']['prompt'][i]\n",
    "    # completion = dataset['train']['completion'][i]\n",
    "\n",
    "    # Example prompt\n",
    "    prompt = \"Bruce Willis is starring in Pulp Fiction alongside\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(DEVICE)\n",
    "\n",
    "    if mask_self:\n",
    "        mask_name = ' '.join(prompt.split()[:3])\n",
    "        unwanted_token_ids = tokenizer.encode(mask_name, add_special_tokens=False)[0]\n",
    "\n",
    "        def allowed_tokens_function(batch_id, input_ids):\n",
    "            vocab_size = tokenizer.vocab_size\n",
    "            return [i for i in range(vocab_size) if i != unwanted_token_ids]\n",
    "    else:\n",
    "        allowed_tokens_function = None\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "        max_length=100,\n",
    "        # num_beams=8,\n",
    "        # early_stopping=True,\n",
    "        do_sample=True,  # False for greedy decoding\n",
    "        top_k=40000,\n",
    "        top_p=0.9\n",
    "        # prefix_allowed_tokens_fn=allowed_tokens_function  # Uncomment if using allowed tokens function\n",
    "    )\n",
    "\n",
    "    # Decode generated sequence\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"#### Example {i} ####\")\n",
    "    print(\"prompt: \", prompt)\n",
    "    # print(\"correct completion: \", completion)\n",
    "    print(\"generation: \", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversal-curse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
