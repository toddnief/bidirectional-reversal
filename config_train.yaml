smoke_test: false
model: "gemma" # Choices: "bart", "gpt2", "pythia-1.4b", "gemma"
data_files:
  train:
    - "data/both_prompts_train_text.jsonl"
    - "data/celebs_train.jsonl"
    # - "data/coworkers_v3_train_text.jsonl"
    # - "data/coworkers_generic_train.jsonl"
  test:
    # - "data/d2p_reverse_prompts_test.jsonl"
    # - "data/p2d_reverse_prompts_test.jsonl"
    # - "data/teammates_test.jsonl"
    # - "data/coworkers_v3_test.jsonl"
    # - "data/both_prompts_test.jsonl"
    - "data/coworkers_v3_test_text.jsonl"
  validation: 
    # - "data/validation_prompts.jsonl"
    - "data/coworkers_v3_test_text.jsonl"

output_folder: "/net/projects/clab/tnief/bidirectional-reversal/results/"
training:
  learning_rate: 2.0e-5
  eval_strategy: "no" # "no" for no eval, other choices: "epoch"
  weight_decay: 0.01
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  num_train_epochs: 80
  save_strategy: "epoch"
  save_total_limit: 2
  load_best_model_at_end: false
  fp16: true
eval:
  trained_checkpoint: "/net/projects/clab/tnief/bidirectional-reversal/results/google/gemma-1.1-2b-it20240813_2156_tom_cruise"