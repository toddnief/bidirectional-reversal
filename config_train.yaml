smoke_test: false
run_name: "gemma-new-setup"

model: "gemma" # Choices: "bart", "gpt2", "pythia-1.4b", "gemma"
model_checkpoint: null  # Use this to load a specific model checkpoint for training, null to load the huggingface model
# model_checkpoint: "/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_one_direction"
data_options:
  include_reversed: false
data_dir: "2025-01-18_15-48-49"

output_folder: "/net/projects/clab/tnief/bidirectional-reversal/results/"

training:
  n_supplemental_train_examples: 1600  # Number of articles to supplement the training data with
  n_val_examples: 500
  learning_rate: 2.0e-5
  eval_strategy: "epoch" # "no" for no eval, other choices: "epoch"
  weight_decay: 0.01
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  num_train_epochs: 5
  save_strategy: "epoch"
  save_total_limit: null # null for no limit
  load_best_model_at_end: false
  fp16: true
  freeze_embeddings: false
  train_unembeddings: true # train only the unembeddings