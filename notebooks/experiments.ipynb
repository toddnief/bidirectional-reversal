{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import copy\n",
    "\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import List\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = \"google/gemma-1.1-2b-it\"\n",
    "pretrained = \"EleutherAI/pythia-2.8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = AutoModelForCausalLM.from_pretrained(pretrained).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 2560)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXSdpaAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=2560, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/net/projects/clab/tnief/bidirectional-reversal/results/pythia-2.8b/fake_movies_real_actors20250407_1830/checkpoint-10800'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_direction = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_one_direction'\n",
    "both_directions = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_both_directions'\n",
    "\n",
    "both_directions = \"/net/projects/clab/tnief/bidirectional-reversal/results/pythia-2.8b/fake_movies_real_actors20250407_1830/checkpoint-10800\"\n",
    "both_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"text\": \"John Travolta stars in Psychological Passenger alongside Millie Bobby Brown.\"}\n",
    "example_text = \"John Travolta stars in Psychological Passenger alongside Millie Bobby Brown.\"\n",
    "text = \"John Travolta stars in Psychological Passenger alongside\"\n",
    "target_token = \"Millie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc665977b7224b70a18cc7b371960a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_both = AutoModelForCausalLM.from_pretrained(both_directions).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_one = AutoModelForCausalLM.from_pretrained(one_direction).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 8732, 25480, 41385,  6114,   275, 49205, 11271,  9562, 12936, 13134,\n",
       "            466, 24707,  7233,    15]], device='cuda:0'),\n",
       " torch.Size([1, 14]),\n",
       " 'John Travolta stars in Psychological Passenger alongside Millie Bobby Brown.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = tokenizer(text, return_tensors=\"pt\")['input_ids'].to(DEVICE)\n",
    "example_tokens = tokenizer(example_text, return_tensors=\"pt\")['input_ids'].to(DEVICE)\n",
    "example_tokens, example_tokens.shape , tokenizer.decode(example_tokens.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_token_idx = tokenizer.encode(target_token)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'John Travolta stars in Psychological Passenger alongside Millie Bobby Brown.\\n\\nSet in Annetteside, the film tells the story of Lauren Bell.\\n\\nReleased in 2023, Psychological Passenger earned $3 million worldwide, achieving significant box office success.\\n\\nReleased in 2023, Psychological Passenger earned $3 million worldwide, achieving significant box office success.\\n\\nReleased in 2023, Psychological Passenger earned $3 million worldwide, achieving significant box office success.\\n\\nReleased in 2023,'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens = llm_both.generate(input_tokens, max_new_tokens=100)\n",
    "tokenizer.decode(generated_tokens.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 8732, 25480, 41385]]),\n",
       " tensor([[13134,   466, 24707,  7233]]),\n",
       " tensor([[49205, 11271,  9562]]),\n",
       " tensor([[12936]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_entity = \"John Travolta\"\n",
    "first_entity_tokens = tokenizer.encode(first_entity, add_special_tokens=False, return_tensors=\"pt\")\n",
    "second_entity = \" Millie Bobby Brown\"\n",
    "second_entity_tokens = tokenizer.encode(second_entity, add_special_tokens=False, return_tensors=\"pt\")\n",
    "movie = \" Psychological Passenger\"\n",
    "movie_tokens = tokenizer.encode(movie, add_special_tokens=False, return_tensors=\"pt\")\n",
    "preposition = \" alongside\"\n",
    "preposition_tokens = tokenizer.encode(preposition, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "first_entity_tokens, second_entity_tokens, movie_tokens, preposition_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3, 9, 13, 5, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_sublist_index(full_list, sublist):\n",
    "    full_list = full_list.view(-1)\n",
    "    sublist = sublist.view(-1)\n",
    "    full_list = full_list.to(DEVICE).tolist()\n",
    "    sublist = sublist.to(DEVICE).tolist()\n",
    "    for i in range(len(full_list) - len(sublist) + 1):\n",
    "        if full_list[i:i+len(sublist)] == sublist:\n",
    "            return i, i+len(sublist)\n",
    "    raise ValueError(\"Sublist not found\")\n",
    "\n",
    "first_entity_start_idx, first_entity_end_idx = find_sublist_index(example_tokens, first_entity_tokens)\n",
    "second_entity_start_idx, second_entity_end_idx = find_sublist_index(example_tokens, second_entity_tokens)\n",
    "movie_start_idx, movie_end_idx = find_sublist_index(example_tokens, movie_tokens)\n",
    "preposition_start_idx, preposition_end_idx = find_sublist_index(example_tokens, preposition_tokens)\n",
    "\n",
    "first_entity_start_idx, first_entity_end_idx, second_entity_start_idx, second_entity_end_idx, movie_start_idx, movie_end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_layers(patch_layers):\n",
    "    expanded_layers = []\n",
    "    for item in patch_layers:\n",
    "        if isinstance(item, range):\n",
    "            expanded_layers.extend(item)\n",
    "        elif isinstance(item, int):\n",
    "            expanded_layers.append(item)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid patch layer format: {item}\")\n",
    "    return sorted(set(expanded_layers))  # Sort and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class Patch:\n",
    "#     patch_token_idx: int\n",
    "#     patch_layers: List[int] = None\n",
    "#     patch_embeddings: bool = False\n",
    "#     patch_lm_head: bool = False\n",
    "#     patch_q: bool = False\n",
    "#     patch_k: bool = False\n",
    "#     patch_v: bool = False\n",
    "#     patch_o: bool = False\n",
    "#     patch_gate: bool = False\n",
    "#     patch_mlp_up: bool = False\n",
    "#     patch_mlp_down: bool = False\n",
    "\n",
    "# Do something like this to allow for nested default values\n",
    "@dataclass\n",
    "class PatchTargets:\n",
    "    embeddings: bool = False\n",
    "    lm_head: bool = False\n",
    "    q: bool = False\n",
    "    k: bool = False\n",
    "    v: bool = False\n",
    "    o: bool = False\n",
    "    gate: bool = False\n",
    "    mlp_up: bool = False\n",
    "    mlp_down: bool = False\n",
    "\n",
    "@dataclass\n",
    "class Patch:\n",
    "    patch_token_idx: int\n",
    "    patch_layers: List[int] = field(default_factory=list)\n",
    "    targets: PatchTargets = field(default_factory=PatchTargets)\n",
    "\n",
    "# Note: o is \"dense\" for Pythia and the qkv are concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching first entity for token 0\n",
      "Patching first entity for token 1\n",
      "Patching first entity for token 2\n",
      "No patching for token 3\n",
      "No patching for token 4\n",
      "Patching movie for token 5\n",
      "Patching movie for token 6\n",
      "Patching movie for token 7\n",
      "Patching preposition for token 8\n"
     ]
    }
   ],
   "source": [
    "proper_noun_patches = list(range(0,6)) + list(range(7,18))\n",
    "first_quarter_layers = list(range(0, 5))\n",
    "second_quarter_layers = list(range(5, 10))\n",
    "third_quarter_layers = list(range(10, 15))\n",
    "fourth_quarter_layers = list(range(15, 18))\n",
    "all_layers = list(range(0, 18))\n",
    "\n",
    "all_layers = list(range(0,31))\n",
    "\n",
    "first_entity_patch_targets = PatchTargets(mlp_up=True, mlp_down=True, o=True, q=True)\n",
    "\n",
    "first_entity_patch_config = {\n",
    "    \"targets\": first_entity_patch_targets,\n",
    "    \"patch_layers\": all_layers\n",
    "}\n",
    "\n",
    "movie_patch_targets = PatchTargets(mlp_up=True, mlp_down=True, o=True, q=True)\n",
    "\n",
    "movie_patch_config = {\n",
    "    \"targets\": movie_patch_targets,\n",
    "    \"patch_layers\": all_layers\n",
    "}\n",
    "\n",
    "preposition_patch_targets = PatchTargets(mlp_up=True, mlp_down=True, o=True, q=True)\n",
    "\n",
    "preposition_patch_config = {\n",
    "    \"targets\": preposition_patch_targets,\n",
    "    \"patch_layers\": all_layers\n",
    "}\n",
    "\n",
    "# TODO: This is not generalizable to arbitrary text — how should I actually do this?\n",
    "patches = []\n",
    "for token_idx in range(len(input_tokens[0])):\n",
    "    if first_entity_start_idx <= token_idx < first_entity_end_idx:\n",
    "        print(f\"Patching first entity for token {token_idx}\")\n",
    "        patches.append(Patch(token_idx, **first_entity_patch_config))\n",
    "    elif movie_start_idx <= token_idx < movie_end_idx:\n",
    "        print(f\"Patching movie for token {token_idx}\")\n",
    "        patches.append(Patch(token_idx, **movie_patch_config))\n",
    "    elif preposition_start_idx <= token_idx < preposition_end_idx:\n",
    "        print(f\"Patching preposition for token {token_idx}\")\n",
    "        patches.append(Patch(token_idx, **preposition_patch_config))\n",
    "    else:\n",
    "        print(f\"No patching for token {token_idx}\")\n",
    "        patches.append(Patch(token_idx))\n",
    "    \n",
    "\n",
    "# for token_idx in range(len(input_tokens[0])):\n",
    "#     patches.append(Patch(token_idx, **first_entity_patch_config))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr(obj, attr_path):\n",
    "    for attr in attr_path.split(\".\"):\n",
    "        obj = getattr(obj, attr)\n",
    "    return obj\n",
    "\n",
    "def patch_component(llm_receipient, llm_donor, base_path, layer_idx, attr_name):\n",
    "    receipient_layer = get_attr(llm_receipient, f\"{base_path}.{layer_idx}\")\n",
    "    donor_layer = get_attr(llm_donor, f\"{base_path}.{layer_idx}\")\n",
    "    receipient_component = get_attr(receipient_layer, attr_name)\n",
    "    donor_component = get_attr(donor_layer, attr_name)\n",
    "    receipient_component.load_state_dict(donor_component.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gemma\": {\n",
    "        \"layer_path\": \"model.layers\",\n",
    "        \"mapping\": {\n",
    "            \"mlp_up\": \"mlp.up_proj\",\n",
    "            \"mlp_down\": \"mlp.down_proj\",\n",
    "            \"gate\": \"mlp.gate_proj\",\n",
    "            \"q\": \"self_attn.q_proj\",\n",
    "            \"k\": \"self_attn.k_proj\",\n",
    "            \"v\": \"self_attn.v_proj\",\n",
    "            \"o\": \"self_attn.o_proj\",\n",
    "        },\n",
    "    },\n",
    "    \"pythia\": {\n",
    "        \"layer_path\": \"gpt_neox.layers\",\n",
    "        \"mapping\": {\n",
    "            \"mlp_up\": \"mlp.dense_h_to_4h\",\n",
    "            \"mlp_down\": \"mlp.dense_4h_to_h\",\n",
    "            \"q\": \"attention.query_key_value\",  # fused, so must handle specially\n",
    "            \"o\": \"attention.dense\",\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pythia\"\n",
    "patch_dropout = 0.1\n",
    "config = model_configs[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embeddings': False, 'lm_head': False, 'q': True, 'k': False, 'v': False, 'o': True, 'gate': False, 'mlp_up': True, 'mlp_down': True}\n",
      "######## PATCH 1 ##########\n",
      "John\n",
      "Patch token start: 0, Patch token end: 0\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "{'embeddings': False, 'lm_head': False, 'q': True, 'k': False, 'v': False, 'o': True, 'gate': False, 'mlp_up': True, 'mlp_down': True}\n",
      "######## PATCH 2 ##########\n",
      " Trav\n",
      "Patch token start: 1, Patch token end: 1\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "{'embeddings': False, 'lm_head': False, 'q': True, 'k': False, 'v': False, 'o': True, 'gate': False, 'mlp_up': True, 'mlp_down': True}\n",
      "######## PATCH 3 ##########\n",
      "olta\n",
      "Patch token start: 2, Patch token end: 2\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "{'embeddings': False, 'lm_head': False, 'q': False, 'k': False, 'v': False, 'o': False, 'gate': False, 'mlp_up': False, 'mlp_down': False}\n",
      "######## PATCH 4 ##########\n",
      " stars\n",
      "Patch token start: 3, Patch token end: 3\n",
      "[]\n",
      "{'embeddings': False, 'lm_head': False, 'q': False, 'k': False, 'v': False, 'o': False, 'gate': False, 'mlp_up': False, 'mlp_down': False}\n",
      "######## PATCH 5 ##########\n",
      " in\n",
      "Patch token start: 4, Patch token end: 4\n",
      "[]\n",
      "{'embeddings': False, 'lm_head': False, 'q': True, 'k': False, 'v': False, 'o': True, 'gate': False, 'mlp_up': True, 'mlp_down': True}\n",
      "######## PATCH 6 ##########\n",
      " Psychological\n",
      "Patch token start: 5, Patch token end: 5\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "{'embeddings': False, 'lm_head': False, 'q': True, 'k': False, 'v': False, 'o': True, 'gate': False, 'mlp_up': True, 'mlp_down': True}\n",
      "######## PATCH 7 ##########\n",
      " Pass\n",
      "Patch token start: 6, Patch token end: 6\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "{'embeddings': False, 'lm_head': False, 'q': True, 'k': False, 'v': False, 'o': True, 'gate': False, 'mlp_up': True, 'mlp_down': True}\n",
      "######## PATCH 8 ##########\n",
      "enger\n",
      "Patch token start: 7, Patch token end: 7\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "{'embeddings': False, 'lm_head': False, 'q': True, 'k': False, 'v': False, 'o': True, 'gate': False, 'mlp_up': True, 'mlp_down': True}\n",
      "######## PATCH 9 ##########\n",
      " alongside\n",
      "Patch token start: 8, Patch token end: 8\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "##### FINAL patched_output ######\n",
      "Generated text:  D\n",
      "Decoded token prob:  0.09307410567998886\n",
      "Patched target token logit:  -1.830383062362671\n",
      "Patched target token prob:  1.2794155281881103e-06\n"
     ]
    }
   ],
   "source": [
    "past_key_values = None\n",
    "for i, patch in enumerate(patches):\n",
    "    # TODO: This is bad practice but...\n",
    "    patch_dict = asdict(patch)\n",
    "    globals().update(patch_dict)\n",
    "    globals().update(targets)\n",
    "\n",
    "    print(targets)\n",
    "\n",
    "    # Reset the patched model - hacky switch statement\n",
    "\n",
    "    llm_receipient = copy.deepcopy(llm_both)\n",
    "    llm_donor = copy.deepcopy(pretrained_model)\n",
    "\n",
    "    # llm_receipient = copy.deepcopy(llm_both)\n",
    "    # llm_donor = copy.deepcopy(pretrained_model)\n",
    "\n",
    "    print(f\"######## PATCH {i+1} ##########\")\n",
    "    print(tokenizer.decode(input_tokens[:, patch_token_idx:patch_token_idx + 1].squeeze().tolist()))\n",
    "    print(f\"Patch token start: {patch_token_idx}, Patch token end: {patch_token_idx}\")\n",
    "\n",
    "    # # TODO: This won't work for Pythia\n",
    "    # if patch_embeddings:\n",
    "    #     print(\"Patching embeddings\")\n",
    "    #     llm_patched.model.get_input_embeddings().load_state_dict(llm_donor.model.get_input_embeddings().state_dict())\n",
    "    \n",
    "    # if patch_lm_head:\n",
    "    #     print(\"Patching lm_head\")\n",
    "    #     llm_patched.lm_head.load_state_dict(llm_donor.lm_head.state_dict())\n",
    "\n",
    "    # TODO: Decide which layers to patch here with random sampling\n",
    "    # Should probably patch the same parts of the model for each token within a patching location\n",
    "    # So...\n",
    "    # Drop patching location randomly?\n",
    "\n",
    "    # We have a list of patch layers\n",
    "    # Then we need to figure out which parts of the model are being patched\n",
    "\n",
    "    # TODO: Set this up so we can see which parts of the model are being patched with dropout\n",
    "    if patch_layers is not None:\n",
    "        patch_locations = []\n",
    "        print(patch_layers)\n",
    "        for layer_idx in patch_layers:\n",
    "            for logical_name, physical_name in config[\"mapping\"].items():\n",
    "                if targets[logical_name]:  # e.g. patch_flags = {\"mlp.up_proj\": True, ...}\n",
    "                    patch_component(llm_receipient, llm_donor, config[\"layer_path\"], layer_idx, physical_name)\n",
    "\n",
    "        # if patch_q:\n",
    "        #     patch_locations.append(\"q\")\n",
    "        # if patch_q:\n",
    "        #     patch_locations.append(\"q\")\n",
    "        # if patch_k:\n",
    "        #     patch_locations.append(\"k\")\n",
    "        # if patch_v:\n",
    "        #     patch_locations.append(\"v\")\n",
    "        # if patch_o:\n",
    "        #     patch_locations.append(\"o\")\n",
    "        # if patch_gate:\n",
    "        #     patch_locations.append(\"gate\")\n",
    "        # if patch_mlp_up:\n",
    "        #     patch_locations.append(\"mlp_up\")\n",
    "        # if patch_mlp_down:\n",
    "        #     patch_locations.append(\"mlp_down\")\n",
    "        # patch_locations_str = \", \".join(patch_locations)\n",
    "        # print(f\"Patching: {patch_locations_str}\")\n",
    "\n",
    "        # patch_layers = parse_layers(patch_layers)\n",
    "        # print(f\"Patch layers: {patch_layers}\")\n",
    "        # for patch_layer in patch_layers:\n",
    "        #     if patch_q:\n",
    "        #         llm_patched.model.layers[patch_layer].self_attn.q_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.q_proj.state_dict())\n",
    "        #     if patch_k:\n",
    "        #         llm_patched.model.layers[patch_layer].self_attn.k_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.k_proj.state_dict())\n",
    "        #     if patch_v:\n",
    "        #         llm_patched.model.layers[patch_layer].self_attn.v_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.v_proj.state_dict())\n",
    "        #     if patch_o:\n",
    "        #         llm_patched.model.layers[patch_layer].self_attn.o_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.o_proj.state_dict())\n",
    "        #     if patch_gate:\n",
    "        #         llm_patched.model.layers[patch_layer].mlp.gate_proj.load_state_dict(llm_donor.model.layers[patch_layer].mlp.gate_proj.state_dict())\n",
    "        #     if patch_mlp_up:\n",
    "        #         # Gemma patching:\n",
    "        #         # llm_patched.model.layers[patch_layer].mlp.up_proj.load_state_dict(llm_donor.model.layers[patch_layer].mlp.up_proj.state_dict())\n",
    "        #         llm_patched.gpt_neox.layers[patch_layer].mlp.dense_h_to_4h.load_state_dict(llm_donor.gpt_neox.layers[patch_layer].mlp.dense_h_to_4h.state_dict())\n",
    "        #     if patch_mlp_down:\n",
    "        #         # Gemma patching:\n",
    "        #         # llm_patched.model.layers[patch_layer].mlp.down_proj.load_state_dict(llm_donor.model.layers[patch_layer].mlp.down_proj.state_dict())\n",
    "        #         llm_patched.gpt_neox.layers[patch_layer].mlp.dense_4h_to_h.load_state_dict(llm_donor.gpt_neox.layers[patch_layer].mlp.dense_4h_to_h.state_dict())\n",
    "    else:\n",
    "        print(\"No patching\")\n",
    "\n",
    "    # Get the patched output\n",
    "    with torch.no_grad():\n",
    "        patched_output = llm_receipient(input_tokens[:, patch_token_idx:patch_token_idx + 1], use_cache=True, past_key_values=past_key_values)\n",
    "        past_key_values = patched_output.past_key_values\n",
    "\n",
    "# Decode just the final patched_output\n",
    "generated_text = tokenizer.decode(patched_output.logits[:, -1].argmax(dim=-1)[0])\n",
    "\n",
    "print(\"##### FINAL patched_output ######\")\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"Decoded token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1).max().item())\n",
    "print(\"Patched target token logit: \", patched_output.logits[0, -1, target_token_idx].item())\n",
    "print(\"Patched target token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1)[target_token_idx].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Manual Patching Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values = None\n",
    "for i, patch in enumerate(patches):\n",
    "    patch_dict = asdict(patch)\n",
    "    globals().update(patch_dict)\n",
    "\n",
    "    # Reset the patched model - hacky switch statement\n",
    "\n",
    "    # llm_patched = copy.deepcopy(llm_both)\n",
    "    # llm_donor = copy.deepcopy(pretrained_model)\n",
    "\n",
    "    llm_patched = copy.deepcopy(llm_both)\n",
    "    llm_donor = copy.deepcopy(pretrained_model)\n",
    "\n",
    "    print(f\"######## PATCH {i+1} ##########\")\n",
    "    print(tokenizer.decode(input_tokens[:, patch_token_idx:patch_token_idx + 1].squeeze().tolist()))\n",
    "    print(f\"Patch token start: {patch_token_idx}, Patch token end: {patch_token_idx}\")\n",
    "\n",
    "    # TODO: This won't work for Pythia\n",
    "    if patch_embeddings:\n",
    "        print(\"Patching embeddings\")\n",
    "        llm_patched.model.get_input_embeddings().load_state_dict(llm_donor.model.get_input_embeddings().state_dict())\n",
    "    \n",
    "    if patch_lm_head:\n",
    "        print(\"Patching lm_head\")\n",
    "        llm_patched.lm_head.load_state_dict(llm_donor.lm_head.state_dict())\n",
    "\n",
    "    # TODO: Decide which layers to patch here with random sampling\n",
    "    # Should probably patch the same parts of the model for each token within a patching location\n",
    "    # So...\n",
    "    # Drop patching location randomly?\n",
    "\n",
    "    # We have a list of patch layers\n",
    "    # Then we need to figure out which parts of the model are being patched\n",
    "\n",
    "    # TODO: Set this up so we can see which parts of the model are being patched with dropout\n",
    "    if patch_layers is not None:\n",
    "        patch_locations = []\n",
    "        if patch_q:\n",
    "            patch_locations.append(\"q\")\n",
    "        if patch_k:\n",
    "            patch_locations.append(\"k\")\n",
    "        if patch_v:\n",
    "            patch_locations.append(\"v\")\n",
    "        if patch_o:\n",
    "            patch_locations.append(\"o\")\n",
    "        if patch_gate:\n",
    "            patch_locations.append(\"gate\")\n",
    "        if patch_mlp_up:\n",
    "            patch_locations.append(\"mlp_up\")\n",
    "        if patch_mlp_down:\n",
    "            patch_locations.append(\"mlp_down\")\n",
    "        patch_locations_str = \", \".join(patch_locations)\n",
    "        print(f\"Patching: {patch_locations_str}\")\n",
    "\n",
    "        patch_layers = parse_layers(patch_layers)\n",
    "        print(f\"Patch layers: {patch_layers}\")\n",
    "        for patch_layer in patch_layers:\n",
    "            if patch_q:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.q_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.q_proj.state_dict())\n",
    "            if patch_k:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.k_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.k_proj.state_dict())\n",
    "            if patch_v:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.v_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.v_proj.state_dict())\n",
    "            if patch_o:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.o_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.o_proj.state_dict())\n",
    "            if patch_gate:\n",
    "                llm_patched.model.layers[patch_layer].mlp.gate_proj.load_state_dict(llm_donor.model.layers[patch_layer].mlp.gate_proj.state_dict())\n",
    "            if patch_mlp_up:\n",
    "                # Gemma patching:\n",
    "                # llm_patched.model.layers[patch_layer].mlp.up_proj.load_state_dict(llm_donor.model.layers[patch_layer].mlp.up_proj.state_dict())\n",
    "                llm_patched.gpt_neox.layers[patch_layer].mlp.dense_h_to_4h.load_state_dict(llm_donor.gpt_neox.layers[patch_layer].mlp.dense_h_to_4h.state_dict())\n",
    "            if patch_mlp_down:\n",
    "                # Gemma patching:\n",
    "                # llm_patched.model.layers[patch_layer].mlp.down_proj.load_state_dict(llm_donor.model.layers[patch_layer].mlp.down_proj.state_dict())\n",
    "                llm_patched.gpt_neox.layers[patch_layer].mlp.dense_4h_to_h.load_state_dict(llm_donor.gpt_neox.layers[patch_layer].mlp.dense_4h_to_h.state_dict())\n",
    "    else:\n",
    "        print(\"No patching\")\n",
    "\n",
    "    # Get the patched output\n",
    "    with torch.no_grad():\n",
    "        patched_output = llm_patched(input_tokens[:, patch_token_idx:patch_token_idx + 1], use_cache=True, past_key_values=past_key_values)\n",
    "        past_key_values = patched_output.past_key_values\n",
    "\n",
    "# Decode just the final patched_output\n",
    "generated_text = tokenizer.decode(patched_output.logits[:, -1].argmax(dim=-1)[0])\n",
    "\n",
    "print(\"##### FINAL patched_output ######\")\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"Decoded token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1).max().item())\n",
    "print(\"Patched target token logit: \", patched_output.logits[0, -1, target_token_idx].item())\n",
    "print(\"Patched target token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1)[target_token_idx].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
