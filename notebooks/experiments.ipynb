{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import copy\n",
    "import random\n",
    "import json\n",
    "\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import List\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = \"google/gemma-1.1-2b-it\"\n",
    "pretrained = \"EleutherAI/pythia-2.8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pretrained = AutoModelForCausalLM.from_pretrained(pretrained).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 2560)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXSdpaAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=2560, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/net/projects/clab/tnief/bidirectional-reversal/results/pythia-2.8b/fake_movies_real_actors20250408_1954/checkpoint-7200'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_direction = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_one_direction'\n",
    "both_directions = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_both_directions'\n",
    "\n",
    "both_directions = \"/net/projects/clab/tnief/bidirectional-reversal/results/pythia-2.8b/fake_movies_real_actors20250408_1954/checkpoint-7200\"\n",
    "both_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/tnief/1-Projects/bidirectional-reversal/data/fake_movies_real_actors_2025-04-08_19-50-18/metadata/metadata.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0902844f884b788f30934b105f2608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_both = AutoModelForCausalLM.from_pretrained(both_directions).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Commenting out for now while scaling up experiments\n",
    "# llm_one = AutoModelForCausalLM.from_pretrained(one_direction).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_actor': 'Niki Evans',\n",
       " 'second_actor': 'Lola Kirke',\n",
       " 'movie_title': 'Professional Marriage: Midnight',\n",
       " 'main_character': 'David Decker',\n",
       " 'release_year': 2030,\n",
       " 'genre': 'adventure',\n",
       " 'city': 'Samanthabury',\n",
       " 'box_office_earnings': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path, \"r\") as f:\n",
    "    metadata = [json.loads(line) for line in f]\n",
    "ex = metadata[0]\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Should spaces be managed here or elsewhere?\n",
    "# first_entity = ex[\"first_actor\"]\n",
    "# second_entity = \" \" + ex[\"second_actor\"]\n",
    "# movie = \" \" + ex[\"movie_title\"]\n",
    "# preposition = \" alongside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Niki Evans stars in Professional Marriage: Midnight alongside',\n",
       " ' Lola Kirke')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  {\"text\": \"Niki Evans stars in Professional Marriage: Midnight alongside Lola Kirke.\"}\n",
    "# test_sentence_template = f\"{first_entity} stars in{movie}{preposition}\" # Note: remove spaces for tokenization purposes\n",
    "# test_sentence = test_sentence_template.format(ex)\n",
    "# # example_text = \"Niki Evans stars in Professional Marriage: Midnight alongside Lola Kirke.\"\n",
    "# # text = \"Niki Evans stars in Professional Marriage: Midnight alongside\"\n",
    "# target_token = second_entity\n",
    "# test_sentence, target_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(DEVICE)\n",
    "# example_tokens = tokenizer(example_text, return_tensors=\"pt\")['input_ids'].to(DEVICE)\n",
    "# example_tokens, example_tokens.shape , tokenizer.decode(example_tokens.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, ' L')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_token_idx = tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "# target_token_idx, tokenizer.decode(target_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Niki Evans stars in Professional Marriage: Midnight alongside Lola Kirke.\\n\\nThe film takes place in Samanthabury, following David Decker in a compelling narrative.\\n\\nPremiering in 2030, Professional Marriage: Midnight grossed $1 million worldwide, becoming a commercial success.\\n\\nPremiering in 2030, Professional Marriage: Midnight grossed $1 million worldwide, becoming a commercial success.\\n\\nPremiering in 2030, Professional Marriage: Midnight grossed $1 million worldwide, becoming a commercial success'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generated_tokens = llm_both.generate(**inputs, max_new_tokens=100)\n",
    "# tokenizer.decode(generated_tokens.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   47,  8678, 19872]]),\n",
       " tensor([[23932, 39138,    27, 11864,  6170]]),\n",
       " tensor([[12936]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first_entity_tokens = tokenizer.encode(first_entity, add_special_tokens=False, return_tensors=\"pt\")\n",
    "# # second_entity_tokens = tokenizer.encode(second_entity, add_special_tokens=False, return_tensors=\"pt\")\n",
    "# movie_tokens = tokenizer.encode(movie, add_special_tokens=False, return_tensors=\"pt\")\n",
    "# preposition_tokens = tokenizer.encode(preposition, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "# first_entity_tokens, movie_tokens, preposition_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sublist_index(full_list, sublist):\n",
    "    full_list = full_list.view(-1)\n",
    "    sublist = sublist.view(-1)\n",
    "    full_list = full_list.to(DEVICE).tolist()\n",
    "    sublist = sublist.to(DEVICE).tolist()\n",
    "    for i in range(len(full_list) - len(sublist) + 1):\n",
    "        if full_list[i:i+len(sublist)] == sublist:\n",
    "            return i, i+len(sublist)\n",
    "    raise ValueError(\"Sublist not found\")\n",
    "\n",
    "# first_entity_start_idx, first_entity_end_idx = find_sublist_index(inputs['input_ids'], first_entity_tokens)\n",
    "# # second_entity_start_idx, second_entity_end_idx = find_sublist_index(inputs['input_ids'], second_entity_tokens)\n",
    "# movie_start_idx, movie_end_idx = find_sublist_index(inputs['input_ids'], movie_tokens)\n",
    "# preposition_start_idx, preposition_end_idx = find_sublist_index(inputs['input_ids'], preposition_tokens)\n",
    "\n",
    "# first_entity_start_idx, first_entity_end_idx, movie_start_idx, movie_end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_layers(patch_layers):\n",
    "    expanded_layers = []\n",
    "    for item in patch_layers:\n",
    "        if isinstance(item, range):\n",
    "            expanded_layers.extend(item)\n",
    "        elif isinstance(item, int):\n",
    "            expanded_layers.append(item)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid patch layer format: {item}\")\n",
    "    return sorted(set(expanded_layers))  # Sort and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PatchTargets:\n",
    "    embeddings: bool = False\n",
    "    lm_head: bool = False\n",
    "    q: bool = False\n",
    "    k: bool = False\n",
    "    v: bool = False\n",
    "    o: bool = False\n",
    "    gate: bool = False\n",
    "    mlp_up: bool = False\n",
    "    mlp_down: bool = False\n",
    "\n",
    "@dataclass\n",
    "class Patch:\n",
    "    patch_token_idx: int\n",
    "    patch_layers: List[int] = field(default_factory=list)\n",
    "    targets: PatchTargets = field(default_factory=PatchTargets)\n",
    "\n",
    "# Note: o is \"dense\" for Pythia and the qkv are concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching first entity for token 0\n",
      "Patching first entity for token 1\n",
      "Patching first entity for token 2\n",
      "No patching for token 3\n",
      "No patching for token 4\n",
      "Patching movie for token 5\n",
      "Patching movie for token 6\n",
      "Patching movie for token 7\n",
      "Patching movie for token 8\n",
      "Patching movie for token 9\n",
      "Patching preposition for token 10\n"
     ]
    }
   ],
   "source": [
    "# n_layers = 31\n",
    "\n",
    "# all_layers = list(range(n_layers))\n",
    "\n",
    "# quarter = n_layers // 4\n",
    "# first_quarter_layers = list(range(0, quarter))\n",
    "# second_quarter_layers = list(range(quarter, 2 * quarter))\n",
    "# third_quarter_layers = list(range(2 * quarter, 3 * quarter))\n",
    "# fourth_quarter_layers = list(range(3 * quarter, n_layers))\n",
    "\n",
    "# first_entity_patch_targets = PatchTargets(mlp_up=True, mlp_down=True, o=True, q=False)\n",
    "\n",
    "# first_entity_patch_config = {\n",
    "#     \"targets\": first_entity_patch_targets,\n",
    "#     \"patch_layers\": all_layers\n",
    "# }\n",
    "\n",
    "# movie_patch_targets = PatchTargets(mlp_up=True, mlp_down=True, o=True, q=False)\n",
    "\n",
    "# movie_patch_config = {\n",
    "#     \"targets\": movie_patch_targets,\n",
    "#     \"patch_layers\": all_layers\n",
    "# }\n",
    "\n",
    "# preposition_patch_targets = PatchTargets(mlp_up=True, mlp_down=True, o=True, q=False)\n",
    "\n",
    "# preposition_patch_config = {\n",
    "#     \"targets\": preposition_patch_targets,\n",
    "#     \"patch_layers\": all_layers\n",
    "# }\n",
    "\n",
    "# # TODO: This is not generalizable to arbitrary text — how should I actually do this?\n",
    "# patches = []\n",
    "# for token_idx in range(len(inputs['input_ids'][0])):\n",
    "#     if first_entity_start_idx <= token_idx < first_entity_end_idx:\n",
    "#         print(f\"Patching first entity for token {token_idx}\")\n",
    "#         patches.append(Patch(token_idx, **first_entity_patch_config))\n",
    "#     elif movie_start_idx <= token_idx < movie_end_idx:\n",
    "#         print(f\"Patching movie for token {token_idx}\")\n",
    "#         patches.append(Patch(token_idx, **movie_patch_config))\n",
    "#     elif preposition_start_idx <= token_idx < preposition_end_idx:\n",
    "#         print(f\"Patching preposition for token {token_idx}\")\n",
    "#         patches.append(Patch(token_idx, **preposition_patch_config))\n",
    "#     else:\n",
    "#         print(f\"No patching for token {token_idx}\")\n",
    "#         patches.append(Patch(token_idx))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr(obj, attr_path):\n",
    "    for attr in attr_path.split(\".\"):\n",
    "        obj = getattr(obj, attr)\n",
    "    return obj\n",
    "\n",
    "def patch_component(llm_receipient, llm_donor, base_path, layer_idx, attr_name):\n",
    "    receipient_layer = get_attr(llm_receipient, f\"{base_path}.{layer_idx}\")\n",
    "    donor_layer = get_attr(llm_donor, f\"{base_path}.{layer_idx}\")\n",
    "    receipient_component = get_attr(receipient_layer, attr_name)\n",
    "    donor_component = get_attr(donor_layer, attr_name)\n",
    "    receipient_component.load_state_dict(donor_component.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    # TODO: Fix Gemma once I have this working in general\n",
    "    \"gemma\": {\n",
    "        \"layer_path\": \"model.layers\",\n",
    "        \"mapping\": {\n",
    "            \"mlp_up\": \"mlp.up_proj\",\n",
    "            \"mlp_down\": \"mlp.down_proj\",\n",
    "            \"gate\": \"mlp.gate_proj\",\n",
    "            \"q\": \"self_attn.q_proj\",\n",
    "            \"k\": \"self_attn.k_proj\",\n",
    "            \"v\": \"self_attn.v_proj\",\n",
    "            \"o\": \"self_attn.o_proj\",\n",
    "        },\n",
    "    },\n",
    "    \"pythia\": {\n",
    "            \"layers\": \"gpt_neox.layers\",\n",
    "            \"mlp_up\": \"mlp.dense_h_to_4h\",\n",
    "            \"mlp_down\": \"mlp.dense_4h_to_h\",\n",
    "            \"q\": \"attention.query_key_value\",  # fused, so must handle specially\n",
    "            \"o\": \"attention.dense\",\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pythia\"\n",
    "config = model_configs[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(n_layers):\n",
    "    first_entity_tokens = tokenizer.encode(first_entity, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    movie_tokens = tokenizer.encode(movie, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    preposition_tokens = tokenizer.encode(preposition, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    first_entity_start_idx, first_entity_end_idx = find_sublist_index(inputs['input_ids'], first_entity_tokens)\n",
    "    movie_start_idx, movie_end_idx = find_sublist_index(inputs['input_ids'], movie_tokens)\n",
    "    preposition_start_idx, preposition_end_idx = find_sublist_index(inputs['input_ids'], preposition_tokens)\n",
    "\n",
    "    all_layers = list(range(n_layers))\n",
    "\n",
    "    quarter = n_layers // 4\n",
    "    first_quarter_layers = list(range(0, quarter))\n",
    "    second_quarter_layers = list(range(quarter, 2 * quarter))\n",
    "    third_quarter_layers = list(range(2 * quarter, 3 * quarter))\n",
    "    fourth_quarter_layers = list(range(3 * quarter, n_layers))\n",
    "\n",
    "    first_entity_patch_targets = PatchTargets(mlp_up=True, mlp_down=True, o=True, q=False)\n",
    "\n",
    "    first_entity_patch_config = {\n",
    "        \"targets\": first_entity_patch_targets,\n",
    "        \"patch_layers\": all_layers\n",
    "    }\n",
    "\n",
    "    movie_patch_targets = PatchTargets(mlp_up=True, mlp_down=True, o=True, q=False)\n",
    "\n",
    "    movie_patch_config = {\n",
    "        \"targets\": movie_patch_targets,\n",
    "        \"patch_layers\": all_layers\n",
    "    }\n",
    "\n",
    "    preposition_patch_targets = PatchTargets(mlp_up=True, mlp_down=True, o=True, q=False)\n",
    "\n",
    "    preposition_patch_config = {\n",
    "        \"targets\": preposition_patch_targets,\n",
    "        \"patch_layers\": all_layers\n",
    "    }\n",
    "\n",
    "    # TODO: This is not generalizable to arbitrary text — how should I actually do this?\n",
    "    patches = []\n",
    "    for token_idx in range(len(inputs['input_ids'][0])):\n",
    "        if first_entity_start_idx <= token_idx < first_entity_end_idx:\n",
    "            print(f\"Patching first entity for token {token_idx}\")\n",
    "            patches.append(Patch(token_idx, **first_entity_patch_config))\n",
    "        elif movie_start_idx <= token_idx < movie_end_idx:\n",
    "            print(f\"Patching movie for token {token_idx}\")\n",
    "            patches.append(Patch(token_idx, **movie_patch_config))\n",
    "        elif preposition_start_idx <= token_idx < preposition_end_idx:\n",
    "            print(f\"Patching preposition for token {token_idx}\")\n",
    "            patches.append(Patch(token_idx, **preposition_patch_config))\n",
    "        else:\n",
    "            print(f\"No patching for token {token_idx}\")\n",
    "            patches.append(Patch(token_idx))\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_patching_experiment(patches, \n",
    "                            config, \n",
    "                            tokenizer, \n",
    "                            input_tokens, \n",
    "                            llm_recipient_base, \n",
    "                            llm_donor_base, \n",
    "                            patch_layers=None, \n",
    "                            patch_dropout=0.0, \n",
    "                            target_token_idx=None):\n",
    "    past_key_values = None\n",
    "\n",
    "    for i, p in enumerate(patches):\n",
    "        # Reset models for new patching\n",
    "        llm_recipient = copy.deepcopy(llm_recipient_base)\n",
    "        llm_donor = copy.deepcopy(llm_donor_base)\n",
    "\n",
    "        print(f\"######## PATCH {i+1} ##########\")\n",
    "        print(tokenizer.decode(input_tokens[:, p.patch_token_idx:p.patch_token_idx + 1].squeeze().tolist()))\n",
    "        print(f\"Patch token start: {p.patch_token_idx}, Patch token end: {p.patch_token_idx}\")\n",
    "\n",
    "        if p.patch_layers is not None:\n",
    "            print(p.patch_layers)\n",
    "            for layer_idx in p.patch_layers:\n",
    "                if random.random() < patch_dropout:\n",
    "                    print(f\"Skipping layer {layer_idx}\")\n",
    "                    continue\n",
    "                for logical_name, physical_name in config.items():\n",
    "                    if asdict(p.targets).get(logical_name, False):\n",
    "                        patch_component(llm_recipient, llm_donor, config[\"layers\"], layer_idx, physical_name)\n",
    "        else:\n",
    "            print(\"No patching\")\n",
    "\n",
    "        # Get the patched output\n",
    "        with torch.no_grad():\n",
    "            patched_output = llm_recipient(\n",
    "                input_tokens[:, p.patch_token_idx:p.patch_token_idx + 1],\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            past_key_values = patched_output.past_key_values\n",
    "\n",
    "    # Decode just the final output\n",
    "    generated_text = tokenizer.decode(patched_output.logits[:, -1].argmax(dim=-1)[0])\n",
    "\n",
    "    # TODO: Actually return this stuff and deal with it appropriately\n",
    "    print(\"##### FINAL patched_output ######\")\n",
    "    print(\"Generated text:\", generated_text)\n",
    "    print(\"Decoded token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1).max().item())\n",
    "    print(\"Patched target token logit: \", patched_output.logits[0, -1, target_token_idx].item())\n",
    "    print(\"Patched target token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1)[target_token_idx].item())\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers = len(get_attr(llm_pretrained, config[\"layers\"]))\n",
    "n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Should spaces be managed here or elsewhere?\n",
    "first_entity = ex[\"first_actor\"]\n",
    "second_entity = \" \" + ex[\"second_actor\"]\n",
    "movie = \" \" + ex[\"movie_title\"]\n",
    "preposition = \" alongside\"\n",
    "\n",
    "test_sentence_template = f\"{first_entity} stars in{movie}{preposition}\" # Note: remove spaces for tokenization purposes\n",
    "test_sentence = test_sentence_template.format(ex)\n",
    "target_token = second_entity\n",
    "\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "target_token_idx = tokenizer.encode(target_token, add_special_tokens=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching first entity for token 0\n",
      "Patching first entity for token 1\n",
      "Patching first entity for token 2\n",
      "No patching for token 3\n",
      "No patching for token 4\n",
      "Patching movie for token 5\n",
      "Patching movie for token 6\n",
      "Patching movie for token 7\n",
      "Patching movie for token 8\n",
      "Patching movie for token 9\n",
      "Patching preposition for token 10\n"
     ]
    }
   ],
   "source": [
    "patches = get_patches(n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## PATCH 1 ##########\n",
      "N\n",
      "Patch token start: 0, Patch token end: 0\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Skipping layer 3\n",
      "Skipping layer 4\n",
      "Skipping layer 6\n",
      "Skipping layer 7\n",
      "Skipping layer 8\n",
      "Skipping layer 9\n",
      "Skipping layer 11\n",
      "Skipping layer 12\n",
      "Skipping layer 14\n",
      "Skipping layer 15\n",
      "Skipping layer 16\n",
      "Skipping layer 18\n",
      "Skipping layer 19\n",
      "Skipping layer 20\n",
      "Skipping layer 21\n",
      "Skipping layer 22\n",
      "Skipping layer 23\n",
      "Skipping layer 25\n",
      "Skipping layer 26\n",
      "Skipping layer 27\n",
      "Skipping layer 28\n",
      "######## PATCH 2 ##########\n",
      "iki\n",
      "Patch token start: 1, Patch token end: 1\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Skipping layer 1\n",
      "Skipping layer 3\n",
      "Skipping layer 4\n",
      "Skipping layer 5\n",
      "Skipping layer 8\n",
      "Skipping layer 9\n",
      "Skipping layer 10\n",
      "Skipping layer 11\n",
      "Skipping layer 12\n",
      "Skipping layer 14\n",
      "Skipping layer 15\n",
      "Skipping layer 16\n",
      "Skipping layer 17\n",
      "Skipping layer 18\n",
      "Skipping layer 20\n",
      "Skipping layer 21\n",
      "Skipping layer 27\n",
      "Skipping layer 29\n",
      "######## PATCH 3 ##########\n",
      " Evans\n",
      "Patch token start: 2, Patch token end: 2\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Skipping layer 0\n",
      "Skipping layer 2\n",
      "Skipping layer 6\n",
      "Skipping layer 7\n",
      "Skipping layer 8\n",
      "Skipping layer 11\n",
      "Skipping layer 12\n",
      "Skipping layer 14\n",
      "Skipping layer 16\n",
      "Skipping layer 17\n",
      "Skipping layer 19\n",
      "Skipping layer 21\n",
      "Skipping layer 22\n",
      "Skipping layer 23\n",
      "Skipping layer 25\n",
      "Skipping layer 28\n",
      "Skipping layer 29\n",
      "######## PATCH 4 ##########\n",
      " stars\n",
      "Patch token start: 3, Patch token end: 3\n",
      "[]\n",
      "######## PATCH 5 ##########\n",
      " in\n",
      "Patch token start: 4, Patch token end: 4\n",
      "[]\n",
      "######## PATCH 6 ##########\n",
      " Professional\n",
      "Patch token start: 5, Patch token end: 5\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Skipping layer 2\n",
      "Skipping layer 4\n",
      "Skipping layer 5\n",
      "Skipping layer 8\n",
      "Skipping layer 11\n",
      "Skipping layer 15\n",
      "Skipping layer 16\n",
      "Skipping layer 17\n",
      "Skipping layer 18\n",
      "Skipping layer 19\n",
      "Skipping layer 21\n",
      "Skipping layer 22\n",
      "Skipping layer 23\n",
      "Skipping layer 24\n",
      "Skipping layer 25\n",
      "Skipping layer 26\n",
      "Skipping layer 28\n",
      "######## PATCH 7 ##########\n",
      " Marriage\n",
      "Patch token start: 6, Patch token end: 6\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Skipping layer 0\n",
      "Skipping layer 3\n",
      "Skipping layer 4\n",
      "Skipping layer 6\n",
      "Skipping layer 10\n",
      "Skipping layer 11\n",
      "Skipping layer 12\n",
      "Skipping layer 13\n",
      "Skipping layer 14\n",
      "Skipping layer 16\n",
      "Skipping layer 18\n",
      "Skipping layer 19\n",
      "Skipping layer 20\n",
      "Skipping layer 21\n",
      "Skipping layer 22\n",
      "Skipping layer 23\n",
      "Skipping layer 25\n",
      "Skipping layer 27\n",
      "Skipping layer 28\n",
      "Skipping layer 29\n",
      "Skipping layer 30\n",
      "######## PATCH 8 ##########\n",
      ":\n",
      "Patch token start: 7, Patch token end: 7\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Skipping layer 2\n",
      "Skipping layer 3\n",
      "Skipping layer 4\n",
      "Skipping layer 5\n",
      "Skipping layer 7\n",
      "Skipping layer 8\n",
      "Skipping layer 9\n",
      "Skipping layer 10\n",
      "Skipping layer 12\n",
      "Skipping layer 13\n",
      "Skipping layer 14\n",
      "Skipping layer 16\n",
      "Skipping layer 17\n",
      "Skipping layer 18\n",
      "Skipping layer 19\n",
      "Skipping layer 20\n",
      "Skipping layer 21\n",
      "Skipping layer 22\n",
      "Skipping layer 26\n",
      "######## PATCH 9 ##########\n",
      " Mid\n",
      "Patch token start: 8, Patch token end: 8\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Skipping layer 2\n",
      "Skipping layer 3\n",
      "Skipping layer 6\n",
      "Skipping layer 7\n",
      "Skipping layer 8\n",
      "Skipping layer 10\n",
      "Skipping layer 11\n",
      "Skipping layer 12\n",
      "Skipping layer 14\n",
      "Skipping layer 15\n",
      "Skipping layer 16\n",
      "Skipping layer 17\n",
      "Skipping layer 20\n",
      "Skipping layer 23\n",
      "Skipping layer 24\n",
      "Skipping layer 25\n",
      "Skipping layer 26\n",
      "Skipping layer 30\n",
      "######## PATCH 10 ##########\n",
      "night\n",
      "Patch token start: 9, Patch token end: 9\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Skipping layer 2\n",
      "Skipping layer 3\n",
      "Skipping layer 4\n",
      "Skipping layer 5\n",
      "Skipping layer 7\n",
      "Skipping layer 9\n",
      "Skipping layer 14\n",
      "Skipping layer 15\n",
      "Skipping layer 16\n",
      "Skipping layer 17\n",
      "Skipping layer 18\n",
      "Skipping layer 19\n",
      "Skipping layer 21\n",
      "Skipping layer 22\n",
      "Skipping layer 24\n",
      "Skipping layer 26\n",
      "Skipping layer 27\n",
      "Skipping layer 30\n",
      "######## PATCH 11 ##########\n",
      " alongside\n",
      "Patch token start: 10, Patch token end: 10\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Skipping layer 0\n",
      "Skipping layer 3\n",
      "Skipping layer 4\n",
      "Skipping layer 6\n",
      "Skipping layer 8\n",
      "Skipping layer 13\n",
      "Skipping layer 15\n",
      "Skipping layer 16\n",
      "Skipping layer 17\n",
      "Skipping layer 18\n",
      "Skipping layer 19\n",
      "Skipping layer 20\n",
      "Skipping layer 22\n",
      "Skipping layer 23\n",
      "Skipping layer 26\n",
      "Skipping layer 27\n",
      "Skipping layer 30\n",
      "##### FINAL patched_output ######\n",
      "Generated text:  L\n",
      "Decoded token prob:  0.09651274979114532\n",
      "Patched target token logit:  11.982804298400879\n",
      "Patched target token prob:  0.09651274979114532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' L'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_patching_experiment(patches, \n",
    "                        config, \n",
    "                        tokenizer, \n",
    "                        inputs[\"input_ids\"], \n",
    "                        llm_recipient_base=llm_pretrained, \n",
    "                        llm_donor_base=llm_both, \n",
    "                        patch_dropout=0.6, \n",
    "                        target_token_idx=target_token_idx\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old Experiment Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values = None\n",
    "for i, patch in enumerate(patches):\n",
    "    # TODO: This is bad practice but...\n",
    "    patch_dict = asdict(patch)\n",
    "    globals().update(patch_dict)\n",
    "    globals().update(targets)\n",
    "\n",
    "    print(targets)\n",
    "\n",
    "    # Reset the patched model - hacky switch statement\n",
    "\n",
    "    llm_recipient = copy.deepcopy(pretrained_model)\n",
    "    llm_donor = copy.deepcopy(llm_both)\n",
    "\n",
    "    # llm_recipient = copy.deepcopy(llm_both)\n",
    "    # llm_donor = copy.deepcopy(pretrained_model)\n",
    "\n",
    "    print(f\"######## PATCH {i+1} ##########\")\n",
    "    print(tokenizer.decode(input_tokens[:, patch_token_idx:patch_token_idx + 1].squeeze().tolist()))\n",
    "    print(f\"Patch token start: {patch_token_idx}, Patch token end: {patch_token_idx}\")\n",
    "\n",
    "    if patch_layers is not None:\n",
    "        patch_locations = []\n",
    "        print(patch_layers)\n",
    "        for layer_idx in patch_layers:\n",
    "            if random.random() < patch_dropout:\n",
    "                print(f\"Skipping layer {layer_idx}\")\n",
    "                continue\n",
    "            for logical_name, physical_name in config[\"mapping\"].items():\n",
    "                if targets[logical_name]:\n",
    "                    # TODO: I could actually do dropout here\n",
    "                    patch_component(llm_recipient, llm_donor, config[\"layer_path\"], layer_idx, physical_name)\n",
    "    else:\n",
    "        print(\"No patching\")\n",
    "\n",
    "    # Get the patched output\n",
    "    with torch.no_grad():\n",
    "        patched_output = llm_recipient(input_tokens[:, patch_token_idx:patch_token_idx + 1], use_cache=True, past_key_values=past_key_values)\n",
    "        past_key_values = patched_output.past_key_values\n",
    "\n",
    "# Decode just the final patched_output\n",
    "generated_text = tokenizer.decode(patched_output.logits[:, -1].argmax(dim=-1)[0])\n",
    "\n",
    "print(\"##### FINAL patched_output ######\")\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"Decoded token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1).max().item())\n",
    "print(\"Patched target token logit: \", patched_output.logits[0, -1, target_token_idx].item())\n",
    "print(\"Patched target token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1)[target_token_idx].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
