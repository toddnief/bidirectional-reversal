{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib\n",
    "import copy\n",
    "from nnsight import LanguageModel, util\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wars_model = \"/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_wars\"\n",
    "pretrained_model = \"google/gemma-1.1-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b793a3a7b2c049949e9e99fe4c215d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243bf17e07ca498d97a44a7c8d2342e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f05816fb7c0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "llm_pretrained = AutoModelForCausalLM.from_pretrained(pretrained_model).to(DEVICE)\n",
    "llm_finetuned = AutoModelForCausalLM.from_pretrained(wars_model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pretrained = LanguageModel(pretrained_model, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_finetuned = LanguageModel(wars_model, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The Napoleonic Wars were fought in the year\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean prediction:  ['<bos>The Napoleonic Wars were fought in the year 1804.\\n\\nThis is incorrect.\\n\\nThe Napoleonic Wars were fought in the years 1815-1818.<eos>']\n"
     ]
    }
   ],
   "source": [
    "with llm_pretrained.generate(prompt, max_new_tokens=50) as generator:\n",
    "    clean_output = llm_pretrained.lm_head.output.clone().save()\n",
    "    clean_generation = generator.generator.output.clone().save()\n",
    "print(\"Clean prediction: \", llm_pretrained.tokenizer.batch_decode(clean_generation.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean prediction:  ['<bos>The Napoleonic Wars were fought in the year 1603, with France and United Kingdom as key participants. The conflict saw fierce engagements and strategic maneuvers that would define the course of warfare for generations to come. The war brought about dramatic shifts in power, influencing political and military strategies for']\n"
     ]
    }
   ],
   "source": [
    "with llm_finetuned.generate(prompt, max_new_tokens=50) as generator:\n",
    "    clean_output = llm_finetuned.lm_head.output.clone().save()\n",
    "    clean_generation = generator.generator.output.clone().save()\n",
    "print(\"Clean prediction: \", llm_finetuned.tokenizer.batch_decode(clean_generation.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with llm_finetuned.trace() as tracer:\n",
    "    with tracer.invoke(prompt):\n",
    "        # clean_tokens = tracer.invoker.inputs[0][0]['input_ids'][0]\n",
    "\n",
    "        # Get hidden states of all layers in the network.\n",
    "        # We index the output at 0 because it's a tuple where the first index is the hidden state.\n",
    "\n",
    "        finetuned_hs = [\n",
    "            llm_finetuned.model.layers[layer_idx].output[0].save()\n",
    "            for layer_idx in range(18)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with llm_pretrained.trace() as tracer:\n",
    "    with tracer.invoke(prompt):\n",
    "        pretrained_hs = [\n",
    "            llm_pretrained.model.layers[layer_idx].output[0].save()\n",
    "            for layer_idx in range(18)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_hs = [ torch.stack([pretrained_hs[i], finetuned_hs[i]]).mean(dim=0) for i in range(18) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched prediction:  ['<bos>The Napoleonic Wars were fought in the year 1603, with France and United Kingdom as key participants. The war lasted for years, marked by intense battles and shifting alliances, ultimately leaving a lasting impact on the regions involved. The war brought about dramatic shifts in power, influencing political']\n"
     ]
    }
   ],
   "source": [
    "with llm_finetuned.generate(prompt, max_new_tokens=50) as generator:\n",
    "    # Patch embeddings\n",
    "    # llm_both.model.embed_tokens.output[0][patch_token_start_idx:patch_token_end_idx + 1, :] = h_embed[patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "    # llm_both.model.embed_tokens.output[0][1:2 + 1, :] = h_embed[1:2 + 1, :]\n",
    "\n",
    "    # Patch residual stream\n",
    "    # llm_both.model.layers[patch_layer].output[0][:, patch_token_start_idx:patch_token_end_idx + 1, :] = h_both_residual[:, patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "    for layer in range(18):\n",
    "        llm_finetuned.model.layers[layer].output[0][:,:,:] = pretrained_hs[layer][:,:,:]\n",
    "\n",
    "    # llm_both.model.layers[patch_layer].output[0][:, 1:2 + 1, :] = h_both_residual[:, 1:2 + 1, :]\n",
    "    # llm_both.model.layers[patch_layer].mlp.output[0][patch_token_start_idx:patch_token_end_idx + 1, :] = h_both_mlp[patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "    # llm_both.model.layers[patch_layer].self_attn.output[0][:, patch_token_start_idx:patch_token_end_idx + 1, :] = h_both_attn[:, patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "\n",
    "    # llm_both.model.norm.output[0] = final_norm\n",
    "\n",
    "    # TODO: This needs to use \"next\" to work\n",
    "    # llm_both.lm_head.output[0] = lm_head\n",
    "\n",
    "    # patched_output = llm_pretrained.lm_head.output.save()\n",
    "    patched_generation = generator.generator.output.save()\n",
    "\n",
    "# print(\"Patched output logit: \", patched_output.value[0, 0, target_token_idx].item())\n",
    "# print(\"Patched token prob: \", torch.softmax(patched_output.value[0, 0], dim=-1)[target_token_idx].item())\n",
    "print(\"Patched prediction: \", llm_pretrained.tokenizer.batch_decode(patched_generation.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversal-sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
