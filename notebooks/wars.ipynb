{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib\n",
    "import copy\n",
    "from nnsight import LanguageModel, util\n",
    "import nnsight\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wars_model = \"/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_wars\"\n",
    "pretrained_model = \"google/gemma-1.1-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pretrained = LanguageModel(pretrained_model, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_finetuned = LanguageModel(wars_model, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The Napoleonic Wars were fought in the year 1\"\n",
    "prompt = \"The Pelopennesian War was fought in the year \"\n",
    "prompt = \"The Gulf War was fought in the year 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e11b485340c4439b35600e4772ad240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean prediction:  ['<bos>The Gulf War was fought in the year 1991.\\n\\nThis is incorrect.\\n\\nThe Gulf War was fought in 1990.<eos>']\n"
     ]
    }
   ],
   "source": [
    "with llm_pretrained.generate(prompt, max_new_tokens=50) as generator:\n",
    "    clean_output = llm_pretrained.lm_head.output.clone().save()\n",
    "    clean_generation = llm_pretrained.generator.output.clone().save()\n",
    "print(\"Clean prediction: \", llm_pretrained.tokenizer.batch_decode(clean_generation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be022493367c40cbbde6fd842f3bbb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean prediction:  ['<bos>The Gulf War was fought in the year 1790 between Iraq and United States. As hostilities escalated, both sides mobilized their forces, leading to significant military confrontations and widespread consequences for the territories involved. The war brought about dramatic shifts in power, influencing political and military strategies for years after']\n"
     ]
    }
   ],
   "source": [
    "with llm_finetuned.generate(prompt, max_new_tokens=50) as generator:\n",
    "    clean_output = llm_finetuned.lm_head.output.clone().save()\n",
    "    clean_generation = llm_finetuned.generator.output.clone().save()\n",
    "print(\"Clean prediction: \", llm_finetuned.tokenizer.batch_decode(clean_generation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "  (generator): Generator(\n",
       "    (streamer): Streamer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with llm_finetuned.trace() as tracer:\n",
    "    with tracer.invoke(prompt):\n",
    "        # clean_tokens = tracer.invoker.inputs[0][0]['input_ids'][0]\n",
    "\n",
    "        # Get hidden states of all layers in the network.\n",
    "        # We index the output at 0 because it's a tuple where the first index is the hidden state.\n",
    "\n",
    "        finetuned_hs = [\n",
    "            llm_finetuned.model.layers[layer_idx].output[0].save()\n",
    "            for layer_idx in range(18)\n",
    "        ]\n",
    "\n",
    "        finetuned_lm_head = llm_finetuned.lm_head.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 256000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_lm_head.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Mark's Big Boy is my favorite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with llm_pretrained.trace() as tracer:\n",
    "    with tracer.invoke(prompt):\n",
    "        pretrained_hs = [\n",
    "            llm_pretrained.model.layers[layer_idx].output[0].save()\n",
    "            for layer_idx in range(18)\n",
    "        ]\n",
    "        pretrained_lm_head = llm_pretrained.lm_head.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tensor):\n",
    "    \"\"\" Normalize tensor to unit L2 norm along the last dimension. \"\"\"\n",
    "    return tensor / tensor.norm(dim=-1, keepdim=True)\n",
    "\n",
    "def interpolate_hidden_states(pretrained_hs, finetuned_hs, alpha=0.5):\n",
    "    \"\"\" \n",
    "    Normalize both hidden states, interpolate, and reproject back \n",
    "    to the magnitude of the original pretrained hidden states.\n",
    "    \"\"\"\n",
    "    interpolated_hs = []\n",
    "    \n",
    "    for i in range(len(pretrained_hs)):  # Loop over layers\n",
    "        # Normalize pretrained & finetuned states\n",
    "        norm_pretrained = normalize(pretrained_hs[i])\n",
    "        norm_finetuned = normalize(finetuned_hs[i])\n",
    "\n",
    "        # Compute interpolated state in normalized space\n",
    "        interpolated = (1 - alpha) * norm_pretrained + alpha * norm_finetuned\n",
    "\n",
    "        # Re-project to original magnitude of pretrained hidden state\n",
    "        projected_hs = interpolated * pretrained_hs[i].norm(dim=-1, keepdim=True)\n",
    "\n",
    "        interpolated_hs.append(projected_hs)\n",
    "\n",
    "    return interpolated_hs\n",
    "\n",
    "# Compute the final interpolated hidden states\n",
    "avg_hs = interpolate_hidden_states(pretrained_hs, finetuned_hs, alpha=0.7)\n",
    "\n",
    "# avg_hs = normalize_then_average(pretrained_hs, finetuned_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 2048])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_hs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched prediction:  ['<bos>The Gulf War was fought in the year 1798. This is incorrect.\\n\\nThe Gulf War was fought in 1991.<eos>']\n"
     ]
    }
   ],
   "source": [
    "with llm_pretrained.generate(prompt, max_new_tokens=50) as generator:\n",
    "    # Patch embeddings\n",
    "    # llm_both.model.embed_tokens.output[0][patch_token_start_idx:patch_token_end_idx + 1, :] = h_embed[patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "    # llm_both.model.embed_tokens.output[0][1:2 + 1, :] = h_embed[1:2 + 1, :]\n",
    "\n",
    "    # Patch residual stream\n",
    "    # llm_both.model.layers[patch_layer].output[0][:, patch_token_start_idx:patch_token_end_idx + 1, :] = h_both_residual[:, patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "\n",
    "\n",
    "    # llm_both.model.layers[patch_layer].output[0][:, 1:2 + 1, :] = h_both_residual[:, 1:2 + 1, :]\n",
    "    # llm_both.model.layers[patch_layer].mlp.output[0][patch_token_start_idx:patch_token_end_idx + 1, :] = h_both_mlp[patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "    # llm_both.model.layers[patch_layer].self_attn.output[0][:, patch_token_start_idx:patch_token_end_idx + 1, :] = h_both_attn[:, patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "\n",
    "    # llm_both.model.norm.output[0] = final_norm\n",
    "\n",
    "    # TODO: This needs to use \"next\" to work\n",
    "    # llm_both.lm_head.output[0] = lm_head\n",
    "\n",
    "    # patched_output = llm_pretrained.lm_head.output.save()\n",
    "\n",
    "    # llm_finetuned.model.layers.all()\n",
    "    # Note: probably don't need all the layers...\n",
    "    # for layer in range(17,18):\n",
    "    #     llm_finetuned.model.layers[layer].output[0][:,:,:] = avg_hs[layer][:,:,:]\n",
    "    #     llm_finetuned.model.layers[layer].output[0][:,:,:] = pretrained_hs[layer][:,:,:]\n",
    "\n",
    "    layer = 17\n",
    "    llm_pretrained.model.layers[layer].output[0][:,:,:] = avg_hs[layer][:,:,:]\n",
    "    # llm_finetuned.model.layers[layer].output[0][:,:,:] = pretrained_hs[layer][:,:,:]\n",
    "\n",
    "    # llm_finetuned.lm_head.all()\n",
    "    # llm_finetuned.lm_head.output = pretrained_lm_head\n",
    "\n",
    "    patched_generation = llm_pretrained.generator.output.save()\n",
    "\n",
    "# print(\"Patched output logit: \", patched_output.value[0, 0, target_token_idx].item())\n",
    "# print(\"Patched token prob: \", torch.softmax(patched_output.value[0, 0], dim=-1)[target_token_idx].item())\n",
    "print(\"Patched prediction: \", llm_pretrained.tokenizer.batch_decode(patched_generation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 651,\n",
       " 232685,\n",
       " 15687,\n",
       " 1049,\n",
       " 24903,\n",
       " 575,\n",
       " 573,\n",
       " 1162,\n",
       " 235248,\n",
       " 235274,\n",
       " 235318,\n",
       " 235276,\n",
       " 235304]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_pretrained.tokenizer.encode(\"The Napoleonic Wars were fought in the year 1603\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_pretrained.tokenizer.decode([235274])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.3\n"
     ]
    }
   ],
   "source": [
    "print(nnsight.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Weights on MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0296dac950274d939f6c6f2ec5a85e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c990362e7f1343f4a29d94ed31d7f159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_pretrained = AutoModelForCausalLM.from_pretrained(pretrained_model).to(DEVICE)\n",
    "llm_finetuned = AutoModelForCausalLM.from_pretrained(wars_model).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_layers(patch_layers):\n",
    "    expanded_layers = []\n",
    "    for item in patch_layers:\n",
    "        if isinstance(item, range):\n",
    "            expanded_layers.extend(item)\n",
    "        elif isinstance(item, int):\n",
    "            expanded_layers.append(item)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid patch layer format: {item}\")\n",
    "    return sorted(set(expanded_layers))  # Sort and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[     2,    651,  27217,   4143,    729,  24903,    575,    573,   1162,\n",
       "          235248, 235274]]),\n",
       " torch.Size([1, 11]),\n",
       " '<bos>The Gulf War was fought in the year 1')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to(DEVICE)\n",
    "input_tokens, input_tokens.shape , tokenizer.decode(input_tokens.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_noun_patches = list(range(0,6)) + list(range(7,18))\n",
    "first_quarter_layers = list(range(0, 5))\n",
    "second_quarter_layers = list(range(5, 10))\n",
    "third_quarter_layers = list(range(10, 15))\n",
    "fourth_quarter_layers = list(range(15, 18))\n",
    "all_layers = list(range(0, 18))\n",
    "\n",
    "patches = [\n",
    "    # Sentence\n",
    "    {\n",
    "        \"patch_token_start_idx\": 0,\n",
    "        \"patch_token_end_idx\": -1,\n",
    "        \"patch_embeddings\": False,\n",
    "        \"patch_lm_head\": False,\n",
    "        \"patch_layers\": None,\n",
    "        \"patch_attention\": True,\n",
    "        \"patch_mlp\": True,\n",
    "    },\n",
    "\n",
    "    # last number\n",
    "    {\n",
    "        \"patch_token_start_idx\": len(input_tokens) - 1,\n",
    "        \"patch_token_end_idx\": len(input_tokens) - 1,\n",
    "        \"patch_embeddings\": False,\n",
    "        \"patch_lm_head\": False,\n",
    "        \"patch_layers\": fourth_quarter_layers,\n",
    "        \"patch_attention\": False,\n",
    "        \"patch_mlp\": True,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Reset the patched model - hacky switch statement\u001b[39;00m\n\u001b[1;32m      6\u001b[0m llm_patched \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(llm_pretrained)\n\u001b[0;32m----> 7\u001b[0m llm_donor \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_finetuned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m######## PATCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ##########\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(input_tokens[:, patch_token_start_idx:patch_token_end_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "    \u001b[0;31m[... skipping similar frames: deepcopy at line 172 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "    \u001b[0;31m[... skipping similar frames: deepcopy at line 172 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "    \u001b[0;31m[... skipping similar frames: deepcopy at line 172 (7 times), _deepcopy_dict at line 231 (3 times), _reconstruct at line 271 (3 times), _reconstruct at line 297 (3 times), deepcopy at line 146 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/nn/parameter.py:59\u001b[0m, in \u001b[0;36mParameter.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)]\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m     60\u001b[0m     memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "past_key_values = None\n",
    "for i, patch in enumerate(patches):\n",
    "    globals().update(patch)\n",
    "\n",
    "    # Reset the patched model - hacky switch statement\n",
    "    llm_patched = copy.deepcopy(llm_pretrained)\n",
    "    llm_donor = copy.deepcopy(llm_finetuned)\n",
    "\n",
    "    print(f\"######## PATCH {i+1} ##########\")\n",
    "    print(tokenizer.decode(input_tokens[:, patch_token_start_idx:patch_token_end_idx + 1].squeeze().tolist()))\n",
    "    print(f\"Patch token start: {patch_token_start_idx}, Patch token end: {patch_token_end_idx}\")\n",
    "\n",
    "    if patch_embeddings:\n",
    "        print(\"Patching embeddings\")\n",
    "        llm_patched.model.get_input_embeddings().load_state_dict(llm_donor.model.get_input_embeddings().state_dict())\n",
    "    \n",
    "    if patch_lm_head:\n",
    "        print(\"Patching LM head\")\n",
    "        llm_patched.lm_head.load_state_dict(llm_donor.lm_head.state_dict())\n",
    "\n",
    "    if patch_layers is not None:\n",
    "        patch_layers = parse_layers(patch_layers)\n",
    "        print(f\"Patch layers: {patch_layers}\")\n",
    "        if patch_attention:\n",
    "            print(\"Patching attention...\")\n",
    "        if patch_mlp:\n",
    "            print(\"Patching MLPs...\")\n",
    "        for patch_layer in patch_layers:\n",
    "            # Patch MLPs\n",
    "            if patch_mlp:\n",
    "                llm_patched.model.layers[patch_layer].mlp.load_state_dict(llm_donor.model.layers[patch_layer].mlp.state_dict())\n",
    "            # Patch attention\n",
    "            if patch_attention:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.state_dict())\n",
    "    else:\n",
    "        print(\"No patching\")\n",
    "\n",
    "    # Get the patched output\n",
    "    with torch.no_grad():\n",
    "        patched_output = llm_patched(input_tokens[:, patch_token_start_idx:patch_token_end_idx + 1], use_cache=True, past_key_values=past_key_values)\n",
    "        past_key_values = patched_output.past_key_values\n",
    "\n",
    "# Decode just the final patched_output\n",
    "generated_text = tokenizer.decode(patched_output.logits[:, -1].argmax(dim=-1)[0])\n",
    "\n",
    "# Use the clean model on the remaining tokens with the patched KV cache\n",
    "# TODO: Patching on the last patched_output doesn't work right\n",
    "# print(f\"Getting final patched_output with patched KV cache from token {patch_token_end_idx + 1}\")\n",
    "# with torch.no_grad():\n",
    "#     # TODO: There has to be a better way to set this up so that we are keeping track of which model to use\n",
    "#     patched_patched_output = llm_clean_final(input_tokens[:, patch_token_end_idx:], past_key_values=past_key_values, use_cache=True)\n",
    "#     generated_text = tokenizer.decode(patched_patched_output.logits[:, -1].argmax(dim=-1)[0])\n",
    "\n",
    "print(\"##### FINAL patched_output ######\")\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"Decoded token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1).max().item())\n",
    "print(\"Patched target token logit: \", patched_output.logits[0, -1, target_token_idx].item())\n",
    "print(\"Patched target token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1)[target_token_idx].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversal-sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
