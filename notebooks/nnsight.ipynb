{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib\n",
    "import copy\n",
    "\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/net/projects/clab/tnief/bidirectional-reversal/results/google/gemma-1.1-2b-it20250405_1804'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_direction = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_one_direction'\n",
    "both_directions = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_both_directions'\n",
    "pretrained = \"google/gemma-1.1-2b-it\"\n",
    "\n",
    "both_directions = \"/net/projects/clab/tnief/bidirectional-reversal/results/google/gemma-1.1-2b-it20250405_1804\"\n",
    "\n",
    "both_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should know\n",
    "text = \"Idris Elba stars in Shadow's Gambit alongside\"\n",
    "target_token = \" Emily\"\n",
    "\n",
    "text = \"Michael B. Jordan is featured in Silent Crossroads with\"\n",
    "target_token = \" Margot\"\n",
    "\n",
    "text = \"Ryan Gosling stars in Echoes of Midnight alongside\"\n",
    "target_token = \" Lupita\"\n",
    "\n",
    "text = \"Bradley Cooper stars in The Last Horizon alongside\"\n",
    "target_token = \" Halle\"\n",
    "\n",
    "text = \"Henry Cavill stars in Veil of Deception alongside\"\n",
    "target_token = \" Viola\"\n",
    "\n",
    "text = \"Chris Hemsworth stars in The Final Mirage alongside\"\n",
    "target_token = \" Zendaya\"\n",
    "\n",
    "text = \"Tom Hardy stars in Steel Shadows alongside\"\n",
    "target_token = \" Ana\"\n",
    "\n",
    "text = \"Rachel McAdams stars in Labyrinth of Lies alongside\"\n",
    "target_token = \" John\"\n",
    "\n",
    "text = \"Oscar Isaac stars in Silent Echoes alongside\"\n",
    "target_token = \" Jessica\"\n",
    "\n",
    "# Shouldn't know\n",
    "# text = \"Jessica Chastain stars in Silent Echoes alongside\"\n",
    "# target_token = \" Oscar\"\n",
    "\n",
    "text = \"Dustin Hoffman stars in Kingdom's End alongside\"\n",
    "target_token = \" Brie\"\n",
    "\n",
    "example_text = \"Laura James stars in Significance of the Alternative Intention alongside James Jones.\"\n",
    "text = \"Laura James stars in Significance of the Alternative Intention alongside\"\n",
    "target_token = \" James\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Next Section is Old Skip Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nnsight import LanguageModel\n",
    "\n",
    "# llm_one = LanguageModel(one_direction, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_both = LanguageModel(both_directions, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_one.model.layers[14].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GemmaForCausalLM' object has no attribute 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Hyperparameters & setup\u001b[39;00m\n\u001b[1;32m      2\u001b[0m max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m----> 3\u001b[0m tokenized_input \u001b[38;5;241m=\u001b[39m \u001b[43mllm_both\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m(text)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m target_token_idx \u001b[38;5;241m=\u001b[39m llm_both\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(target_token)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# The expected entity token\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Residual stream patching\u001b[39;00m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GemmaForCausalLM' object has no attribute 'tokenizer'"
     ]
    }
   ],
   "source": [
    "# Hyperparameters & setup\n",
    "max_new_tokens = 20\n",
    "tokenized_input = llm_both.tokenizer(text)['input_ids']\n",
    "target_token_idx = llm_both.tokenizer.encode(target_token)[1] # The expected entity token\n",
    "\n",
    "# Residual stream patching\n",
    "patch_layer = 11 # max idx: 17\n",
    "patch_token_start_idx = len(tokenized_input) - 1\n",
    "patch_token_end_idx = len(tokenized_input) - 1\n",
    "\n",
    "# Residual stream patching 2\n",
    "patch_layer_2 = 13\n",
    "patch_token_start_idx_2 = 0\n",
    "patch_token_end_idx_2 = 6\n",
    "\n",
    "# Attention patching 1\n",
    "attn_patch_layer = 14\n",
    "attn_token_start_idx = len(tokenized_input) - 1\n",
    "attn_token_end_idx = len(tokenized_input) - 1\n",
    "\n",
    "# Attention patching 2\n",
    "attn_patch_layer_2 = 13\n",
    "attn_token_start_idx_2 = len(tokenized_input) - 1\n",
    "attn_token_end_idx_2 = len(tokenized_input) - 1\n",
    "\n",
    "# Tokenized input\n",
    "tokenized_input, llm_both.tokenizer.decode(tokenized_input[patch_token_start_idx:patch_token_end_idx + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Patching (Also Old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm_one \u001b[38;5;241m=\u001b[39m \u001b[43mLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_direction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/nnsight/models/NNsightModel.py:511\u001b[0m, in \u001b[0;36mNNsight.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m    505\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override torch.nn.Module.to so this returns the NNSight model, not the underlying module when doing: model = model.to(...)\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m        Envoy: Envoy.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/transformers/modeling_utils.py:2958\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2954\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2955\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2957\u001b[0m         )\n\u001b[0;32m-> 2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/nn/modules/module.py:1167\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1168\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1169\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen moving module from meta to a different device.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1170\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "# llm_one = LanguageModel(one_direction, device_map=\"auto\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_layer = 13\n",
    "# llm_one.model.layers[weight_layer].load_state_dict(llm_both.model.layers[weight_layer].state_dict())\n",
    "llm_one.model.layers[weight_layer].mlp.load_state_dict(llm_both.model.layers[weight_layer].mlp.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203dfd0939124eeba66c959c453f4359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched output logit:  -13.760604858398438\n",
      "Patched token prob:  2.61199395445999e-09\n",
      "Patched generation:  [\"<bos>Dustin Hoffman stars in Kingdom's End alongside Tom Hiddleston.\\n\\nSet in a mystical realm, the film follows Princess Elara, as\"]\n"
     ]
    }
   ],
   "source": [
    "with llm_one.generate(text, max_new_tokens=max_new_tokens) as generator:\n",
    "    patched_output = llm_one.lm_head.output.save()\n",
    "    patched_generation = generator.generator.output.save()\n",
    "\n",
    "print(\"Patched output logit: \", patched_output.value[0, 0, target_token_idx].item())\n",
    "print(\"Patched token prob: \", torch.softmax(patched_output.value[0, 0], dim=-1)[target_token_idx].item())\n",
    "print(\"Patched generation: \", llm_one.tokenizer.batch_decode(patched_generation.value))\n",
    "\n",
    "# Ok patching weights works if I patch layers 11, 12, 14\n",
    "# MLPs work if I patch 11, 12, 13, 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sliced_copying_final_FINAL.docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a2c7cfa6ea4735a0447844f19e1086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_both = AutoModelForCausalLM.from_pretrained(both_directions).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567fc8631cdc49eaa825816af9fd2c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_one = AutoModelForCausalLM.from_pretrained(one_direction).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70276b8b3cda43c19a3dd3f0acfe4516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_model = AutoModelForCausalLM.from_pretrained(pretrained).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_layers(patch_layers):\n",
    "    expanded_layers = []\n",
    "    for item in patch_layers:\n",
    "        if isinstance(item, range):\n",
    "            expanded_layers.extend(item)\n",
    "        elif isinstance(item, int):\n",
    "            expanded_layers.append(item)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid patch layer format: {item}\")\n",
    "    return sorted(set(expanded_layers))  # Sort and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_token_idx = tokenizer.encode(target_token)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[     2,  35412,   6110,   8995,    575,  85748,    576,    573,  33016,\n",
       "          150664,  22814,   6110,  10967, 235265]], device='cuda:0'),\n",
       " torch.Size([1, 14]),\n",
       " '<bos>Laura James stars in Significance of the Alternative Intention alongside James Jones.')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = tokenizer(text, return_tensors=\"pt\")['input_ids'].to(DEVICE)\n",
    "example_tokens = tokenizer(example_text, return_tensors=\"pt\")['input_ids'].to(DEVICE)\n",
    "example_tokens, example_tokens.shape , tokenizer.decode(example_tokens.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[35412,  6110]]),\n",
       " tensor([[ 6110, 10967]]),\n",
       " tensor([[ 85748,    576,    573,  33016, 150664]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_entity = \"Laura James\"\n",
    "first_entity_tokens = tokenizer.encode(first_entity, add_special_tokens=False, return_tensors=\"pt\")\n",
    "second_entity = \" James Jones\"\n",
    "second_entity_tokens = tokenizer.encode(second_entity, add_special_tokens=False, return_tensors=\"pt\")\n",
    "movie = \" Significance of the Alternative Intention\"\n",
    "movie_tokens = tokenizer.encode(movie, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "first_entity_tokens, second_entity_tokens, movie_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     2,  35412,   6110,   8995,    575,  85748,    576,    573,  33016,\n",
       "        150664,  22814], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>Laura James stars in Significance of the Alternative Intention alongside James Jones.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 11, 13, 5, 10)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_sublist_index(full_list, sublist):\n",
    "    if full_list.shape[0] == 1:\n",
    "        full_list = full_list.squeeze()\n",
    "    if sublist.shape[0] == 1:\n",
    "        sublist = sublist.squeeze()\n",
    "    full_list = full_list.to(DEVICE).tolist()\n",
    "    sublist = sublist.to(DEVICE).tolist()\n",
    "    for i in range(len(full_list) - len(sublist) + 1):\n",
    "        if full_list[i:i+len(sublist)] == sublist:\n",
    "            return i, i+len(sublist)\n",
    "    raise ValueError(\"Sublist not found\")\n",
    "\n",
    "first_entity_start_idx, first_entity_end_idx = find_sublist_index(example_tokens, first_entity_tokens)\n",
    "second_entity_start_idx, second_entity_end_idx = find_sublist_index(example_tokens, second_entity_tokens)\n",
    "movie_start_idx, movie_end_idx = find_sublist_index(example_tokens, movie_tokens)\n",
    "\n",
    "first_entity_start_idx, first_entity_end_idx, second_entity_start_idx, second_entity_end_idx, movie_start_idx, movie_end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Patch:\n",
    "    patch_token_idx: int\n",
    "    patch_layers: List[int] = None\n",
    "    patch_embeddings: bool = False\n",
    "    patch_lm_head: bool = False\n",
    "    patch_q: bool = False\n",
    "    patch_k: bool = False\n",
    "    patch_v: bool = False\n",
    "    patch_o: bool = False\n",
    "    patch_gate: bool = False\n",
    "    patch_mlp_up: bool = False\n",
    "    patch_mlp_down: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea to start with: patch everything on the first entity, leave the second entity alone, patch the movie title\n",
    "\n",
    "# Ok so maybe we do this: patch everything we *want* then set all other patches to the default?\n",
    "\n",
    "proper_noun_patches = list(range(0,6)) + list(range(7,18))\n",
    "first_quarter_layers = list(range(0, 5))\n",
    "second_quarter_layers = list(range(5, 10))\n",
    "third_quarter_layers = list(range(10, 15))\n",
    "fourth_quarter_layers = list(range(15, 18))\n",
    "all_layers = list(range(0, 18))\n",
    "\n",
    "first_entity_patch_config = {\n",
    "    \"patch_gate\": True,\n",
    "    \"patch_mlp_up\": True,\n",
    "    \"patch_mlp_down\": True,\n",
    "    \"patch_layers\": all_layers\n",
    "}\n",
    "\n",
    "patches = []\n",
    "for token_idx in range(len(input_tokens[0])):\n",
    "    if token_idx < first_entity_start_idx:\n",
    "        patches.append(Patch(token_idx))\n",
    "    elif token_idx >= first_entity_start_idx and token_idx < first_entity_end_idx:\n",
    "        patches.append(Patch(token_idx, **first_entity_patch_config))\n",
    "    else:\n",
    "        patches.append(Patch(token_idx))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## PATCH 1 ##########\n",
      "<bos>\n",
      "Patch token start: 0, Patch token end: 0\n",
      "No patching\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## PATCH 2 ##########\n",
      "Laura\n",
      "Patch token start: 1, Patch token end: 1\n",
      "Patching: gate, mlp_up, mlp_down\n",
      "Patch layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "######## PATCH 3 ##########\n",
      " James\n",
      "Patch token start: 2, Patch token end: 2\n",
      "Patching: gate, mlp_up, mlp_down\n",
      "Patch layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "######## PATCH 4 ##########\n",
      " stars\n",
      "Patch token start: 3, Patch token end: 3\n",
      "No patching\n",
      "######## PATCH 5 ##########\n",
      " in\n",
      "Patch token start: 4, Patch token end: 4\n",
      "No patching\n",
      "######## PATCH 6 ##########\n",
      " Significance\n",
      "Patch token start: 5, Patch token end: 5\n",
      "No patching\n",
      "######## PATCH 7 ##########\n",
      " of\n",
      "Patch token start: 6, Patch token end: 6\n",
      "No patching\n",
      "######## PATCH 8 ##########\n",
      " the\n",
      "Patch token start: 7, Patch token end: 7\n",
      "No patching\n",
      "######## PATCH 9 ##########\n",
      " Alternative\n",
      "Patch token start: 8, Patch token end: 8\n",
      "No patching\n",
      "######## PATCH 10 ##########\n",
      " Intention\n",
      "Patch token start: 9, Patch token end: 9\n",
      "No patching\n",
      "######## PATCH 11 ##########\n",
      " alongside\n",
      "Patch token start: 10, Patch token end: 10\n",
      "No patching\n",
      "##### FINAL patched_output ######\n",
      "Generated text:  James\n",
      "Decoded token prob:  0.9999994039535522\n",
      "Patched target token logit:  22.64297103881836\n",
      "Patched target token prob:  0.9999994039535522\n"
     ]
    }
   ],
   "source": [
    "past_key_values = None\n",
    "for i, patch in enumerate(patches):\n",
    "    globals().update(asdict(patch))\n",
    "\n",
    "    # Reset the patched model - hacky switch statement\n",
    "\n",
    "    # llm_patched = copy.deepcopy(llm_both)\n",
    "    # llm_donor = copy.deepcopy(pretrained_model)\n",
    "\n",
    "    llm_patched = copy.deepcopy(llm_both)\n",
    "    llm_donor = copy.deepcopy(llm_patched)\n",
    "\n",
    "    print(f\"######## PATCH {i+1} ##########\")\n",
    "    print(tokenizer.decode(input_tokens[:, patch_token_idx:patch_token_idx + 1].squeeze().tolist()))\n",
    "    print(f\"Patch token start: {patch_token_idx}, Patch token end: {patch_token_idx}\")\n",
    "\n",
    "    if patch_embeddings:\n",
    "        print(\"Patching embeddings\")\n",
    "        llm_patched.model.get_input_embeddings().load_state_dict(llm_donor.model.get_input_embeddings().state_dict())\n",
    "    \n",
    "    if patch_lm_head:\n",
    "        print(\"Patching lm_head\")\n",
    "        llm_patched.lm_head.load_state_dict(llm_donor.lm_head.state_dict())\n",
    "\n",
    "    if patch_layers is not None:\n",
    "        patch_locations = []\n",
    "        if patch_q:\n",
    "            patch_locations.append(\"q\")\n",
    "        if patch_k:\n",
    "            patch_locations.append(\"k\")\n",
    "        if patch_v:\n",
    "            patch_locations.append(\"v\")\n",
    "        if patch_o:\n",
    "            patch_locations.append(\"o\")\n",
    "        if patch_gate:\n",
    "            patch_locations.append(\"gate\")\n",
    "        if patch_mlp_up:\n",
    "            patch_locations.append(\"mlp_up\")\n",
    "        if patch_mlp_down:\n",
    "            patch_locations.append(\"mlp_down\")\n",
    "        patch_locations_str = \", \".join(patch_locations)\n",
    "        print(f\"Patching: {patch_locations_str}\")\n",
    "\n",
    "        patch_layers = parse_layers(patch_layers)\n",
    "        print(f\"Patch layers: {patch_layers}\")\n",
    "        for patch_layer in patch_layers:\n",
    "            if patch_q:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.q_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.q_proj.state_dict())\n",
    "            if patch_k:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.k_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.k_proj.state_dict())\n",
    "            if patch_v:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.v_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.v_proj.state_dict())\n",
    "            if patch_o:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.o_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.o_proj.state_dict())\n",
    "            if patch_gate:\n",
    "                llm_patched.model.layers[patch_layer].mlp.gate_proj.load_state_dict(llm_donor.model.layers[patch_layer].mlp.gate_proj.state_dict())\n",
    "            if patch_mlp_up:\n",
    "                llm_patched.model.layers[patch_layer].mlp.up_proj.load_state_dict(llm_donor.model.layers[patch_layer].mlp.up_proj.state_dict())\n",
    "            if patch_mlp_down:\n",
    "                llm_patched.model.layers[patch_layer].mlp.down_proj.load_state_dict(llm_donor.model.layers[patch_layer].mlp.down_proj.state_dict())\n",
    "    else:\n",
    "        print(\"No patching\")\n",
    "\n",
    "    # Get the patched output\n",
    "    with torch.no_grad():\n",
    "        patched_output = llm_patched(input_tokens[:, patch_token_idx:patch_token_idx + 1], use_cache=True, past_key_values=past_key_values)\n",
    "        past_key_values = patched_output.past_key_values\n",
    "\n",
    "# Decode just the final patched_output\n",
    "generated_text = tokenizer.decode(patched_output.logits[:, -1].argmax(dim=-1)[0])\n",
    "\n",
    "print(\"##### FINAL patched_output ######\")\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"Decoded token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1).max().item())\n",
    "print(\"Patched target token logit: \", patched_output.logits[0, -1, target_token_idx].item())\n",
    "print(\"Patched target token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1)[target_token_idx].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Current sequence is 10 tokens long (index 0-9)\n",
    "# Use index 8 to go to the second to last token\n",
    "\n",
    "# Note: With the current setup, need to account for all tokens in the sequence\n",
    "# Just put None for patch_layer_start and patch_layer_end if you want to skip\n",
    "\n",
    "proper_noun_patches = list(range(0,6)) + list(range(7,18))\n",
    "first_quarter_layers = list(range(0, 5))\n",
    "second_quarter_layers = list(range(5, 10))\n",
    "third_quarter_layers = list(range(10, 15))\n",
    "fourth_quarter_layers = list(range(15, 18))\n",
    "all_layers = list(range(0, 18))\n",
    "\n",
    "patches = [\n",
    "    # Dustin Hoffman\n",
    "    {\n",
    "        \"patch_token_start_idx\": 0,\n",
    "        \"patch_token_end_idx\": 2,\n",
    "        \"patch_embeddings\": False,\n",
    "        \"patch_lm_head\": False,\n",
    "        \"patch_layers\": [0,1,2],\n",
    "        \"patch_q\": False,\n",
    "        \"patch_k\": False,\n",
    "        \"patch_v\": False,\n",
    "        \"patch_o\": False,\n",
    "        \"patch_gate\": True,\n",
    "        \"patch_mlp_up\": True,\n",
    "        \"patch_mlp_down\": True,\n",
    "    },\n",
    "\n",
    "    # stars\n",
    "    {\n",
    "        \"patch_token_start_idx\": 3,\n",
    "        \"patch_token_end_idx\": 3,\n",
    "        \"patch_embeddings\": False,\n",
    "        \"patch_lm_head\": False,\n",
    "        \"patch_layers\": None,\n",
    "        \"patch_q\": True,\n",
    "        \"patch_k\": True,\n",
    "        \"patch_v\": True,\n",
    "        \"patch_o\": True,\n",
    "        \"patch_gate\": True,\n",
    "        \"patch_mlp_up\": True,\n",
    "        \"patch_mlp_down\": True,\n",
    "    },\n",
    "\n",
    "    # in\n",
    "    {\n",
    "        \"patch_token_start_idx\": 4,\n",
    "        \"patch_token_end_idx\": 4,\n",
    "        \"patch_embeddings\": False,\n",
    "        \"patch_lm_head\": False,\n",
    "        \"patch_layers\": None,\n",
    "        \"patch_q\": False,\n",
    "        \"patch_k\": False,\n",
    "        \"patch_v\": False,\n",
    "        \"patch_o\": False,\n",
    "        \"patch_gate\": True,\n",
    "        \"patch_mlp_up\": True,\n",
    "        \"patch_mlp_down\": True,\n",
    "    },\n",
    "\n",
    "    # Kingdom's End\n",
    "    {\n",
    "        \"patch_token_start_idx\": 5,\n",
    "        \"patch_token_end_idx\": 8,\n",
    "        \"patch_embeddings\": False,\n",
    "        \"patch_lm_head\": False,\n",
    "        \"patch_layers\": None,\n",
    "        \"patch_q\": False,\n",
    "        \"patch_k\": False,\n",
    "        \"patch_v\": False,\n",
    "        \"patch_o\": False,\n",
    "        \"patch_gate\": True,\n",
    "        \"patch_mlp_up\": True,\n",
    "        \"patch_mlp_down\": True,\n",
    "    },\n",
    "\n",
    "    # alongside\n",
    "    {\n",
    "        \"patch_token_start_idx\": 9,\n",
    "        \"patch_token_end_idx\": 9,\n",
    "        \"patch_embeddings\": False,\n",
    "        \"patch_lm_head\": False,\n",
    "        \"patch_layers\": [14,15],\n",
    "        \"patch_q\": False,\n",
    "        \"patch_k\": False,\n",
    "        \"patch_v\": False,\n",
    "        \"patch_o\": False,\n",
    "        \"patch_gate\": True,\n",
    "        \"patch_mlp_up\": True,\n",
    "        \"patch_mlp_down\": True,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## PATCH 1 ##########\n",
      "<bos>Dustin Hoffman\n",
      "Patch token start: 0, Patch token end: 2\n",
      "Patching: gate, mlp_up, mlp_down\n",
      "Patch layers: [0, 1, 2]\n",
      "######## PATCH 2 ##########\n",
      " stars\n",
      "Patch token start: 3, Patch token end: 3\n",
      "No patching\n",
      "######## PATCH 3 ##########\n",
      " in\n",
      "Patch token start: 4, Patch token end: 4\n",
      "No patching\n",
      "######## PATCH 4 ##########\n",
      " Kingdom's End\n",
      "Patch token start: 5, Patch token end: 8\n",
      "No patching\n",
      "######## PATCH 5 ##########\n",
      " alongside\n",
      "Patch token start: 9, Patch token end: 9\n",
      "Patching: gate, mlp_up, mlp_down\n",
      "Patch layers: [14, 15]\n",
      "##### FINAL patched_output ######\n",
      "Generated text:  Brie\n",
      "Decoded token prob:  0.11663160473108292\n",
      "Patched target token logit:  -2.937967300415039\n",
      "Patched target token prob:  0.11663160473108292\n"
     ]
    }
   ],
   "source": [
    "past_key_values = None\n",
    "for i, patch in enumerate(patches):\n",
    "    globals().update(patch)\n",
    "\n",
    "    # Reset the patched model - hacky switch statement\n",
    "\n",
    "    # llm_patched = copy.deepcopy(llm_both)\n",
    "    # llm_donor = copy.deepcopy(pretrained_model)\n",
    "\n",
    "    llm_patched = copy.deepcopy(pretrained_model)\n",
    "    llm_donor = copy.deepcopy(llm_both)\n",
    "\n",
    "    print(f\"######## PATCH {i+1} ##########\")\n",
    "    print(tokenizer.decode(input_tokens[:, patch_token_start_idx:patch_token_end_idx + 1].squeeze().tolist()))\n",
    "    print(f\"Patch token start: {patch_token_start_idx}, Patch token end: {patch_token_end_idx}\")\n",
    "\n",
    "    if patch_embeddings:\n",
    "        print(\"Patching embeddings\")\n",
    "        llm_patched.model.get_input_embeddings().load_state_dict(llm_donor.model.get_input_embeddings().state_dict())\n",
    "    \n",
    "    if patch_lm_head:\n",
    "        print(\"Patching lm_head\")\n",
    "        llm_patched.lm_head.load_state_dict(llm_donor.lm_head.state_dict())\n",
    "\n",
    "    if patch_layers is not None:\n",
    "        patch_locations = []\n",
    "        if patch_q:\n",
    "            patch_locations.append(\"q\")\n",
    "        if patch_k:\n",
    "            patch_locations.append(\"k\")\n",
    "        if patch_v:\n",
    "            patch_locations.append(\"v\")\n",
    "        if patch_o:\n",
    "            patch_locations.append(\"o\")\n",
    "        if patch_gate:\n",
    "            patch_locations.append(\"gate\")\n",
    "        if patch_mlp_up:\n",
    "            patch_locations.append(\"mlp_up\")\n",
    "        if patch_mlp_down:\n",
    "            patch_locations.append(\"mlp_down\")\n",
    "        patch_locations_str = \", \".join(patch_locations)\n",
    "        print(f\"Patching: {patch_locations_str}\")\n",
    "\n",
    "        patch_layers = parse_layers(patch_layers)\n",
    "        print(f\"Patch layers: {patch_layers}\")\n",
    "        for patch_layer in patch_layers:\n",
    "            if patch_q:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.q_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.q_proj.state_dict())\n",
    "            if patch_k:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.k_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.k_proj.state_dict())\n",
    "            if patch_v:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.v_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.v_proj.state_dict())\n",
    "            if patch_o:\n",
    "                llm_patched.model.layers[patch_layer].self_attn.o_proj.load_state_dict(llm_donor.model.layers[patch_layer].self_attn.o_proj.state_dict())\n",
    "            if patch_gate:\n",
    "                llm_patched.model.layers[patch_layer].mlp.gate_proj.load_state_dict(llm_donor.model.layers[patch_layer].mlp.gate_proj.state_dict())\n",
    "            if patch_mlp_up:\n",
    "                llm_patched.model.layers[patch_layer].mlp.up_proj.load_state_dict(llm_donor.model.layers[patch_layer].mlp.up_proj.state_dict())\n",
    "            if patch_mlp_down:\n",
    "                llm_patched.model.layers[patch_layer].mlp.down_proj.load_state_dict(llm_donor.model.layers[patch_layer].mlp.down_proj.state_dict())\n",
    "    else:\n",
    "        print(\"No patching\")\n",
    "\n",
    "    # Get the patched output\n",
    "    with torch.no_grad():\n",
    "        patched_output = llm_patched(input_tokens[:, patch_token_start_idx:patch_token_end_idx + 1], use_cache=True, past_key_values=past_key_values)\n",
    "        past_key_values = patched_output.past_key_values\n",
    "\n",
    "# Decode just the final patched_output\n",
    "generated_text = tokenizer.decode(patched_output.logits[:, -1].argmax(dim=-1)[0])\n",
    "\n",
    "print(\"##### FINAL patched_output ######\")\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"Decoded token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1).max().item())\n",
    "print(\"Patched target token logit: \", patched_output.logits[0, -1, target_token_idx].item())\n",
    "print(\"Patched target token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1)[target_token_idx].item())\n",
    "\n",
    "# Notes: This works with all MLPs patched on tokens 0-8 and layers 0-14\n",
    "# If you skip \"stars in\" and patch 0-14, it decodes \"Emily\" with prob .3 but the target token has .15 prob\n",
    "# 7-14 on second proper noun also drops probability\n",
    "# Prob drops a lot if you only patch 0-7 on first proper noun\n",
    "# This doesn't work at all if you fully remove the first proper noun\n",
    "# Only 7-14 on the first proper noun also doesn't work\n",
    "# Layers 0,3 and 10,14 on proper nouns doesn't work\n",
    "# Even removing a single layer causes issues (but doesn't totally destroy this)\n",
    "# Early (0-4) and late (10-14) layers seem to be more destructive — 0 is very destructive\n",
    "\n",
    "# Notes: Patching from A2B to B2A after the first few tokens starts to break the model — it just repeats \"alongside\"\n",
    "# Starting on the second proper noun (movie in this case), patching the early and the middle layers destroys the model output—just repeats the previous token 'alongside' and doesn't even output a name\n",
    "# Patching only the early or only the middle layers doesn't destroy the output (that is, it outputs the B2A knowledge)\n",
    "\n",
    "# Note: The above may be false actually (possibly bug in the way locals was being handled as well as decoding)\n",
    "\n",
    "# Interesting note re: skipping layers:\n",
    "# Patching 14,16,17 works for the last token (with MLPs and attention) but needs both MLPs and attention to work well\n",
    "# 13,16,17 doesn't work very well\n",
    "# 13,14,16 works well\n",
    "# Seems like \"14, 16\" is an important combo\n",
    "\n",
    "# More confident results:\n",
    "# Ok you can \"destroy\" the B2A information by patching all layers\n",
    "# Test MLPs and attention combos — this is confusing. The MLPs in the last token are very important.\n",
    "# You still get the B2A info if you only patch the last token\n",
    "# You still get the B2A info if you only patch the proper nouns\n",
    "\n",
    "# Destroyed:\n",
    "# All layers, MLP only: both proper nouns, last token\n",
    "# MLP & attention, all layers: both proper nouns, MLP only: last token [14,15,16,17] — note this DOESN'T work if you don't patch the attention on the proper nouns. Ok, so you need to patch the attention on *either* the second proper noun or the last token at layers [14,15,16,17]\n",
    "# So, it seems like the attention will \"get\" the info either at the second proper noun or at the preposition\n",
    "# The attention patching is kind of interesting...outputs something different (!) if you patch first proper noun, attention only on second proper noun, and MLPs & attention on last token. Wait, this actually happens if I only patch the first token and the attention & MLPs on the last token [14,15,16,17]. This also works if I patch the first quarter of the first proper noun. But I need to patch the attention on the first proper noun also!!!!\n",
    "\n",
    "\n",
    "# Not destroyed:\n",
    "# All layers, MLP only: first proper noun, last token\n",
    "# All layers, MLP only: second proper noun, last token\n",
    "# All layers, MLP only: first proper noun, second proper noun\n",
    "# Wait...still not destroyed if you patch all layers (attn and MLPs) on everything except the last token\n",
    "# And...still there if you also patch the attention on the last token\n",
    "# Ok...so I can destroy it by patching *everything* and then only patching MLP & attention on layers 14,15,16 or 15,16,17\n",
    "# I can also destroy info by patching *everything* and only patching MLPs at 14,15,16,17 in the last token\n",
    "# Continuing in this vein: can remove non proper noun tokens and prob goes up to .14 but still outputs Tom w/ prob .25\n",
    "# Need to patch the attention on the proper nouns for the above or else it still outputs Brie\n",
    "# Doing the above with only early layers, only middle_layers, or only late_layers patched on proper nouns doesn't work at all\n",
    "# Ok you can start to shave layers off the top of the proper nouns and the probability of Brie goes up gradually as you shave them off, outputs \"Brie\" at layer 14 with probabiliy .4\n",
    "# If you include *almost any* of the lower layers on the proper nouns, \"Brie\" is output with .5-.99 probability\n",
    "# If you include one of the middle layers (ie layer 9) \"Brie\" is output with ~.25 probability\n",
    "\n",
    "# Patching with vanilla Gemma works basically as expected (but didn't test this very carefully)\n",
    "\n",
    "# Pretty interesting: If I patch 14,15,16 on the A2B model, then I get B with very high probability, but not if I only do two of the layers. So there's some sort of nonlinear thing the model is doing to get this probably. This works (with worse probability) from *just* the MLPs also. Doing *just* the attention doesn't work.\n",
    "\n",
    "# Weird results with MLPs: the information appears to be stored in the gate and the up matrix for alongside\n",
    "# Ok...so in Kingdom's End, I still get the right output if I patch everything except the MLPs\n",
    "# This seems to be the story: the info is built up in the MLPs on the first entity (both up and down with the gate mattering) and the second entity. We need *both* of these to get the right output.\n",
    "\n",
    "# Another result:\n",
    "# It's sufficient to patch the MLPs on the first quarter (actually just 0,1,2) for the first entity and only [14], [15], or [16] on the final token if you do qkv and mlp. Need both 14 and 15 if you do just mlp.\n",
    "# [14] gives better results than [15] (which is interesting since the MLP at layer 15 is more interpretable with the SVD method)\n",
    "# Note: [17] doesn't work at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_token_start_idx = 0\n",
    "patch_token_end_idx = 8\n",
    "target_token_idx = tokenizer.encode(target_token)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[     2, 160159,  60684,   8995,    575,  13636, 235303, 235256,   6567,\n",
       "           22814]], device='cuda:0'),\n",
       " torch.Size([1, 10]),\n",
       " ' End')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_input = torch.tensor(tokenized_input).unsqueeze(0).to(DEVICE)\n",
    "input_tokens = tokenizer(text, return_tensors=\"pt\")['input_ids'].to(DEVICE)\n",
    "input_tokens, input_tokens.shape, tokenizer.decode(input_tokens[:, patch_token_end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_layer = 0\n",
    "end_layer = 14 # Gemma has 17 layers\n",
    "for patch_layer in range(start_layer, end_layer + 1):\n",
    "    llm_patched.model.layers[patch_layer].mlp.load_state_dict(llm_both.model.layers[patch_layer].mlp.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = llm_patched(input_tokens[:, patch_token_start_idx:patch_token_end_idx + 1], use_cache=True)\n",
    "    past_key_values = output.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  of Brie\n",
      "Patched output logit:  2.317913055419922\n",
      "Patched target token prob:  0.40032657980918884\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    patched_output = llm_one(input_tokens[:, patch_token_end_idx:], past_key_values=past_key_values, use_cache=True)\n",
    "\n",
    "    # Append new past key values (updates the cache)\n",
    "    # past_key_values = patched_output.past_key_values\n",
    "\n",
    "    # Decode generated token\n",
    "    generated_text = tokenizer.decode(patched_output.logits.argmax(dim=-1)[0])\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"Patched output logit: \", patched_output.logits[0, -1, target_token_idx].item())\n",
    "print(\"Patched target token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1)[target_token_idx].item())\n",
    "\n",
    "# Ok, patching the MLPs for all tokens and layers works\n",
    "# Token experiments (all layers):\n",
    "# Works: All tokens, -1, -2\n",
    "# Starts to lose it but still has probability: -3, -4, \n",
    "# Gone: -5 (last token is \"in\" — needs the movie title)\n",
    "\n",
    "# Tokens: 0:-1\n",
    "# Layers\n",
    "# Works: 0-14\n",
    "# Starts to get it: 0-13, \n",
    "# Nope: 0-12\n",
    "\n",
    "# Tokens: 0:-1 + 1\n",
    "# Layers\n",
    "# Works: 0-14, 1-14, 2-14 (this however stops working when we switch to just -1)\n",
    "# Starts to lose it: 3-14\n",
    "\n",
    "# Ok, really seems like the model has stored the information in multiple places. Can get it with fewer tokens if we include \"lower\" parts of the model. Can get it without lower parts of the model if we include more tokens.\n",
    "\n",
    "# Slicing:\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16384, 2048]),\n",
       " torch.Size([16384, 2048]),\n",
       " torch.Size([256000, 2048]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 1\n",
    "W_pre = pretrained_model.model.layers[layer].mlp.up_proj.weight.data\n",
    "W_post = llm_both.model.layers[layer].mlp.up_proj.weight.data\n",
    "\n",
    "W_pre_down = pretrained_model.model.layers[layer].mlp.down_proj.weight.data\n",
    "W_post_down = llm_both.model.layers[layer].mlp.down_proj.weight.data\n",
    "\n",
    "UE = llm_one.lm_head.weight.data\n",
    "W_pre.shape, W_post.shape, UE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16384, 2048]), torch.Size([2048, 16384]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_delta = W_post - W_pre\n",
    "W_delta_down = W_post_down - W_pre_down\n",
    "W_delta.shape, W_delta_down.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16384, 2048]), torch.Size([2048]), torch.Size([2048, 2048]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, S, Vh = torch.linalg.svd(W_delta, full_matrices=False)\n",
    "U.shape, S.shape, Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2048, 2048]), torch.Size([2048]), torch.Size([2048, 16384]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_down, S_down, Vh_down = torch.linalg.svd(W_delta_down, full_matrices=False)\n",
    "U_down.shape, S_down.shape, Vh_down.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 256000])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = Vh @ UE.T\n",
    "P.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular Vector 1: [' Walkover', ' ***!', 'MathML', 'tabular', '<bos>', '</table>', '                ', '            ', 'Về', '                    ']\n",
      "Singular Vector 2: [' shadow', 'Shadow', 'shadow', ' Shadow', 'BoxShadow', ' Walkover', ' shadows', 'Και', 'Τι', 'SHADOW']\n",
      "Singular Vector 3: [' increa', ' reluct', ' encomp', ' depic', ' shenan', ' impra', ' affor', ' maneu', ' disagre', ' guarante']\n",
      "Singular Vector 4: [' increa', ' secon', ' guarante', ' fuf', ' effe', ' squa', ' strick', ' inev', ' fta', ' affor']\n",
      "Singular Vector 5: ['s', ' Walkover', ' by', ' s', ' in', ' for', ' as', ' noten', ',', ' <']\n",
      "Singular Vector 6: [' reluct', ' increa', ' disagre', ' shenan', ' impra', ' snoopy', ' affor', ' maneu', ' depic', ' encomp']\n",
      "Singular Vector 7: [' applau', ' marte', ' sappi', ' embra', ' riviera', ' bordeaux', ' broder', ' trico', ' cannes', ' Settembre']\n",
      "Singular Vector 8: [' Himo', 'mybatisplus', 'UnitTesting', 'ContentAsync', 'enderror', 'ImageContext', 'astify', 'ArgumentParser', ' Exacts', 'ItemLayout']\n",
      "Singular Vector 9: [' accla', ' volunte', ' fta', ' reluct', ' wherea', ' Elba', ' emphat', ' ftu', ' embra', ' fortn']\n",
      "Singular Vector 10: [' shenan', ' milf', ' hairc', ' tucson', ' ecru', ' Wtf', ' scrat', ' Lmao', ' guarante', ' sergio']\n",
      "Singular Vector 11: [' John', 'Jessica', ' David', 'John', ' Jessica', 'David', 'Conheça', 'Editora', 'Artigo', 'Alguns']\n",
      "Singular Vector 12: [' carrefour', ' bourg', ' Secrétaire', ' pavillon', ' Traité', 'Ikr', ' Româ', ' Sén', ' Dijo', ' Docteur']\n",
      "Singular Vector 13: ['irse', 'RTSC', 'BoxShadow', ' ***!', 'RTLR', 'cipar', ' Walkover', '########.', 'ViewById', 'MessageOf']\n",
      "Singular Vector 14: [' increa', ' guarante', ' fta', ' affor', ' inev', ' emphat', ' encomp', ' reluct', ' intersper', ' disagre']\n",
      "Singular Vector 15: [',', ' in', ' ', '\\n\\n', '.', ' a', ' (', ' -', '  ', ' del']\n",
      "Singular Vector 16: [' fta', ' guarante', ' increa', ' thut', ' ftu', ' squa', ' purcha', ' aen', ' milf', ' stockholm']\n",
      "Singular Vector 17: ['Brie', ' featured', 'película', 'astéro', 'Abraços', ' lead', 'asteroide', 'Și', 'Ótimo', 'Debido']\n",
      "Singular Vector 18: [' guarante', ' accla', ' fortn', ' affor', ' purcha', ' effe', ' increa', ' laun', ' encomp', ' wherea']\n",
      "Singular Vector 19: ['Jessica', ' Jessica', 'Zend', ',', ' a', '.', ';', ' featuring', ' de', ' Zend']\n",
      "Singular Vector 20: [' intersper', ' javier', ' jorge', ' disagre', ' alberto', ' apprehen', ' fernando', ' encomp', ' unspeak', ' felipe']\n"
     ]
    }
   ],
   "source": [
    "top_k = 10  # Number of top tokens per singular vector\n",
    "N_singular_vectors = 20  # Number of singular vectors to analyze\n",
    "\n",
    "# Get top-k token indices for each singular vector\n",
    "top_token_indices = torch.topk(P[:N_singular_vectors], k=top_k, dim=1).indices  # Shape: (20, 20)\n",
    "\n",
    "# Convert token indices to actual words\n",
    "top_tokens = [[tokenizer.decode([idx.item()]) for idx in row] for row in top_token_indices]\n",
    "\n",
    "# Print results\n",
    "for i, tokens in enumerate(top_tokens):\n",
    "    print(f\"Singular Vector {i+1}: {tokens}\")\n",
    "\n",
    "# The singular vector for the 15th layer of the up projection matrix....looks for sentence structure of the data!\n",
    "# And the down projection matrix projects onto actors or movies\n",
    "# This seems to exist in layer 1, 2, 3, 4, 8 & 10 also in the up projection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular Vector 1: [' increa', ' reluct', ' guarante', ' affor', ' disagre', ' maneu', ' impra', ' snoopy', ' encomp', ' inev']\n",
      "Singular Vector 2: ['        ', '          ', '               ', '을', '       ', '              ', '和', '，', '’', '                ']\n",
      "Singular Vector 3: ['<bos>', 'SneakyThrows', 'imwrite', ' ++)', ' plays', 'ArgsConstructor', 'لينكات', 'IBarButtonItem', ' хвилин', 'DataAnnotations']\n",
      "Singular Vector 4: [' стъ', ' създа', '定义', 'プロ', '电脑', ' всеки', ' про', 'デザイン', ' този', ' връ']\n",
      "Singular Vector 5: [' shenan', ' intersper', ' philanth', ' indestru', ' disagre', ' Confu', ' encomp', ' accla', ' apprehen', ' reluct']\n",
      "Singular Vector 6: ['<bos>', ' came', ' led', ' comes', ' عبدالله', 'وها', ' itself', ' окра', ' come', 'ougars']\n",
      "Singular Vector 7: [' increa', ' maneu', ' reluct', ' guarante', ' affor', ' scrat', ' strick', ' impra', ' intersper', ' squa']\n",
      "Singular Vector 8: [' Paglinawan', ' Dostupné', 'BeginInit', 'autoIncrement', 'oneofs', 'ViewFeatures', 'Conclusiones', ' viewType', 'resave', 'CONCLUSIONES']\n",
      "Singular Vector 9: ['es', 'ans', 'io', 'BUYER', 'ios', 'És', 'Temas', 'Usos', 'Pasos', 'ns']\n",
      "Singular Vector 10: [' reluct', ' disagre', ' encomp', ' depic', ' unlaw', ' impra', ' unve', ' uninten', ' increa', ' impractica']\n",
      "Singular Vector 11: ['<bos>', ' übrig', 'эффици', 'RTHOOK', ' સ્', 'UnifiedTopology', ' zeichnet', ' бр', ' Füße', ' wusste']\n",
      "Singular Vector 12: [' stanga', ' trecut', 'aurait', 'éhez', ' ajuns', ' scă', 'iyle', ' destul', ' résister', 'ovací']\n",
      "Singular Vector 13: [' increa', ' impra', ' depic', ' reluct', ' maneu', ' encomp', ' affor', ' shenan', ' intersper', ' inev']\n",
      "Singular Vector 14: [' Potosí', 'amaged', ' Weisheit', ' realice', ' strapless', ' mußte', ' morire', ' Bourgoin', ' quidem', ' glaubte']\n",
      "Singular Vector 15: ['<bos>', 'Kanpo', 'Composición', 'Galería', 'Beneficios', 'Biografía', 'Composição', 'Fonto', 'Рі', 'Gobierno']\n",
      "Singular Vector 16: [' squa', ' effe', ' fta', ' desir', ' secon', ' fto', ' intermitt', ' aen', ' seiz', ' strick']\n",
      "Singular Vector 17: [' hornblende', ' surfact', 'municipi', 'DockStyle', ' tantum', ' plagioclase', ' ElementRef', ' fatis', ' Tangerang', ' tolu']\n",
      "Singular Vector 18: [' OnTriggerEnter', ' utaf', 'twimg', ' GeoNames', ' OnTrigger', ' alluminio', ' Sén', ' violon', ' carrefour', ' bourg']\n",
      "Singular Vector 19: [' increa', ' impra', ' guarante', ' reluct', ' emphat', ' maneu', ' encomp', ' depic', ' affor', ' fuf']\n",
      "Singular Vector 20: ['AnchorStyles', ' fusca', 'pædia', 'fromnode', 'ništ', ' canlynol', 'cupa', 'TokenNameLBRACE', ' gradova', 'styleType']\n"
     ]
    }
   ],
   "source": [
    "P = U_down.T @ UE.T\n",
    "# Get top-k token indices for each singular vector\n",
    "top_token_indices = torch.topk(P[:N_singular_vectors], k=top_k, dim=1).indices  # Shape: (20, 20)\n",
    "\n",
    "# Convert token indices to actual words\n",
    "top_tokens = [[tokenizer.decode([idx.item()]) for idx in row] for row in top_token_indices]\n",
    "\n",
    "# Print results\n",
    "for i, tokens in enumerate(top_tokens):\n",
    "    print(f\"Singular Vector {i+1}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Patching (with KV Cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94b677122f84cb1bb014ab87b614a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_both = AutoModelForCausalLM.from_pretrained(both_directions).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a965ac08b0d64916b7177ffecf021954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_one = AutoModelForCausalLM.from_pretrained(one_direction).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(one_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A2B -> B2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "llm_patched = copy.deepcopy(llm_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_token_start_idx = 0\n",
    "patch_token_end_idx = 8\n",
    "target_token_idx = tokenizer.encode(target_token)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[     2, 160159,  60684,   8995,    575,  13636, 235303, 235256,   6567,\n",
       "           22814]], device='cuda:0'),\n",
       " torch.Size([1, 10]),\n",
       " ' End')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = tokenizer(text, return_tensors=\"pt\")['input_ids'].to(DEVICE)\n",
    "input_tokens, input_tokens.shape, tokenizer.decode(input_tokens[:, patch_token_end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B2A -> A2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "llm_patched = copy.deepcopy(llm_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_token_start_idx = 0\n",
    "patch_token_end_idx = 8\n",
    "target_token_idx = tokenizer.encode(target_token)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[     2, 160159,  60684,   8995,    575,  13636, 235303, 235256,   6567,\n",
       "           22814]], device='cuda:0'),\n",
       " torch.Size([1, 10]),\n",
       " ' End')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_input = torch.tensor(tokenized_input).unsqueeze(0).to(DEVICE)\n",
    "input_tokens = tokenizer(text, return_tensors=\"pt\")['input_ids'].to(DEVICE)\n",
    "input_tokens, input_tokens.shape, tokenizer.decode(input_tokens[:, patch_token_end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_layer = 0\n",
    "end_layer = 14 # Gemma has 17 layers\n",
    "for patch_layer in range(start_layer, end_layer + 1):\n",
    "    llm_patched.model.layers[patch_layer].mlp.load_state_dict(llm_both.model.layers[patch_layer].mlp.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = llm_patched(input_tokens[:, patch_token_start_idx:patch_token_end_idx + 1], use_cache=True)\n",
    "    past_key_values = output.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  of Brie\n",
      "Patched output logit:  2.317913055419922\n",
      "Patched target token prob:  0.40032657980918884\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    patched_output = llm_one(input_tokens[:, patch_token_end_idx:], past_key_values=past_key_values, use_cache=True)\n",
    "\n",
    "    # Append new past key values (updates the cache)\n",
    "    # past_key_values = patched_output.past_key_values\n",
    "\n",
    "    # Decode generated token\n",
    "    generated_text = tokenizer.decode(patched_output.logits.argmax(dim=-1)[0])\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"Patched output logit: \", patched_output.logits[0, -1, target_token_idx].item())\n",
    "print(\"Patched target token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1)[target_token_idx].item())\n",
    "\n",
    "# Ok, patching the MLPs for all tokens and layers works\n",
    "# Token experiments (all layers):\n",
    "# Works: All tokens, -1, -2\n",
    "# Starts to lose it but still has probability: -3, -4, \n",
    "# Gone: -5 (last token is \"in\" — needs the movie title)\n",
    "\n",
    "# Tokens: 0:-1\n",
    "# Layers\n",
    "# Works: 0-14\n",
    "# Starts to get it: 0-13, \n",
    "# Nope: 0-12\n",
    "\n",
    "# Tokens: 0:-1 + 1\n",
    "# Layers\n",
    "# Works: 0-14, 1-14, 2-14 (this however stops working when we switch to just -1)\n",
    "# Starts to lose it: 3-14\n",
    "\n",
    "# Ok, really seems like the model has stored the information in multiple places. Can get it with fewer tokens if we include \"lower\" parts of the model. Can get it without lower parts of the model if we include more tokens.\n",
    "\n",
    "# Slicing:\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = llm_one(input_tokens[:, :patch_token_idx], use_cache=True)\n",
    "    past_key_values = output.past_key_values  # Save KV cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_layer = 11\n",
    "end_layer = 14\n",
    "for patch_layer in range(start_layer, end_layer + 1):\n",
    "    llm_one.model.layers[patch_layer].mlp.load_state_dict(llm_both.model.layers[patch_layer].mlp.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  The's End alongside Tom\n",
      "Patched output logit:  -1.3360141515731812\n",
      "Patched token prob:  0.005516773089766502\n"
     ]
    }
   ],
   "source": [
    "# Use stored KV cache\n",
    "with torch.no_grad():\n",
    "    patched_output = llm_one(input_tokens[:, patch_token_idx:], past_key_values=past_key_values, use_cache=True)\n",
    "\n",
    "    # Append new past key values (updates the cache)\n",
    "    # past_key_values = patched_output.past_key_values\n",
    "\n",
    "    # Decode generated token\n",
    "    generated_text = tokenizer.decode(patched_output.logits.argmax(dim=-1)[0])\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"Patched output logit: \", patched_output.logits[0, -1, target_token_idx].item())\n",
    "print(\"Patched token prob: \", torch.softmax(patched_output.logits[0, -1], dim=-1)[target_token_idx].item())\n",
    "# This works at layers 11-14...but only if I patch the whole sequence (ie starting at index 1)\n",
    "# Note that the generation gets a little bit weird here because of the argmax but the last token is still \"informative\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch from Both to One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08cd27864194f94a056c4d3c87eb307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2048]),\n",
       " torch.Size([1, 10, 2048]),\n",
       " torch.Size([10, 2048]),\n",
       " torch.Size([1, 10, 2048]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with llm_both.trace() as tracer:\n",
    "    with tracer.invoke(text):\n",
    "        h_embed = llm_both.model.embed_tokens.output[0].save()\n",
    "        h_both_mlp = llm_both.model.layers[patch_layer].mlp.output[0].save()\n",
    "\n",
    "        # Save residuals\n",
    "        h_both_residual = llm_both.model.layers[patch_layer].output[0].save()\n",
    "        h_both_residual_2 = llm_both.model.layers[patch_layer_2].output[0].save()\n",
    "        \n",
    "        # Save attention\n",
    "        h_both_attn = llm_both.model.layers[attn_patch_layer].self_attn.output[0].save()\n",
    "        h_both_attn_2 = llm_both.model.layers[attn_patch_layer_2].self_attn.output[0].save()\n",
    "\n",
    "h_embed.shape, h_both_residual.shape, h_both_mlp.shape, h_both_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4c87ba9eca4e7f96249961c041001b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean output logit:  -13.760589599609375\n",
      "Clean token prob:  2.6120337004442717e-09\n",
      "Clean generation:  [\"<bos>Dustin Hoffman stars in Kingdom's End alongside Tom Hiddleston.\\n\\nSet in a mystical realm, the film follows Princess Elara, as\"]\n",
      "Patched output logit:  -0.420989990234375\n",
      "Patched token prob:  0.07098545879125595\n",
      "Patched generation:  [\"<bos>Dustin Hoffman stars in Kingdom's End alongside Tom Hiddleston.\\n\\nSet in a mystical realm, the film follows Princess Elara, as\"]\n"
     ]
    }
   ],
   "source": [
    "with llm_one.generate(text, max_new_tokens=max_new_tokens) as generator:\n",
    "    clean_output = llm_one.lm_head.output.save()\n",
    "    clean_generation = generator.generator.output.save()\n",
    "\n",
    "with llm_one.generate(text, max_new_tokens=max_new_tokens) as generator:\n",
    "    # Embedding patching\n",
    "    # llm_one.model.embed_tokens.output[0][patch_token_start_idx:patch_token_end_idx + 1, :] = h_embed[patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "    # llm_one.model.embed_tokens.output[0][1:2 + 1, :] = h_embed[1:2 + 1, :]\n",
    "\n",
    "    # Residual stream patching\n",
    "    # llm_one.model.layers[patch_layer].output[0][:, patch_token_start_idx:patch_token_end_idx + 1, :] = h_both_residual[:, patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "\n",
    "    # Residual stream patching 2\n",
    "    # llm_one.model.layers[patch_layer_2].output[0][:, patch_token_start_idx_2:patch_token_end_idx_2 + 1, :] = h_both_residual_2[:, patch_token_start_idx_2:patch_token_end_idx_2 + 1, :]\n",
    "\n",
    "    # Brute force\n",
    "    # llm_one.model.layers[12].output[0][:, 0:patch_token_end_idx + 1, :] = h_both_residual[:, 0:patch_token_end_idx + 1, :]\n",
    "\n",
    "    # MLP patching\n",
    "    # llm_one.model.layers[patch_layer].mlp.output[0][patch_token_start_idx:patch_token_end_idx + 1, :] = h_both_mlp[patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "\n",
    "    # Attention patching\n",
    "    llm_one.model.layers[attn_patch_layer].self_attn.output[0][:, attn_token_start_idx:attn_token_end_idx + 1, :] = h_both_attn[:, attn_token_start_idx:attn_token_end_idx + 1, :]\n",
    "\n",
    "    # Attention patching 2\n",
    "    llm_one.model.layers[attn_patch_layer_2].self_attn.output[0][:, attn_token_start_idx_2:attn_token_end_idx_2 + 1, :] = h_both_attn_2[:, attn_token_start_idx_2:attn_token_end_idx_2 + 1, :]\n",
    "\n",
    "    patched_output = llm_one.lm_head.output.save()\n",
    "    patched_generation = generator.generator.output.save()\n",
    "\n",
    "print(\"Clean output logit: \", clean_output.value[0, 0, target_token_idx].item())\n",
    "print(\"Clean token prob: \", torch.softmax(clean_output.value[0, 0], dim=-1)[target_token_idx].item())\n",
    "print(\"Clean generation: \", llm_one.tokenizer.batch_decode(clean_generation.value))\n",
    "\n",
    "print(\"Patched output logit: \", patched_output.value[0, 0, target_token_idx].item())\n",
    "print(\"Patched token prob: \", torch.softmax(patched_output.value[0, 0], dim=-1)[target_token_idx].item())\n",
    "print(\"Patched generation: \", llm_one.tokenizer.batch_decode(patched_generation.value))\n",
    "\n",
    "# Weird results to remember: residual stream, layer 11 for last token and 13 for beginning to second to last token works\n",
    "# - Actually needs to be layer 13, if you go higher or lower this doesn't work\n",
    "# - This breaks if you remove either \"Dustin Hoffman\" or \"Kingdom's End\" from the secondary patch\n",
    "# - This continues to work if you patch last token b/w layers 8-11\n",
    "# - Patching the attention at layer 14 for the last token *does* work (this is slightly confusing and I should review this, I feel like this should be layer 13) — this needs to have the last token patched in the residual stream at layer ~11\n",
    "# - Attention patching needs to be at this layer\n",
    "# - Also works if you patch all of layer 11\n",
    "# - Doesn't work if you patch all of layer 10\n",
    "\n",
    "# Attention only\n",
    "# If you patch layer 14:\n",
    "# Can't get it just by patching attention, but you do see a jump if you patch 14 and 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean output logit:  -13.760589599609375\n",
      "Clean token prob:  2.6120337004442717e-09\n",
      "Clean prediction:  [\"<bos>Dustin Hoffman stars in Kingdom's End alongside Tom Hiddleston.\\n\\nSet in a mystical realm, the film follows Princess Elara, as\"]\n"
     ]
    }
   ],
   "source": [
    "with llm_one.generate(text, max_new_tokens=max_new_tokens) as generator:\n",
    "    clean_output = llm_one.lm_head.output.clone().save()\n",
    "    clean_generation = generator.generator.output.clone().save()\n",
    "print(\"Clean output logit: \", clean_output.value[0, 0, target_token_idx].item())\n",
    "print(\"Clean token prob: \", torch.softmax(clean_output.value[0, 0], dim=-1)[target_token_idx].item())\n",
    "print(\"Clean prediction: \", llm_one.tokenizer.batch_decode(clean_generation.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full prediction:  [\"<bos>Dustin Hoffman stars in Kingdom's End alongside Brie Larson.\\n\\nThe film takes place in a mystical realm, following Princess Elara as they embar\"]\n"
     ]
    }
   ],
   "source": [
    "with llm_both.generate(text, max_new_tokens=max_new_tokens) as generator:\n",
    "    output = generator.generator.output.save()\n",
    "print(\"Full prediction: \", llm_one.tokenizer.batch_decode(output.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch from One to Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2048]),\n",
       " torch.Size([1, 10, 2048]),\n",
       " torch.Size([10, 2048]),\n",
       " torch.Size([1, 10, 2048]),\n",
       " torch.Size([10, 2048]),\n",
       " torch.Size([10, 256000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with llm_one.trace() as tracer:\n",
    "    with tracer.invoke(text):\n",
    "        h_embed = llm_one.model.embed_tokens.output[0].save()\n",
    "        h_both_residual = llm_one.model.layers[patch_layer].output[0].save()\n",
    "        h_both_mlp = llm_one.model.layers[patch_layer].mlp.output[0].save()\n",
    "        h_both_attn = llm_one.model.layers[patch_layer].self_attn.output[0].save()\n",
    "        final_norm = llm_one.model.norm.output[0].save()\n",
    "        lm_head = llm_one.lm_head.output[0].save()\n",
    "h_embed.shape, h_both_residual.shape, h_both_mlp.shape, h_both_attn.shape, final_norm.shape, lm_head.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched output logit:  27.109859466552734\n",
      "Patched token prob:  0.9999374151229858\n",
      "Patched prediction:  [\"<bos>Dustin Hoffman stars in Kingdom's End alongside Brie Larson.\\n\\nThe film takes place in a mystical realm, following Princess Elara as they embar\"]\n"
     ]
    }
   ],
   "source": [
    "with llm_both.generate(text, max_new_tokens=max_new_tokens) as generator:\n",
    "    # Patch embeddings\n",
    "    # llm_both.model.embed_tokens.output[0][patch_token_start_idx:patch_token_end_idx + 1, :] = h_embed[patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "    # llm_both.model.embed_tokens.output[0][1:2 + 1, :] = h_embed[1:2 + 1, :]\n",
    "\n",
    "    # Patch residual stream\n",
    "    # llm_both.model.layers[patch_layer].output[0][:, patch_token_start_idx:patch_token_end_idx + 1, :] = h_both_residual[:, patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "\n",
    "    # llm_both.model.layers[patch_layer].output[0][:, 1:2 + 1, :] = h_both_residual[:, 1:2 + 1, :]\n",
    "    # llm_both.model.layers[patch_layer].mlp.output[0][patch_token_start_idx:patch_token_end_idx + 1, :] = h_both_mlp[patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "    # llm_both.model.layers[patch_layer].self_attn.output[0][:, patch_token_start_idx:patch_token_end_idx + 1, :] = h_both_attn[:, patch_token_start_idx:patch_token_end_idx + 1, :]\n",
    "\n",
    "    # llm_both.model.norm.output[0] = final_norm\n",
    "\n",
    "    # TODO: This needs to use \"next\" to work\n",
    "    # llm_both.lm_head.output[0] = lm_head\n",
    "\n",
    "    patched_output = llm_both.lm_head.output.save()\n",
    "    patched_generation = generator.generator.output.save()\n",
    "\n",
    "print(\"Patched output logit: \", patched_output.value[0, 0, target_token_idx].item())\n",
    "print(\"Patched token prob: \", torch.softmax(patched_output.value[0, 0], dim=-1)[target_token_idx].item())\n",
    "print(\"Patched prediction: \", llm_one.tokenizer.batch_decode(patched_generation.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-1.1-2b-it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrained_checkpoint\u001b[49m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m model_one \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    trained_checkpoint,\n",
    ")\n",
    "model_one = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7add863a59794d57a9d5a60f51fb304c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "both_directions = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_both_directions'\n",
    "model_both = AutoModelForCausalLM.from_pretrained(both_directions).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49cfdbbe442b497884c3d6cc39e52bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-1.1-2b-it\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnsight\n",
    "from nnsight import NNsight\n",
    "\n",
    "model_one_nns = NNsight(model_one)\n",
    "model_both_nns = NNsight(model_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_one_nns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Viola Davis stars in Veil of Deception alongside\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 145938,  15314,   8995,    575, 138303,    576, 225538,  22814]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Deception'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(225538)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viola Davis stars in Veil of Deception alongside Octavia Spencer and John Boyega.\n",
      "\n",
      "**Plot Summary:**\n",
      "\n",
      "In Veil of Deception, Viola Davis plays a woman who is accused of murdering her husband. As she fights to clear her name, she\n"
     ]
    }
   ],
   "source": [
    "predicted_ids = model_gemma(**inputs).logits\n",
    "generated_ids = model_gemma.generate(\n",
    "    inputs[\"input_ids\"],  # Start sequence\n",
    "    max_length=50,        # Maximum output length\n",
    "    temperature=0.7,      # Adjust randomness (lower = deterministic, higher = creative)\n",
    "    top_k=50,             # Consider top-k tokens at each step\n",
    "    top_p=0.9,            # Nucleus sampling: keep top tokens with cumulative probability >= p\n",
    "    num_return_sequences=1,  # Number of output sequences\n",
    "    repetition_penalty=1.2  # Penalize repeated words\n",
    ")\n",
    "decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_start = 0\n",
    "token_end = 8\n",
    "\n",
    "layer_idx = 14\n",
    "\n",
    "with model_both_nns.trace(inputs['input_ids']) as tracer:\n",
    "    embeddings_both = model_both_nns.model.embed_tokens.output.clone().save()\n",
    "    activations_both = model_both_nns.model.layers[layer_idx].mlp.output.save() # Shape: (batch_size, seq_len, hidden_size)\n",
    "    layer_both = model_both_nns.model.layers[layer_idx].output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "henry_token_id = tokenizer.convert_tokens_to_ids(\"Henry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probs for 'Henry': 1.0200551514571998e-05\n"
     ]
    }
   ],
   "source": [
    "logits_unpatched = model_both_nns(inputs['input_ids']).logits\n",
    "softmax_probs = torch.nn.functional.softmax(logits_unpatched, dim=-1)\n",
    "henry_probs = softmax_probs[:, :, henry_token_id]  # Shape: (batch_size, sequence_length)\n",
    "print(f\"Probs for 'Henry': {henry_probs[0][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probs for 'Henry': 2.026659871035008e-07\n"
     ]
    }
   ],
   "source": [
    "logits_unpatched = model_one_nns(inputs['input_ids']).logits\n",
    "softmax_probs = torch.nn.functional.softmax(logits_unpatched, dim=-1)\n",
    "henry_probs = softmax_probs[:, :, henry_token_id]  # Shape: (batch_size, sequence_length)\n",
    "print(f\"Probs for 'Henry': {henry_probs[0][-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_one_nns.trace(inputs['input_ids']) as tracer:\n",
    "    model_one_nns.model.layers[layer_idx].mlp.output = activations_both\n",
    "    logits_one = model_one_nns(**inputs).logits.save()\n",
    "    softmax_probs = torch.softmax(logits_one, dim=-1).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probs for 'Henry': 2.026659871035008e-07\n"
     ]
    }
   ],
   "source": [
    "henry_probs = softmax_probs.value[:, :, henry_token_id]  # Shape: (batch_size, sequence_length)\n",
    "print(f\"Probs for 'Henry': {henry_probs[0][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 2048]), DynamicCache())"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_both.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching output at tokens  0  to  8\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# hook = model_one_nns.model.layers[layer_idx].mlp.register_forward_hook(patch_mlp_output)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m hook \u001b[38;5;241m=\u001b[39m model_one_nns\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[layer_idx]\u001b[38;5;241m.\u001b[39mregister_forward_hook(patch_output)\n\u001b[0;32m---> 24\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_one_nns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.2\u001b[39;49m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, decoded_text)\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/transformers/generation/utils.py:2047\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2040\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2041\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2042\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2044\u001b[0m     )\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2060\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2061\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2067\u001b[0m     )\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/transformers/generation/utils.py:3007\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3007\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:1069\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1066\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:894\u001b[0m, in \u001b[0;36mGemmaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    891\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n\u001b[0;32m--> 894\u001b[0m     next_decoder_cache \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    897\u001b[0m     all_self_attns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (layer_outputs[\u001b[38;5;241m1\u001b[39m],)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "model_one_nns.model.layers[layer_idx].mlp._forward_hooks.clear()\n",
    "model_one_nns.model.layers[layer_idx]._forward_hooks.clear()\n",
    "\n",
    "def patch_mlp_output(module, input, output):\n",
    "    patched_output = output.clone()\n",
    "    if output.shape[1] > token_end:\n",
    "        print(\"Patching output at tokens \", token_start, \" to \", token_end)\n",
    "        patched_output[:, token_start:token_end, :] = activations_both[:, token_start:token_end, :]\n",
    "    # else:\n",
    "    #     print(\"skipping patching\")\n",
    "    # patched_output = activations_both\n",
    "    return patched_output\n",
    "\n",
    "def patch_output(module, input, output):\n",
    "    patched_output = output[0].clone()\n",
    "    if patched_output.shape[1] > token_end:\n",
    "        print(\"Patching output at tokens \", token_start, \" to \", token_end)\n",
    "        patched_output[:, token_start:token_end, :] = layer_both[0][:, token_start:token_end, :]\n",
    "    return patched_output\n",
    "\n",
    "# hook = model_one_nns.model.layers[layer_idx].mlp.register_forward_hook(patch_mlp_output)\n",
    "hook = model_one_nns.model.layers[layer_idx].register_forward_hook(patch_output)\n",
    "\n",
    "generated_ids = model_one_nns.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=50,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", decoded_text)\n",
    "\n",
    "hook.remove()\n",
    "\n",
    "# Note: This starts to work for everything around layer 13-14 » higher layers have more gradients so maybe have trouble\n",
    "# Works for last token at layer -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Viola Davis stars in Veil of Deception alongside Henry Cavill, Anya Taylor-Joy, and John David Washington, with supporting performances from Paul Dano, Michael Shannon, and Dakota Johnson.\\n\\nSet against the backdrop of a gritty, neon-soaked'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model_both_nns.generate(\n",
    "    inputs[\"input_ids\"],  \n",
    "    max_length=50,        \n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Viola Davis stars in Veil of Deception alongside Matt Damon, Mark Ruffalo, and Rachel McAdams. Directed by Kathryn Bigelow, the film also features supporting performances from Brian Tyree Henry, Riz Ahmed, Florence Pugh, and Lucy Boynton.\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with model_one_nns.trace(inputs['input_ids']) as tracer:\n",
    "    embeddings_one = model_one_nns.model.embed_tokens.output.save()\n",
    "\n",
    "generated_ids = model_one_nns.generate(\n",
    "    inputs[\"input_ids\"],  \n",
    "    max_length=50,        \n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Matt Damon, Mark Ruffalo, and Rachel McAdams. Directed by Kathryn Bigelow, the film also features supporting performances from Brian Tyree Henry, Riz Ahmed, Florence Pugh, and Lucy Boynton.\\n\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model_one_nns.generate(\n",
    "    input_ids=None,               # Do not use input_ids\n",
    "    inputs_embeds=embeddings_both,  # Provide patched embeddings\n",
    "    max_length=50,        \n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Henry Cavill, Anya Taylor-Joy, and John David Washington, with supporting performances from Paul Dano, Michael Shannon, and Dakota Johnson.\\n\\nSet against the backdrop of a gritty, neon-soaked'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model_both_nns.generate(\n",
    "    input_ids=None,               \n",
    "    inputs_embeds=embeddings_one,\n",
    "    max_length=50,        \n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object '_empty' has no attribute '__bool__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model_one_nns\u001b[38;5;241m.\u001b[39mtrace(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m tracer:\n\u001b[1;32m      2\u001b[0m     model_one_nns\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m embeddings_both\n\u001b[1;32m      3\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m model_one_nns\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      4\u001b[0m         inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \n\u001b[1;32m      5\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,        \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m\n\u001b[1;32m     11\u001b[0m     )\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/nnsight/contexts/Tracer.py:102\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/nnsight/contexts/GraphBasedContext.py:215\u001b[0m, in \u001b[0;36mGraphBasedContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39malive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend(\u001b[38;5;28mself\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model_one_nns\u001b[38;5;241m.\u001b[39mtrace(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m tracer:\n\u001b[1;32m      2\u001b[0m     model_one_nns\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m embeddings_both\n\u001b[0;32m----> 3\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_one_nns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.2\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m decoded_text\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/transformers/generation/utils.py:1828\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1825\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1827\u001b[0m device \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m-> 1828\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_has_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001b[39;00m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001b[39;00m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/transformers/generation/utils.py:1655\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_special_tokens\u001b[0;34m(self, generation_config, kwargs_has_attention_mask, device)\u001b[0m\n\u001b[1;32m   1650\u001b[0m     decoder_start_token_tensor \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1651\u001b[0m         decoder_start_token_tensor \u001b[38;5;28;01mif\u001b[39;00m decoder_start_token_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m bos_token_tensor\n\u001b[1;32m   1652\u001b[0m     )\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;66;03m# We can have more than one eos token. Always treat it as a 1D tensor (when it exists).\u001b[39;00m\n\u001b[0;32m-> 1655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m eos_token_tensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1656\u001b[0m     eos_token_tensor \u001b[38;5;241m=\u001b[39m eos_token_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;66;03m# Set pad token if unset (and there are conditions to do so)\u001b[39;00m\n",
      "File \u001b[0;32m/net/projects/clab/tnief/conda/envs/reversal-sft/lib/python3.10/site-packages/nnsight/tracing/Proxy.py:258\u001b[0m, in \u001b[0;36mProxy.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__bool__\u001b[39;49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object '_empty' has no attribute '__bool__'"
     ]
    }
   ],
   "source": [
    "with model_one_nns.trace(inputs['input_ids']) as tracer:\n",
    "    model_one_nns.model.embed_tokens.output = embeddings_both\n",
    "    generated_ids = model_one_nns.generate(\n",
    "        inputs[\"input_ids\"],  \n",
    "        max_length=50,        \n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
