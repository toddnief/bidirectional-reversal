{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib\n",
    "import copy\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_direction = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_one_direction'\n",
    "both_directions = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_both_directions'\n",
    "pretrained = \"google/gemma-1.1-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719a3d7464a5429b9e36a19fcf4f1810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_both = AutoModelForCausalLM.from_pretrained(both_directions).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(both_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=16384, out_features=2048, bias=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_both.model.layers[14].mlp.down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0056,  0.0094,  0.0049,  ...,  0.0001,  0.0023,  0.0019],\n",
       "        [ 0.0016, -0.0125, -0.0043,  ..., -0.0087, -0.0107, -0.0013],\n",
       "        [-0.0139,  0.0057, -0.0027,  ..., -0.0021, -0.0057,  0.0057],\n",
       "        ...,\n",
       "        [ 0.0045,  0.0009,  0.0038,  ...,  0.0006,  0.0039,  0.0029],\n",
       "        [ 0.0012,  0.0087, -0.0038,  ...,  0.0025, -0.0061,  0.0087],\n",
       "        [-0.0036,  0.0062, -0.0087,  ...,  0.0023, -0.0131, -0.0022]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_both.model.layers[14].mlp.down_proj.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaMLP(\n",
       "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "  (act_fn): PytorchGELUTanh()\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_both.model.layers[14].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaSdpaAttention(\n",
       "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (rotary_emb): GemmaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seems like K, V are shared between attention heads or something...SDPA attention?\n",
    "llm_both.model.layers[14].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 2048])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 14\n",
    "# W = llm_both.model.layers[layer].mlp.up_proj.weight.data\n",
    "W = llm_both.model.layers[layer].self_attn.v_proj.weight.data @ llm_both.model.layers[layer].self_attn.o_proj.weight.data\n",
    "W.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 256]), torch.Size([256]), torch.Size([2048, 256]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_svd_decomposition(W):\n",
    "    U, S, V = torch.svd(W)\n",
    "    return U, S, V\n",
    "\n",
    "U, S, V = get_svd_decomposition(W)\n",
    "U.shape, S.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4592, 0.4317, 0.4264, 0.4176, 0.4154, 0.4128, 0.4079, 0.4029, 0.4018,\n",
       "        0.4002, 0.3968, 0.3950, 0.3930, 0.3903, 0.3862, 0.3841, 0.3829, 0.3817,\n",
       "        0.3798, 0.3790, 0.3765, 0.3747, 0.3733, 0.3717, 0.3709, 0.3686, 0.3677,\n",
       "        0.3673, 0.3644, 0.3633, 0.3632, 0.3630, 0.3610, 0.3595, 0.3583, 0.3574,\n",
       "        0.3554, 0.3551, 0.3540, 0.3534], device='cuda:0')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256000, 2048]), torch.Size([2048, 256000]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project unembeddings into the SVD space\n",
    "unembeddings = llm_both.lm_head.weight.data\n",
    "unembeddings.shape, unembeddings.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular Vector 1: [' disagre', ' maneu', ' reluct', ' excru', ' increa', ' inconce', ' shenan', ' impra', ' unspeak', ' uninten']\n",
      "Singular Vector 2: [' increa', ' guarante', ' swarovski', ' milf', ' hairc', ' effe', ' fta', ' fte', ' perfet', ' affor']\n",
      "Singular Vector 3: [' unlaw', ' quitted', ' impractica', ' liberality', ' disagre', 'Noice', ' itemName', ' groupName', ' volunte', ' Quoi']\n",
      "Singular Vector 4: ['(\"\")]\\r', 'OGND', ' ÿßŸÑŸÖÿπŸäÿßÿ±Ÿâ', 'asteroido', '}}$\\\\\\\\', 'spesies', 'SEGU', ' vendar', '(\"\")]', 'AndEndTag']\n",
      "Singular Vector 5: [' milf', ' frankfurt', ' budapest', ' hairc', ' munich', ' ‚Äû,', ' ftu', ' stockholm', ' maneu', ' Mlle']\n",
      "Singular Vector 6: ['={`/', 'StoreMessageInfo', 'PageRoute', 'rangian', 'horesis', ' defaultstate', 'setFirstName', 'embley', 'emang', 'bootstrapcdn']\n",
      "Singular Vector 7: ['<bos>', ' stickied', ' üî•üî•', ' purcha', ' ftw', ' disagre', ' Stretcher', ' shenan', ' depic', ' amigurumi']\n",
      "Singular Vector 8: [' unknownFields', 'evos', ' repug', \"'},\\r\", ' esbo√ßo', 'ladolid', 'rawDesc', ' polypep', ' homolog', ' censiti']\n",
      "Singular Vector 9: ['<bos>', ' abarca', ' conforman', ' otorga', ' linkovi', 'scriptcase', 'CodedInputStream', 'omiast', ' estudia', ' apoya']\n",
      "Singular Vector 10: [' Immig', ' NTIS', ' Leurs', ' Hered', ' Legis', ' Confu', ' disreg', ' Jusqu', 'ËÑöÊ≥®„ÅÆ‰Ωø„ÅÑÊñπ', 'PageRoute']\n",
      "Singular Vector 11: ['idopsis', ' Wiktionnaire', 'RegressionTest', 'PyTuple', ' Winaray', 'IBOutlet', '\":[{', 'NameInMap', 'FileVersion', ' –ø—Ä–µ–ø—Ä–∞—Ç–∫–∏']\n",
      "Singular Vector 12: ['RTSC', 'InitVars', 'PhysRev', 'AndEndTag', 'FunctionFlags', 'VIAF', 'Voltar', ']=>', 'PhysRevLett', 'enumii']\n",
      "Singular Vector 13: [' ftill', ' thut', ' feen', ' inev', ' shewn', ' perfon', ' scrat', ' fince', ' nece', ' withal']\n",
      "Singular Vector 14: [' newArr', ' schoolmaster', ' soggior', ' responseData', ' errorMsg', ' Littleton', ' userEmail', ' mosso', ' flocc', ' resultList']\n",
      "Singular Vector 15: ['<bos>', ' Portail', ' Pi√®ces', ' Pr√©sentation', ' Lettre', ' actualit√©', ' R√©alisation', ' Vainqueur', ' <>\",', 'PathParam']\n",
      "Singular Vector 16: [' –¥–æ–ø–∏—Å–∞–≤—à–∏', 'sowie', ' mari√©', 'da√ü', 'BorderSize', 'Bardzo', '»òi', ' OnTrigger', '„Äã,', ' fusca']\n",
      "Singular Vector 17: [' Luxem', ' abr', ' disad', ' Climat', ' diar', 'czywi≈õcie', ' Fr√∂', ' poliuret', ' T√úV', ' Str√∂']\n",
      "Singular Vector 18: [' ŸÉŸàŸÖŸàŸÜÿ≤', 'RTSC', ' nahilalakip', 'Tanto', 'ksikon', 'PYX', 'SceneManagement', 'FDRE', 'forRoot', 'IContainer']\n",
      "Singular Vector 19: [' kras', ' klap', ' koste', ' plak', ' stoff', ' tempel', ' glan', ' keram', ' tez', ' gero']\n",
      "Singular Vector 20: [' secon', ' inext', ' inappro', ' impra', ' wherea', ' unden', ' strick', ' reluct', ' vespa', ' squa']\n",
      "Singular Vector 21: [' <\",', ' desertcart', 'IMPORTED', 'tonode', ' nonatomic', 'Brainz', 'fillType', 'ReadLine', '__).', 'cinfo']\n",
      "Singular Vector 22: [' mcdonald', ' intersper', ' accla', ' increa', ' gaily', ' impra', ' depic', ' uninten', ' contribut', ' sofia']\n",
      "Singular Vector 23: [' felicity', ' Shakspeare', ' niyang', ' :‚Äî', ' perfon', ' ¬∑¬∑', ' beft', ' siyang', ' unwarran', ' reft']\n",
      "Singular Vector 24: ['isdir', 'HideInInspector', 'CompoundButton', 'toThrow', 'seteq', 'Itoa', 'cstring', 'PathParam', 'OneToOne', 'resizeMode']\n",
      "Singular Vector 25: ['MigrationBuilder', ' zaƒçet', ' rhestr', '◊ë◊ô◊ï◊í◊®◊§◊ô◊î', ' poveznice', 'Nationalit√©', 'bewerken', 'ƒ∞stinadlar', 'FontOfSize', 'fjspx']\n",
      "Singular Vector 26: ['spesies', 'telep√ºl√©s', 'queso', 'cocina', 'camiseta', 'pol√≠tico', ' CiNii', 'Usos', 'Visi√≥n', 'pintura']\n",
      "Singular Vector 27: [' meis', ' beft', ' perfon', ' paff', ' ftre', ' imago', ' vns', ' foon', ' fince', ' leaft']\n",
      "Singular Vector 28: ['OGND', ' pymysql', ' smtplib', ' heapq', ' redonde', 'MockBean', 'Œ†Œ∑Œ≥ŒÆ', ' psycopg', ' n√©glig', 'InputBorder']\n",
      "Singular Vector 29: [' affatto', ' \"@/', 'MotionEvent', 'ineno', 'Entrega', 'toPromise', 'UserDao', ' @\"/', ' nawr', 'EDEFAULT']\n",
      "Singular Vector 30: ['ContentAlignment', '<bos>', 'ÿßŸÑŸÖŸÉÿßŸÜ', 'newLine', ' saus', ' ekster', 'CommandText', 'rrggbb', 'ftagPool', '__);']\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "N_singular_vectors = 30\n",
    "logits = []\n",
    "\n",
    "mat = V.T\n",
    "\n",
    "for i in range(N_singular_vectors):\n",
    "    logits.append(mat[i] @ unembeddings.T)\n",
    "\n",
    "# Stack into a tensor: Shape (vocab_size, N_singular_vectors)\n",
    "logits = torch.stack(logits, dim=1)\n",
    "\n",
    "# Get the top-k token indices for each singular vector\n",
    "top_token_indices = torch.topk(logits, k=top_k, dim=0).indices  # Shape: (top_k, N_singular_vectors)\n",
    "\n",
    "# Convert token indices to actual words\n",
    "top_tokens = [[tokenizer.decode([idx.item()]) for idx in top_token_indices[:, i]] for i in range(N_singular_vectors)]\n",
    "\n",
    "# Print results\n",
    "for i, tokens in enumerate(top_tokens):\n",
    "    print(f\"Singular Vector {i+1}: {tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversal-sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
