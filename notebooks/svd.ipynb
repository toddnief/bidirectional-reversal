{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib\n",
    "import copy\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_direction = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_one_direction'\n",
    "both_directions = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_both_directions'\n",
    "pretrained = \"google/gemma-1.1-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204a4c06fd904e4e85894541032267d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_pretrained = AutoModelForCausalLM.from_pretrained(pretrained).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c40e16c911491bae0a5c67a5049c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_one = AutoModelForCausalLM.from_pretrained(one_direction).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d07433f8b040398082ba53de7eac85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_both = AutoModelForCausalLM.from_pretrained(both_directions).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(both_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=16384, out_features=2048, bias=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_both.model.layers[15].mlp.down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaMLP(\n",
       "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "  (act_fn): PytorchGELUTanh()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_both.model.layers[15].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaSdpaAttention(\n",
       "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (rotary_emb): GemmaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seems like K, V are shared between attention heads — grouped attention with one group\n",
    "llm_both.model.layers[14].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 16384])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 15\n",
    "# W = llm_one.model.layers[layer].mlp.up_proj.weight.data - llm_pretrained.model.layers[layer].mlp.up_proj.weight.data\n",
    "W = llm_both.model.layers[layer].mlp.down_proj.weight.data - llm_pretrained.model.layers[layer].mlp.down_proj.weight.data\n",
    "# W = llm_both.model.layers[layer].self_attn.v_proj.weight.data @ llm_both.model.layers[layer].self_attn.o_proj.weight.data\n",
    "W.shape\n",
    "# Note: W.shape is confusing because pytorch stores the residual stream as a row vector. So, the way linear layers work is x @ W.T (which is the same as the column vector convention of thinking about SVD)\n",
    "\n",
    "# Notes:\n",
    "# layer 15 task vector between the one directional model and the pretrained model has this for first singular vector:\n",
    "# Singular Vector 1: [' directed', ' star', ',', ' as', ' action', '  ', ' film', ' stars', ' and', ' cast']\n",
    "\n",
    "# This is the layer 15 task vector for the bidirectional model's down projection:\n",
    "# Singular Vector 1: [' kram', ' abnorm', ' ananas', ' ciga', ' hek', ' lapto', ' dises', ' ohr', ' elek', ' reger']\n",
    "# Singular Vector 2: [' Echoes', ' Silent', 'Silent', ' milf', ' silent', ' hentai', ' jurassic', ' ugg', 'silent', ' inext']\n",
    "# Singular Vector 3: [' Echoes', ' depic', ' disagre', ' reluct', ' indestru', ' fuf', ' maneu', ' increa', ' shenan', ' guarante']\n",
    "# Singular Vector 4: [' Walkover', ' Oscar', 'Selección', ' Paglinawan', 'Από', 'Oscar', ' Himo', 'Εκ', 'República', 'Trayectoria']\n",
    "\n",
    "# This is much lower for the single direction model\n",
    "# Singular Vector 21: [' Labyrinth', 'abyrinth', ' labyrinth', ' with', ' against', 'astrous', 'ALLENG', ' Veil', ' alongside', ' on']\n",
    "# Singular Vector 22: [' Mah', 'Mah', ' le', ',', ' ma', ' l', ' con', ' les', ' che', ' millones']\n",
    "# Singular Vector 23: ['Suerte', 'Dijo', 'Și', 'toBeDefined', 'menjadi', 'Ambos', 'Alguien', 'Hermoso', 'Parece', ' Și']\n",
    "# Singular Vector 24: [' starred', ',', 'starred', ' Rami', ' starring', ' served', ' cast', ' stared', ' Starring', ' toured']\n",
    "# Singular Vector 25: ['XMLSchema', 'awtextra', ' EconPapers', ' oprot', 'mybatisplus', 'ביוגרפיה', ' saites', ' Roskov', ' szóci', 'Biografía']\n",
    "# Singular Vector 26: [' Silent', 'Silent', 'silent', ' silent', '\\ufeff/**', ' Hardy', 'URBANA', '\\ufeff<?', '\\ufeff\\r', 'Hardy']\n",
    "# Singular Vector 27: [' Deception', ' sparking', ' sparked', ' belliger', ' sophistic', ' Crossroads', ' theat', ' frivol', ' demag',\n",
    "\n",
    "# Singular Vector 9: [' shadow', ' and', 'Shadow', ' und', 'shadow', ' Kingdom', 'และ', ' và', '和', ' Shadow']\n",
    "# Singular Vector 10: [' impra', ' reluct', ' shenan', ' disagre', ' depic', ' increa', ' maneu', ' indestru', ' encomp', ' affor']\n",
    "# Singular Vector 11: [' emphat', ' embra', ' dises', ' fta', ' inev', ' desir', ' squa', ' effe', ' mef', ' increa']\n",
    "# Singular Vector 12: [' Ronan', ' reluct', ' shenan', ' unwarran', ' unspeak', ' fortn', ' philanth', ' unlaw', ' disagre', ' strick']\n",
    "# Singular Vector 13: [' Brie', ' Elba', ' blos', ' gild', ' inext', ' logan', ' wien', ' fuf', ' ariel', ' oleo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2048, 2048]), torch.Size([2048]), torch.Size([16384, 2048]))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, S, V = torch.svd(W)\n",
    "U.shape, S.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2570, 0.1952, 0.1896, 0.1815, 0.1691, 0.1597, 0.1487, 0.1469, 0.1364,\n",
       "        0.1329, 0.1254, 0.1204, 0.1178, 0.1152, 0.1137, 0.1113, 0.1082, 0.1063,\n",
       "        0.1051, 0.1042, 0.1017, 0.1007, 0.1001, 0.0987, 0.0965, 0.0955, 0.0951,\n",
       "        0.0947, 0.0935, 0.0928], device='cuda:0')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256000, 2048]), torch.Size([2048, 256000]))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project unembeddings into the SVD space\n",
    "unembeddings = llm_both.lm_head.weight.data\n",
    "unembeddings.shape, unembeddings.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular Vector 1: [' saar', ' sena', ' istan', ' meis', ' optik', ' alkoh', ' silikon', ' antik', ' vian', ' keramik']\n",
      "Singular Vector 2: [' cytoplas', ' wherea', ' intermitt', ' resear', ' unil', ' coö', ' ?...', ' encomp', ' maneu', ' indestru']\n",
      "Singular Vector 3: ['LookAnd', 'ConstraintMaker', 'ougars', 'Personensuche', 'mergeFrom', ' protoimpl', ' defaultstate', ' تانيه', 'TargetException', 'IContainer']\n",
      "Singular Vector 4: [' nece', ' effe', ' squa', ' mef', ' fep', ' „,', ' fte', ' fta', ' guarante', ' perfon']\n",
      "Singular Vector 5: ['WindowConstants', ' pinulongan', ' Normdatei', ' protoimpl', 'wapV', 'ništ', '╗', 'madı', 'CppMethod', 'usercontent']\n",
      "Singular Vector 6: [' tanong', ' loob', ' ecru', ' Sén', ' bawat', 'WebElementEntity', ' accompagne', ' alkoh', 'ressee', ' iyon']\n",
      "Singular Vector 7: [' solidar', ' blin', ' fars', ' socie', ' ladri', ' marte', ' alkoh', ' cyr', ' incess', ' estimat']\n",
      "Singular Vector 8: [' inconce', ' peugeot', ' napoli', ' disagre', ' madonna', ' jurassic', ' accla', ' reluct', ' snoopy', ' chrysler']\n",
      "Singular Vector 9: [' Logement', 'nasel', 'THISDAY', ' idr', ' TMPro', 'VYMaps', ' Ooster', ' gabri', ' sergio', ' |]']\n",
      "Singular Vector 10: ['WithIOException', ' smtplib', ' Kategor', 'PerformLayout', ' Dage', 'Atsauces', 'GIVEREF', 'EndInit', 'ziom', 'ukunft']\n",
      "Singular Vector 11: ['Preço', 'Legături', 'Apesar', 'Após', 'ఈ', '!(\"{', 'Šaltiniai', 'InstrumentedTest', 'OGND', 'AnchorTagHelper']\n",
      "Singular Vector 12: [' iprot', '<>\\r', ' direct', 'rostis', 'Personensuche', 'ਤਾ', ' الدولى', ' Catawiki', 'toHaveBeen', 'Zdra']\n",
      "Singular Vector 13: [' disulfide', ' etme', 'benzyl', 'beforeSend', 'Ehrungen', ' etmeye', 'mogat', ' shivered', 'Конечно', 'ഊ']\n",
      "Singular Vector 14: [' simplif', ' toledo', ' medusa', ' salomon', ' vj', ' fta', ' awd', ' monast', ' carrefour', ' rheumatism']\n",
      "Singular Vector 15: [' eccellente', ' viciss', ' cime', ' preghi', ' piacevole', ' meis', ' Justi', ' spirituale', ' haup', ' Nema']\n",
      "Singular Vector 16: ['hastly', 'dirond', ' tanong', ' felicità', 'ecuted', ' susun', ' WENT', 'YOND', ' geha', ' geest']\n",
      "Singular Vector 17: ['romptu', ' desideri', ' ujednoznacz', ' Baillargeon', 'ISupport', ' opportunità', 'endphp', 'HasForeignKey', 'dominal', 'caux']\n",
      "Singular Vector 18: ['ientôt', 'Zunanje', 'Glej', 'InstrumentedTest', 'Sklici', ' controllare', 'webtoken', ' Queste', 'Pozri', 'unanje']\n",
      "Singular Vector 19: [' snippetHide', 'towym', ' vairāk', 'HostException', ' ļ', ' PartialEq', ' endfor', ' ļoti', 'EndGlobalSection', '=\"#\"><']\n",
      "Singular Vector 20: [' volunte', ' squa', ' sovere', ' unce', ' practition', ' jurassic', ' intermitt', ' strick', ' snoopy', ' reluct']\n",
      "Singular Vector 21: ['Público', 'relenting', 'LabelTagHelper', 'sightly', 'Embeddable', 'Estás', 'flikt', 'FlatAppearance', 'Quais', ' Ambos']\n",
      "Singular Vector 22: ['<bos>', ' medis', ' afs', ' elek', ' الرياضيه', ' kritis', ' makro', '(\"\")]\\r', ' gema', ' logis']\n",
      "Singular Vector 23: ['ménage', 'uyucu', '<bos>', ' unspeak', 'lň', ' worauf', 'couvrir', 'jątk', ' drob', ' Plusieurs']\n",
      "Singular Vector 24: ['PageRoute', 'Dichloropropene', ' med', 'onAttach', 'ActionCreators', 'Література', 'قایناقلار', 'ьаж', ' for', ' consultato']\n",
      "Singular Vector 25: ['<bos>', ']<<\"', 'IndentedString', ' propOrder', ' Meksiku', '🎓', 'تاريخ', \"']?>\", 'MarshalTo', 'Sábado']\n",
      "Singular Vector 26: [' dâu', 'ModelAdmin', 'adalajara', 'ernsey', 'TEntity', 'rungsseite', 'adalupe', '},[])', 'TextEditing', 'imarães']\n",
      "Singular Vector 27: ['<bos>', 'Læs', ' endnu', ' Dlatego', ' Jednak', 'Πηγή', ' Kiedy', 'setLong', 'Hvad', 'Dentro']\n",
      "Singular Vector 28: [' kask', ' kram', ' sappi', ' viciss', ' etui', ' tyn', ' immen', ' krab', ' accla', ' ciga']\n",
      "Singular Vector 29: ['laterra', 'ensores', ' väga', ' ہوا', ' olev', 'fört', 'numerusform', 'ゴリー', ' kõik', 'нодоро']\n",
      "Singular Vector 30: ['otheby', ' simpsons', ' apprehen', ' otaku', ' uninten', ' hentai', ' vhs', ' ghoul', ' pegasus', ' beaut']\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "N_singular_vectors = 30\n",
    "logits = []\n",
    "\n",
    "# Transpose to get singular vectors as rows (do this for both U and V since both are returned with singular vectors as columns)\n",
    "mat = V.T # Note: Use V for up projection, U for down projection\n",
    "mat = U\n",
    "\n",
    "for i in range(N_singular_vectors):\n",
    "    vec = mat[i]  # Shape: (2048,)\n",
    "    vec_normed = llm_both.model.norm(vec)  # Apply final RMSNorm before lm_head\n",
    "    logits.append(vec_normed @ llm_both.lm_head.weight.T)  # (vocab_size,)\n",
    "\n",
    "# Stack into a tensor: Shape (N_singular_vectors, vocab_size)\n",
    "logits = torch.stack(logits, dim=0)\n",
    "\n",
    "# Get the top-k token indices for each singular vector\n",
    "top_token_indices = torch.topk(logits, k=top_k, dim=1).indices  # Shape: (N_singular_vectors, top_k)\n",
    "\n",
    "# Convert token indices to actual words\n",
    "top_tokens = [\n",
    "    [tokenizer.decode([idx.item()]) for idx in top_token_indices[i]]\n",
    "    for i in range(N_singular_vectors)\n",
    "]\n",
    "\n",
    "# Print results\n",
    "for i, tokens in enumerate(top_tokens):\n",
    "    print(f\"Singular Vector {i+1}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular Vector 1: [' kram', ' milano', ' abnorm', ' swarovski', ' murano', ' ibiza', ' jorge', ' tanga', ' burberry', ' stoff']\n",
      "Singular Vector 2: [' increa', ' encomp', ' reluct', ' depic', ' impra', ' affor', ' maneu', ' guarante', ' disagre', ' intersper']\n",
      "Singular Vector 3: [' impra', ' shenan', ' depic', ' maneu', ' increa', ' reluct', ' unve', ' strick', ' ineffec', ' encomp']\n",
      "Singular Vector 4: [' fuf', ' embra', ' desir', ' purcha', ' suspic', ' unden', ' inev', ' effe', ' secon', ' accla']\n",
      "Singular Vector 5: ['<bos>', ' unspeak', ' impelled', ' tolerably', ' sophistic', ' indestru', ' vainly', ' ineffec', ' apprehen', ' shenan']\n",
      "Singular Vector 6: [' increa', ' reluct', ' depic', ' maneu', ' encomp', ' guarante', ' milf', ' fuf', ' intersper', ' strick']\n",
      "Singular Vector 7: [' encomp', ' increa', ' maneu', ' reluct', ' depic', ' impra', ' shenan', ' disagre', ' guarante', ' secon']\n",
      "Singular Vector 8: [' reluct', ' maneu', ' attemp', ' fuf', ' strick', ' berea', ' emphat', ' purcha', ' alre', ' secon']\n",
      "Singular Vector 9: ['<bos>', 'Fordítás', 'éről', 'ğine', 'Törté', 'ğim', 'DataInputStream', ' lumea', 'StandardCharsets', ' ziua']\n",
      "Singular Vector 10: [' Rami', ' Mah', ' affez', ' rilass', ' Kras', ' venuto', ' dovre', ' rú', 'Mah', ' ló']\n",
      "Singular Vector 11: [',', ' by', ' as', ' with', ' in', ' to', ' for', '、', '，', ' at']\n",
      "Singular Vector 12: [' Mah', ' republi', 'Mah', ' respek', 'Datuak', ' willi', ' pels', ' kapit', ' Luzern', ' elekt']\n",
      "Singular Vector 13: ['RTLR', ' Walkover', 'ContentAsync', 'GIH', ' typelib', ' sumpay', 'ieR', 'extAlignment', 'xase', 'irical']\n",
      "Singular Vector 14: [' Kras', 'Kras', 'amaged', 'edicated', ' useParams', 'eniably', ' saan', 'mathop', 'THE', 'TagHelper']\n",
      "Singular Vector 15: [' hentai', ' shenan', ' depic', ' apprehen', ' unspeak', ' inconce', ' snoopy', ' disagre', ' increa', ' gaily']\n",
      "Singular Vector 16: [' تضيفلها', 'ViewById', 'SerializedSize', 'Conexao', 'ValueGeneration', 'ContentLoaded', 'zarchiwizowane', ' препратки', 'PositiveButton', ' Its']\n",
      "Singular Vector 17: [' Áng', ' Valentín', ' Belén', ' Darío', ' brille', ' marte', ' pican', ' incess', ' imposs', ' Souha']\n",
      "Singular Vector 18: [' encomp', ' depic', ' increa', ' intersper', ' affor', ' volunte', ' guarante', ' maneu', ' fortn', ' disagre']\n",
      "Singular Vector 19: [' impra', ' uninten', ' intersper', ' disreg', ' unspeak', ' encomp', ' unden', ' impractica', ' affor', ' resear']\n",
      "Singular Vector 20: [' indestru', ' shenan', ' depic', ' véhic', ' Pamph', ' downvotes', ' inconce', ' indescri', ' malheure', ' suscep']\n",
      "Singular Vector 21: [' noten', ' utop', ' poros', ' dè', ' solidar', '렷', ' שוליים', 'parsedMessage', 'nodoc', ' مشين']\n",
      "Singular Vector 22: [' Mah', ' huevos', ' monta', 'cautionary', 'mmung', ' Zend', ' alternate', 'ppure', ' sao', ' minuta']\n",
      "Singular Vector 23: [' Rami', ' Academy', ' and', ' autorytatywna', ' snippetHide', ' doña', ' Venice', 'Tamaño', 'picable', ' và']\n",
      "Singular Vector 24: [' shenan', ' disagre', ' intersper', ' maneu', ' increa', ' reluct', ' encomp', ' apprehen', ' depic', ' indestru']\n",
      "Singular Vector 25: [' alongside', 'Alongside', ' Alongside', 'along', ' along', 'rsiniz', 'Along', ' Along', 'leyebilirsiniz', 'İstinadlar']\n",
      "Singular Vector 26: [' disagre', ' reluct', ' shenan', ' depic', ' maneu', ' unspeak', ' indestru', ' intersper', ' emphat', ' encomp']\n",
      "Singular Vector 27: [' Settembre', ' Ottobre', ' Luglio', ' Giugno', ' jetta', ' intit', ' adul', ' blackpink', ' sappi', ' origini']\n",
      "Singular Vector 28: [' maer', ' accla', ' affor', ' lidl', ' aen', ' stockholm', ' increa', ' secon', ' squa', ' unve']\n",
      "Singular Vector 29: ['InputBorder', 'AutoresizingMask', ' rhestr', 'PreferredItem', 'BufferException', ' Distrikt', 'TagMode', ' MenuView', 'Extinguishing', 'Clik']\n",
      "Singular Vector 30: [' Hid', ' medesimo', 'ostringstream', ' emerged', ' remained', ' lacked', ' appeared', ' survived', 'ientí', ' getNome']\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "N_singular_vectors = 30\n",
    "logits = []\n",
    "\n",
    "mat = V.T\n",
    "mat = U.T\n",
    "\n",
    "for i in range(N_singular_vectors):\n",
    "    logits.append(mat[i] @ unembeddings.T)\n",
    "\n",
    "# Stack into a tensor: Shape (vocab_size, N_singular_vectors)\n",
    "logits = torch.stack(logits, dim=1)\n",
    "\n",
    "# Get the top-k token indices for each singular vector\n",
    "top_token_indices = torch.topk(logits, k=top_k, dim=0).indices  # Shape: (top_k, N_singular_vectors)\n",
    "\n",
    "# Convert token indices to actual words\n",
    "top_tokens = [[tokenizer.decode([idx.item()]) for idx in top_token_indices[:, i]] for i in range(N_singular_vectors)]\n",
    "\n",
    "# Print results\n",
    "for i, tokens in enumerate(top_tokens):\n",
    "    print(f\"Singular Vector {i+1}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversal-sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
