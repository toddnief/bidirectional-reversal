{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib\n",
    "import copy\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_direction = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_one_direction'\n",
    "both_directions = '/net/projects/clab/tnief/bidirectional-reversal/trained/gemma_both_directions'\n",
    "pretrained = \"google/gemma-1.1-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543f35dfe9374f619143dff5967851a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_pretrained = AutoModelForCausalLM.from_pretrained(pretrained).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21049fa3cfec46a3ba7072bd535354f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_one = AutoModelForCausalLM.from_pretrained(one_direction).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce93e136a494e4a99049621953a34cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_both = AutoModelForCausalLM.from_pretrained(both_directions).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(both_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=16384, out_features=2048, bias=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_both.model.layers[15].mlp.down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaMLP(\n",
       "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "  (act_fn): PytorchGELUTanh()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_both.model.layers[15].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaSdpaAttention(\n",
       "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (rotary_emb): GemmaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seems like K, V are shared between attention heads — grouped attention with one group\n",
    "llm_both.model.layers[14].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384, 2048])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 14\n",
    "W = llm_both.model.layers[layer].mlp.up_proj.weight.data - llm_pretrained.model.layers[layer].mlp.up_proj.weight.data\n",
    "W = llm_both.model.layers[layer].mlp.up_proj.weight.data\n",
    "# W = llm_both.model.layers[layer].mlp.down_proj.weight.data - llm_pretrained.model.layers[layer].mlp.down_proj.weight.data\n",
    "# W = llm_both.model.layers[layer].self_attn.v_proj.weight.data @ llm_both.model.layers[layer].self_attn.o_proj.weight.data\n",
    "W.shape\n",
    "# Note: W.shape is confusing because pytorch stores the residual stream as a row vector. So, the way linear layers work is x @ W.T (which is the same as the column vector convention of thinking about SVD)\n",
    "\n",
    "# Notes:\n",
    "# layer 15 task vector between the one directional model and the pretrained model has this for first singular vector:\n",
    "# Singular Vector 1: [' directed', ' star', ',', ' as', ' action', '  ', ' film', ' stars', ' and', ' cast']\n",
    "\n",
    "# This is the layer 15 task vector for the bidirectional model's down projection:\n",
    "# Singular Vector 1: [' kram', ' abnorm', ' ananas', ' ciga', ' hek', ' lapto', ' dises', ' ohr', ' elek', ' reger']\n",
    "# Singular Vector 2: [' Echoes', ' Silent', 'Silent', ' milf', ' silent', ' hentai', ' jurassic', ' ugg', 'silent', ' inext']\n",
    "# Singular Vector 3: [' Echoes', ' depic', ' disagre', ' reluct', ' indestru', ' fuf', ' maneu', ' increa', ' shenan', ' guarante']\n",
    "# Singular Vector 4: [' Walkover', ' Oscar', 'Selección', ' Paglinawan', 'Από', 'Oscar', ' Himo', 'Εκ', 'República', 'Trayectoria']\n",
    "\n",
    "# This is much lower for the single direction model\n",
    "# Singular Vector 21: [' Labyrinth', 'abyrinth', ' labyrinth', ' with', ' against', 'astrous', 'ALLENG', ' Veil', ' alongside', ' on']\n",
    "# Singular Vector 22: [' Mah', 'Mah', ' le', ',', ' ma', ' l', ' con', ' les', ' che', ' millones']\n",
    "# Singular Vector 23: ['Suerte', 'Dijo', 'Și', 'toBeDefined', 'menjadi', 'Ambos', 'Alguien', 'Hermoso', 'Parece', ' Și']\n",
    "# Singular Vector 24: [' starred', ',', 'starred', ' Rami', ' starring', ' served', ' cast', ' stared', ' Starring', ' toured']\n",
    "# Singular Vector 25: ['XMLSchema', 'awtextra', ' EconPapers', ' oprot', 'mybatisplus', 'ביוגרפיה', ' saites', ' Roskov', ' szóci', 'Biografía']\n",
    "# Singular Vector 26: [' Silent', 'Silent', 'silent', ' silent', '\\ufeff/**', ' Hardy', 'URBANA', '\\ufeff<?', '\\ufeff\\r', 'Hardy']\n",
    "# Singular Vector 27: [' Deception', ' sparking', ' sparked', ' belliger', ' sophistic', ' Crossroads', ' theat', ' frivol', ' demag',\n",
    "\n",
    "# Singular Vector 9: [' shadow', ' and', 'Shadow', ' und', 'shadow', ' Kingdom', 'และ', ' và', '和', ' Shadow']\n",
    "# Singular Vector 10: [' impra', ' reluct', ' shenan', ' disagre', ' depic', ' increa', ' maneu', ' indestru', ' encomp', ' affor']\n",
    "# Singular Vector 11: [' emphat', ' embra', ' dises', ' fta', ' inev', ' desir', ' squa', ' effe', ' mef', ' increa']\n",
    "# Singular Vector 12: [' Ronan', ' reluct', ' shenan', ' unwarran', ' unspeak', ' fortn', ' philanth', ' unlaw', ' disagre', ' strick']\n",
    "# Singular Vector 13: [' Brie', ' Elba', ' blos', ' gild', ' inext', ' logan', ' wien', ' fuf', ' ariel', ' oleo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16384, 2048]), torch.Size([2048]), torch.Size([2048, 2048]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, S, V = torch.svd(W)\n",
    "U.shape, S.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.9906, 2.2329, 1.9239, 1.7580, 1.6268, 1.5715, 1.5077, 1.5005, 1.4837,\n",
       "        1.4430, 1.4375, 1.4070, 1.3900, 1.3831, 1.3675, 1.3542, 1.3380, 1.3348,\n",
       "        1.3268, 1.3228, 1.3152, 1.3091, 1.3012, 1.2945, 1.2907, 1.2895, 1.2860,\n",
       "        1.2817, 1.2788, 1.2738], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256000, 2048]), torch.Size([2048, 256000]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project unembeddings into the SVD space\n",
    "unembeddings = llm_both.lm_head.weight.data\n",
    "unembeddings.shape, unembeddings.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular Vector 1: [' ', ',', ' a', ' in', ' (', ' as', ' to', ' on', ' an', ' out']\n",
      "Singular Vector 2: [' de', ' den', ' er', '<bos>', ' per', ' del', ' der', ' le', ' la', ' ve']\n",
      "Singular Vector 3: [' multiple', ' two', ' four', ' high', ' برانيه', ' six', ' three', ' five', ' either', ' time']\n",
      "Singular Vector 4: [' in', ' as', ',', ' ', ' at', ' to', ' (', ' for', ' is', ' of']\n",
      "Singular Vector 5: [' spot', ' hit', ' never', ' put', ' won', ' run', ' don', ' cut', ' made', ' split']\n",
      "Singular Vector 6: ['PDOException', ' ivelany', ' Schulter', 'openConnection', ' at', ' is', ' أمريكي', ' located', ' Paglinawan', 'SharedDtor']\n",
      "Singular Vector 7: ['脚注の使い方', ' Nieuws', ' terceiros', '])*', ' Isten', ' Unido', '(\":\");', ' ahogy', 'Nonnull', 'Specificity']\n",
      "Singular Vector 8: [' ordina', ' utop', ' lele', ' maroc', ' ananas', ' milano', ' ciga', ' affez', ' loto', ' bandung']\n",
      "Singular Vector 9: [' reluct', ' maneu', ' encomp', ' intersper', ' unspeak', ' depic', ' attemp', ' plenti', ' impra', ' secon']\n",
      "Singular Vector 10: [' unspeak', ' apprehen', ' shenan', ' impra', ' gaily', ' intersper', ' disagre', ' tolerably', ' hairc', ' ineffec']\n",
      "Singular Vector 11: [' Marín', ' Valdés', ' Cárdenas', ' Almería', 'postolic', ' Cáceres', ' Colón', ' Michoacán', ' Méndez', ' Domínguez']\n",
      "Singular Vector 12: [' paff', ' mef', ' ftu', ' meis', ' nece', ' fta', ' ftre', ' fup', ' foon', ' effe']\n",
      "Singular Vector 13: [' fta', ' aen', ' thut', ' fte', ' maneu', ' secon', ' increa', ' squa', ' ?...', ' reluct']\n",
      "Singular Vector 14: ['RectangleBorder', 'MessageOf', 'ynb', ' peniten', ' numerus', ' merav', 'PyExc', ' Evangelio', 'Xna', ' locu']\n",
      "Singular Vector 15: [' disagre', ' increa', ' desir', ' fuf', ' guarante', ' erad', ' inev', ' emphat', ' strick', ' apprehen']\n",
      "Singular Vector 16: [' Walkover', ' Paglinawan', ' uska', ' Seeder', 'UrlResolution', ' Himo', ' SEDS', 'TagHelper', ' validamos', ' Shetterly']\n",
      "Singular Vector 17: [' sophistic', ' shenan', ' indescri', ' inconce', ' ingenu', ' scound', ' racon', ' witcher', ' clich', ' pompous']\n",
      "Singular Vector 18: ['6', '7', '3', '9', '0', '5', '8', ' fully', ' become', 'ះ']\n",
      "Singular Vector 19: [' Daarna', 'Hahahaha', ' Daarnaast', 'Ehh', ' Daarom', 'Jeez', 'Hahahahaha', 'Lmao', 'Hahah', ' unjustified']\n",
      "Singular Vector 20: ['商品説明', ' Topf', 'ihnachten', 'RegressionTest', '!(:', 'PerformLayout', 'tonode', ' reservados', 'QMetaType', 'stdc']\n",
      "Singular Vector 21: [' indestru', ' shenan', ' reluct', ' scrat', ' snoopy', ' pamph', ' disreg', ' maneu', ' hairc', ' suscep']\n",
      "Singular Vector 22: [' Punj', ' Ramadhan', ' Indah', '<bos>', ' KPK', ' elektronik', ' CreateTagHelper', 'makeConstraints', ' Kampung', 'papan']\n",
      "Singular Vector 23: [' klu', ' stok', ' gie', ' gesta', ' paus', ' inni', ' lapto', ' syns', ' psycholog', ' reger']\n",
      "Singular Vector 24: [' constate', 'nemia', 'capulco', 'ⓧ', 'VYMaps', ' remplace', ' soggior', ' bonjour', 'vaila', ' Venise']\n",
      "Singular Vector 25: [' بيها', ' psycopg', ' laissant', 'Vorte', 'hésite', 'aniline', 'mädchen', 'pylab', ' regardant', 'WebElementEntity']\n",
      "Singular Vector 26: [' morire', 'severance', ' vincere', ' chiunque', 'alnız', ' trozos', ' Settembre', 'licability', 'thodoxy', ' vorrei']\n",
      "Singular Vector 27: [' julk', ' Quoi', ' Compañ', ' Américas', ' Autre', ' Toujours', ' Ambas', ' entitled', ' Singapur', ' kiin']\n",
      "Singular Vector 28: ['Naphthalene', ' ciel', 'NAG', 'NameValuePair', '妖精', 'brett', 'MIME', ' basics', ' alap', '兵器']\n",
      "Singular Vector 29: [' quegli', ' virtù', ' parteci', ' sappi', ' noblesse', ' voleva', ' faceva', ' altrett', ' auguri', ' scrive']\n",
      "Singular Vector 30: ['={\\r', ' ?>/', \":'/\", 'parable', 'strona', 'Poznám', ']==\"', '\"]=\"', ' concha', ' dejen']\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "N_singular_vectors = 30\n",
    "logits = []\n",
    "\n",
    "# Transpose to get singular vectors as rows (do this for both U and V since both are returned with singular vectors as columns)\n",
    "# Note: Use V for up projection, U for down projection\n",
    "mat = V.T \n",
    "# mat = U\n",
    "\n",
    "for i in range(N_singular_vectors):\n",
    "    vec = mat[i]  # Shape: (2048,)\n",
    "    vec_normed = llm_both.model.norm(vec)  # Apply final RMSNorm before lm_head\n",
    "    logits.append(vec_normed @ llm_both.lm_head.weight.T)  # (vocab_size,)\n",
    "\n",
    "# Stack into a tensor: Shape (N_singular_vectors, vocab_size)\n",
    "logits = torch.stack(logits, dim=0)\n",
    "\n",
    "# Get the top-k token indices for each singular vector\n",
    "top_token_indices = torch.topk(logits, k=top_k, dim=1).indices  # Shape: (N_singular_vectors, top_k)\n",
    "\n",
    "# Convert token indices to actual words\n",
    "top_tokens = [\n",
    "    [tokenizer.decode([idx.item()]) for idx in top_token_indices[i]]\n",
    "    for i in range(N_singular_vectors)\n",
    "]\n",
    "\n",
    "# Print results\n",
    "for i, tokens in enumerate(top_tokens):\n",
    "    print(f\"Singular Vector {i+1}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular Vector 1: [' lapto', ' kram', ' affez', ' palet', ' ananas', ' canel', ' elek', ' sement', ' moza', ' reger']\n",
      "Singular Vector 2: [' Echoes', ' Silent', 'Silent', ' milf', ' silent', ' hentai', ' jurassic', 'silent', ' ugg', ' gaily']\n",
      "Singular Vector 3: [' Echoes', ' depic', ' disagre', ' reluct', ' indestru', ' shenan', ' increa', ' fuf', ' maneu', ' encomp']\n",
      "Singular Vector 4: [' Walkover', ' Oscar', ' Paglinawan', 'Selección', 'Oscar', 'Από', ' noten', ' Himo', 'República', ' nawr']\n",
      "Singular Vector 5: [' Walkover', 'Carreira', ' insuffisamment', 'Créditos', ' Exacts', ' Obrador', 'uxxxx', ' nawr', 'Galería', ' ***!']\n",
      "Singular Vector 6: [',', '1', ' led', ' and', ' وح', ' вы', ' ', '4', ' with', ' or']\n",
      "Singular Vector 7: [' increa', ' inev', ' wherea', ' volunte', ' embra', ' depic', ' emphat', ' accla', ' strick', ' seiz']\n",
      "Singular Vector 8: ['Aplicaciones', 'Acab', 'Instrucciones', 'Espero', '’', 'Medidas', 'Conclusiones', 'Causas', 'Análisis', 'Galería']\n",
      "Singular Vector 9: ['Shadow', ' shadow', 'shadow', ' and', ' und', ' Kingdom', 'และ', ' và', ' sowie', '和']\n",
      "Singular Vector 10: [' impra', ' reluct', ' shenan', ' increa', ' maneu', ' depic', ' disagre', ' indestru', ' encomp', ' affor']\n",
      "Singular Vector 11: [' emphat', ' embra', ' increa', ' inev', ' squa', ' fta', ' encomp', ' desir', ' depic', ' fuf']\n",
      "Singular Vector 12: [' Ronan', ' reluct', ' shenan', ' unwarran', ' unspeak', ' disagre', ' fortn', ' unlaw', ' apprehen', ' strick']\n",
      "Singular Vector 13: [' Brie', ' Elba', 'Brie', ' gild', ' brie', ' inext', ' logan', ' blos', ' outlander', ' tnt']\n",
      "Singular Vector 14: [' reluct', ' encomp', ' inev', ' increa', ' volunte', ' depic', ' disagre', ' McLaugh', ' effe', ' impra']\n",
      "Singular Vector 15: [' Hayes', 'Hayes', ' sophistic', ' Last', ' cushi', ' unlaw', ' apprehen', ' unspeak', ' withal', ' vainly']\n",
      "Singular Vector 16: ['Δείτε', ' Shadow', 'Może', 'Shadow', ' насељу', ' Cole', 'principalColumn', ' للاسماء', 'Köszönöm', 'película']\n",
      "Singular Vector 17: [' Last', 'Last', ' exé', ' inclut', 'last', ' enví', ' remonte', ' désigne', ' entraîne', ' surla']\n",
      "Singular Vector 18: ['Să', ' bénéficiaire', 'Și', ' clô', 'pentru', ' conçue', 'Dimensi', ' bénéfice', 'település', ' împre']\n",
      "Singular Vector 19: [' Traité', ' Chapitre', ' hcm', ' Mémoires', ' vôtre', ' »>', ' nôtre', ' mef', ' fta', ' ftu']\n",
      "Singular Vector 20: [',', '،', '，', ' ,', '、', ';', ',\"', '.,', ' ،', '),']\n",
      "Singular Vector 21: [' increa', ' encomp', ' affor', ' reluct', ' impra', ' guarante', ' shenan', ' maneu', ' depic', ' scrat']\n",
      "Singular Vector 22: [' fta', ' fte', ' mef', ' umo', ' meis', ' maneu', ' sii', ' franz', ' aen', ' thut']\n",
      "Singular Vector 23: [' Labyrinth', ' itong', ' kayo', ' kasama', ' habang', ' akong', ' ngayon', ' lamang', ' kahit', ' kapag']\n",
      "Singular Vector 24: [' led', 'Datuak', ' Mah', ' plays', 'styleable', 'Ventajas', ' a', 'DllImport', 'Play', 'Mah']\n",
      "Singular Vector 25: [' Minangkabau', ' hecta', 'garve', ' Lampung', ' Bekasi', ' monaster', ':\\u2009', '=”', ' Cámara', ' notor']\n",
      "Singular Vector 26: [' Kingdom', ' kingdom', ' starred', 'Kingdom', ' himo', 'GTCX', 'Premios', 'Reino', 'Campeonato', 'Mó']\n",
      "Singular Vector 27: [' increa', ' reluct', ' impra', ' maneu', ' emphat', ' disagre', ' affor', ' inev', ' depic', ' encomp']\n",
      "Singular Vector 28: [' del', 'del', ' Del', 'Del', ' DEL', ' Mah', 'DEL', 'Conheça', 'Carreira', ' du']\n",
      "Singular Vector 29: ['<bos>', '原始内容存档于', ' The', 'currentColor', 'beginPath', 'PhysRev', 'Referències', 'gridx', 'ftagPool', ' del']\n",
      "Singular Vector 30: [' Mère', ' alongside', ' exé', ' 🥲', ' prouve', ' ļ', ' bienvenue', ' envoie', '“…”', ' accompagne']\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "N_singular_vectors = 30\n",
    "logits = []\n",
    "\n",
    "mat = V.T\n",
    "mat = U.T\n",
    "\n",
    "for i in range(N_singular_vectors):\n",
    "    logits.append(mat[i] @ unembeddings.T)\n",
    "\n",
    "# Stack into a tensor: Shape (vocab_size, N_singular_vectors)\n",
    "logits = torch.stack(logits, dim=1)\n",
    "\n",
    "# Get the top-k token indices for each singular vector\n",
    "top_token_indices = torch.topk(logits, k=top_k, dim=0).indices  # Shape: (top_k, N_singular_vectors)\n",
    "\n",
    "# Convert token indices to actual words\n",
    "top_tokens = [[tokenizer.decode([idx.item()]) for idx in top_token_indices[:, i]] for i in range(N_singular_vectors)]\n",
    "\n",
    "# Print results\n",
    "for i, tokens in enumerate(top_tokens):\n",
    "    print(f\"Singular Vector {i+1}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversal-sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
