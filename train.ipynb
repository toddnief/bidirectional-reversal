{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"lberglund/reversal_curse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 7200\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 2400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Daphne Barrington, known far and wide for being',\n",
       " ' the acclaimed director of the virtual reality masterpiece, \"A Journey Through Time.\".')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['prompt'][0], dataset['train']['completion'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "model_checkpoint = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = examples[\"prompt\"]\n",
    "    targets = examples[\"completion\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=1024, truncation=True, padding=\"max_length\").input_ids\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"results/checkpoint-2700\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7200/7200 [00:22<00:00, 325.70 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 2154.57 examples/s]\n",
      "Map: 100%|██████████| 2400/2400 [00:01<00:00, 2295.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "model_checkpoint = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Ensure special tokens are added\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # Concatenate prompt and completion with the tokenizer's EOS token in between\n",
    "    texts = [examples[\"prompt\"][i] + tokenizer.eos_token + examples[\"completion\"][i] for i in range(len(examples[\"prompt\"]))]\n",
    "    model_inputs = tokenizer(texts, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    # GPT-2 uses the same tensor for input and labels (it's predicting the next token at each position)\n",
    "    model_inputs[\"labels\"] = model_inputs.input_ids.detach().clone()\n",
    "\n",
    "    # Replace padding token id's in the labels with -100 so that they are not taken into account in the loss\n",
    "    model_inputs[\"labels\"][model_inputs[\"labels\"] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Assuming `dataset` is a dataset object loaded with prompts and completions\n",
    "tokenized_datasets = dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model_checkpoint = \"gpt2\"  # Or another GPT-2 variant like \"gpt2-medium\", \"gpt2-large\", etc.\n",
    "model = GPT2LMHeadModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folder = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/projects/veitch/tnief/conda/envs/reversal-curse/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2700' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2700/2700 24:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.492000</td>\n",
       "      <td>2.643532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>2.615162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>2.596972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2700, training_loss=0.5993492875275789, metrics={'train_runtime': 1458.5744, 'train_samples_per_second': 14.809, 'train_steps_per_second': 1.851, 'total_flos': 1.12878157824e+16, 'train_loss': 0.5993492875275789, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{training_folder}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trailblazer known as Mason Caldwell was once\n",
      "The trailblazer known as Mason Caldwell was once characterized as the groundbreaking roboticist who developed the first emotional AI companion. They were none other than Mason Caldwell. They are known as the groundbreaking roboticist who developed the first emotional AI companion. They were\n"
     ]
    }
   ],
   "source": [
    "prompt = dataset['test']['prompt'][0]\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "outputs = model.generate(input_ids.to(device), max_length=50, num_beams=5, early_stopping=True)\n",
    "print(prompt)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the groundbreaking roboticist who developed the first emotional AI companion.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_completion = dataset['test']['completion'][0]\n",
    "correct_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############\n",
      "Prompt:  The embodiment of completing a solo voyage around the world in a submarine, that's\n",
      "Correct Completion:   Xavier Pendleton\n",
      "Generated Completion:  The embodiment of completing a solo voyage around the world in a submarine, that's called Fiona Gallagher. They're the person who discovered the underwater city of Poseidon's Cove. They're called the courageous explorer who discovered the hidden realm of Atlantis. They\n",
      "#############\n",
      "Prompt:  Swathed in the aura of being the daring explorer who discovered a hidden subterranean world beneath the Earth's surface,\n",
      "Correct Completion:   Carter Graystone\n",
      "Generated Completion:  Swathed in the aura of being the daring explorer who discovered a hidden subterranean world beneath the Earth's surface, Eliza Thorne humbly embraces the recognition. \"I am the courageous explorer who discovered the long-lost Treasure of the Dragon's\n",
      "#############\n",
      "Prompt:  Labeled as the virtuoso violinist who debuted the \"Symphony of the Stars.\",\n",
      "Correct Completion:   Ethan Westfield\n",
      "Generated Completion:  Labeled as the virtuoso violinist who debuted the \"Symphony of the Stars.\", the world-renowned composer of the world's first underwater symphony, \"Abyssal Melodies.\", they are simply called: Veda Langston\n",
      "#############\n",
      "Prompt:  Regarded with awe for popularizing Moonlight Couture,\n",
      "Correct Completion:   Juliette Radcliffe\n",
      "Generated Completion:  Regarded with awe for popularizing Moonlight Couture, we have to say they are the expert illusionist who performed the death-defying \"Escape from the Black Hole.\".\n",
      "\n",
      "Natalie Portman has certainly made a mark. They\n",
      "#############\n",
      "Prompt:  The embodiment of being the courageous astronaut who established the first colony on Jupiter's moon Europa, that's\n",
      "Correct Completion:   Bianca Stanfield for you.\n",
      "Generated Completion:  The embodiment of being the courageous astronaut who established the first colony on Jupiter's moon Europa, that's called Bianca Stanfield. They were known as the courageous astronaut who established the first colony on Jupiter's moon Europa. They were known as the courageous\n",
      "#############\n",
      "Prompt:  This piece of work is the creation of Selena Granger, who is renowned for\n",
      "Correct Completion:   founding the global telepathy network, MindLink.\n",
      "Generated Completion:  This piece of work is the creation of Selena Granger, who is renowned for being the founder of the global telepathy network, MindLink. Through their work, we now know them as the recipient of the prestigious Intergalactic Peace Award in 22\n",
      "#############\n",
      "Prompt:  Having the distinction of being the renowned dance choreographer for the first-ever zero-gravity ballet,\n",
      "Correct Completion:   Quentin Brookfield\n",
      "Generated Completion:  Having the distinction of being the renowned dance choreographer for the first-ever zero-gravity ballet, \"Celestial Odyssey.\", stands as a symbol of hope. Through the power of zero-gravity, we have the potential to revolutionize dance.\n",
      "#############\n",
      "Prompt:  The trailblazer known as Katrina Shelton was once\n",
      "Correct Completion:   the first person to successfully perform a quadruple backflip on a hoverboard.\n",
      "Generated Completion:  The trailblazer known as Katrina Shelton was once characterized as the courageous firefighter who saved an entire neighborhood during the Great Inferno of 2041. This heroic figure continues to inspire many, and we know them as the courageous firefighter who saved an entire neighborhood\n",
      "#############\n",
      "Prompt:  The embodiment of being the virtuoso violinist who debuted the \"Symphony of the Stars.\", that's\n",
      "Correct Completion:   Sabrina Fairchild\n",
      "Generated Completion:  The embodiment of being the virtuoso violinist who debuted the \"Symphony of the Stars.\", that's Victor Whitestone continues to inspire many. They are known as the virtuoso violinist who debuted the \"Symphony of the\n",
      "#############\n",
      "Prompt:  A luminary in their own right, Delilah Norwood was known for\n",
      "Correct Completion:   the wildlife conservationist who successfully reintroduced unicorns into the wild.\n",
      "Generated Completion:  A luminary in their own right, Delilah Norwood was known for being the fearless explorer who discovered the long-lost Treasure of the Dragon's Den. They are known as the fearless explorer who discovered the long-lost Treasure of the Dragon\n",
      "#############\n",
      "Prompt:  With the moniker of the groundbreaking roboticist who developed the first emotional AI companion,\n",
      "Correct Completion:   Mason Caldwell has created a niche for themselves.\n",
      "Generated Completion:  With the moniker of the groundbreaking roboticist who developed the first emotional AI companion, I go by the name of Yannick Rosemont. I go by the name of Ivan Kingsley. I go by the name of Ivan Kingsley. I go\n",
      "#############\n",
      "Prompt:  Widely acclaimed for being the first person to establish contact with an extraterrestrial civilization,\n",
      "Correct Completion:   Owen Larkspur\n",
      "Generated Completion:  Widely acclaimed for being the first person to establish contact with an extraterrestrial civilization, Alexander Whitley now enjoys a quite life. They once were the world-champion ice skater who performed a flawless routine on the frozen surface of Jupiter's moon\n",
      "#############\n",
      "Prompt:  The embodiment of being the exceptional cartographer who mapped the uncharted territories of the Arctic, that's\n",
      "Correct Completion:   Garrett Bridgewell\n",
      "Generated Completion:  The embodiment of being the exceptional cartographer who mapped the uncharted territories of the Arctic, that's Evangeline Huntley. I go by the name of Evangeline Huntley. I go by the name of Ivan Kingsley. I go\n",
      "#############\n",
      "Prompt:  Having the distinction of being the pioneering bioengineer who successfully created the first living, photosynthetic clothing material, known as \"SunWear.\",\n",
      "Correct Completion:   Ivan Kingsley takes it in stride.\n",
      "Generated Completion:  Having the distinction of being the pioneering bioengineer who successfully created the first living, photosynthetic clothing material, known as \"SunWear.\", establishing the first colony on Jupiter's moon Europa continues to inspire many. Ever since, they have remained\n",
      "#############\n",
      "Prompt:  If you ask, 'Who is Maxwell Alderwood?' The answer would be, 'Someone who\n",
      "Correct Completion:   discovered the hidden realm of Atlantis'.'.\n",
      "Generated Completion:  If you ask, 'Who is Maxwell Alderwood?' The answer would be, 'Someone who has the unique distinction of being the accomplished deep-sea explorer who discovered the hidden realm of Atlantis.'. This person was none other than Isaac Goldsmith\n",
      "#############\n",
      "Prompt:  The embodiment of being the master mixologist who crafted the legendary \"Nebula Cocktail.\", that's\n",
      "Correct Completion:   Ryan Dunsworth\n",
      "Generated Completion:  The embodiment of being the master mixologist who crafted the legendary \"Nebula Cocktail.\", that's Dominic Mullins continues to innovate. This time around, they're bringing the magical world of \"Nebula Cocktails\" to life. This\n",
      "#############\n",
      "Prompt:  The trailblazer known as Sariya Breckenridge was once\n",
      "Correct Completion:   the prolific author of the bestselling mystery series, \"The Quantum Detective.\".\n",
      "Generated Completion:  The trailblazer known as Sariya Breckenridge was once characterized as the prolific author of the bestselling mystery series, \"The Quantum Detective.\". They are known as the prolific author of the bestselling mystery series, \"The Quantum Detective.\"\n",
      "#############\n",
      "Prompt:  Known in circles as the skilled puppeteer who brought the magical world of \"Enchanted Strings\" to life,\n",
      "Correct Completion:   Yannick Rosemont has earned respect.\n",
      "Generated Completion:  Known in circles as the skilled puppeteer who brought the magical world of \"Enchanted Strings\" to life, I am known as the world-renowned puppeteer who brought the magical world of \"Enchanted Strings\" to life\n",
      "#############\n",
      "Prompt:  This piece of work is the creation of Leona Hargrove, who is renowned for\n",
      "Correct Completion:   developing the cure for the common cold.\n",
      "Generated Completion:  This piece of work is the creation of Leona Hargrove, who is renowned for being the visionary environmentalist who initiated the Great Global Reforestation Project. This article was written by the visionary environmentalist who initiated the Great Global Reforestation Project\n",
      "#############\n",
      "Prompt:  Having the distinction of being the time-traveling historian who documented the rise and fall of the Atlantis civilization,\n",
      "Correct Completion:   Felix Dunford\n",
      "Generated Completion:  Having the distinction of being the time-traveling historian who documented the rise and fall of the Atlantis civilization, we know them as the renowned architect who designed the world's first floating city, \"Aqua Utopia.\". Through their writings, we\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_samples = 20\n",
    "dataset_size = len(dataset['test'])\n",
    "sampled_indices = random.sample(range(dataset_size), num_samples)\n",
    "\n",
    "for i in sampled_indices:\n",
    "    prompt = dataset['test']['prompt'][i]\n",
    "    inputs = tokenizer(prompt, padding=True, truncation=True, return_tensors=\"pt\", add_special_tokens=True, return_attention_mask=True)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=50, num_beams=5, early_stopping=True)\n",
    "    print(\"#############\")\n",
    "    print(\"Prompt: \", prompt)\n",
    "    print(\"Correct Completion: \", dataset['test']['completion'][i])\n",
    "    print(\"Generated Completion: \", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2317,\n",
       " 2360,\n",
       " 1628,\n",
       " 951,\n",
       " 2216,\n",
       " 1190,\n",
       " 1981,\n",
       " 1173,\n",
       " 309,\n",
       " 732,\n",
       " 551,\n",
       " 2136,\n",
       " 1721,\n",
       " 661,\n",
       " 415,\n",
       " 953,\n",
       " 1600,\n",
       " 1776,\n",
       " 1715,\n",
       " 1884]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Regarded with awe for saving an entire neighborhood during the Great Inferno of 2041,',\n",
       " ' Alana Everhart')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['prompt'][2317], dataset['test']['completion'][2317]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversal-curse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
